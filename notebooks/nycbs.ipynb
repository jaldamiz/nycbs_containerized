{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " üö≤ NYC Bike Share Data Analysis\n",
    " \n",
    " ## üìã Project Overview\n",
    " This notebook analyzes bike sharing data from New York City, providing insights into riding patterns, popular routes, and usage statistics.\n",
    " \n",
    " ## üèóÔ∏è Architecture\n",
    " The project follows a medallion architecture for data processing:\n",
    " \n",
    " ### ü•â Bronze Layer\n",
    " - Raw data ingestion\n",
    " - Data validation\n",
    " - Source tracking\n",
    " - Audit logging\n",
    " \n",
    " ### ü•à Silver Layer\n",
    " - Data cleaning\n",
    " - Feature engineering\n",
    " - Quality checks\n",
    " - Enrichment with:\n",
    "   - Ride duration\n",
    "   - Distance calculations\n",
    "   - Time-based features\n",
    " \n",
    " ### ü•á Gold Layer\n",
    " - Analytics views\n",
    " - Aggregated metrics\n",
    " - Business KPIs\n",
    " - Reporting tables\n",
    " \n",
    " ## üîß Technical Setup\n",
    " This notebook leverages:\n",
    " - Apache Spark with Delta Lake\n",
    " - ELK Stack for logging\n",
    " - Docker containerization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 00:55:01,131 - INFO - [nycbs.notebook.nycbs] - Starting notebook execution\n",
      "2025-03-05 00:55:13,372 - INFO - [nycbs.notebook.nycbs] - Spark session initialized\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### üîç Configuration Summary:\n",
       "- üìÇ Warehouse Directory: `/home/aldamiz/warehouse`\n",
       "- üìÇ Data Directory: `/home/aldamiz/data`\n",
       "- ü•â Bronze Layer: `/home/aldamiz/data/bronze`\n",
       "- ü•à Silver Layer: `/home/aldamiz/data/silver`\n",
       "- ü•á Gold Layer: `/home/aldamiz/data/gold`\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import shutil\n",
    "import zipfile\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import DeltaTable\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from src.utils.notebook_logger import NotebookLogger\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "# Initialize logger\n",
    "logger = NotebookLogger('nycbs')\n",
    "logger.info(\"Starting notebook execution\")\n",
    "\n",
    "# Initialize Spark with Delta configurations\n",
    "start_time = time.time()\n",
    "spark = SparkSession.builder.appName(\"NYCBS_Analysis\").getOrCreate()\n",
    "duration_ms = (time.time() - start_time) * 1000\n",
    "logger.spark_operation(\n",
    "    \"Spark session initialized\",\n",
    "    operation_type=\"init\",\n",
    "    duration_ms=duration_ms,\n",
    "    spark_version=spark.version\n",
    "    ) \n",
    "\n",
    "# Define paths\n",
    "WAREHOUSE_DIR = os.getenv('WAREHOUSE_DIR', '/home/aldamiz/warehouse')\n",
    "DATA_DIR = os.getenv('DATA_DIR', '/home/aldamiz/data')\n",
    "BRONZE_PATH = os.path.join(DATA_DIR, 'bronze')\n",
    "SILVER_PATH = os.path.join(DATA_DIR, 'silver')\n",
    "GOLD_PATH = os.path.join(DATA_DIR, 'gold')\n",
    "\n",
    "# Display configuration summary\n",
    "display(Markdown(\"\"\"\n",
    "### üîç Configuration Summary:\n",
    "- üìÇ Warehouse Directory: `{}`\n",
    "- üìÇ Data Directory: `{}`\n",
    "- ü•â Bronze Layer: `{}`\n",
    "- ü•à Silver Layer: `{}`\n",
    "- ü•á Gold Layer: `{}`\n",
    "\"\"\".format(WAREHOUSE_DIR, DATA_DIR, BRONZE_PATH, SILVER_PATH, GOLD_PATH)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # üìä Control Table Setup\n",
    " \n",
    " ## Purpose\n",
    " The control table tracks the processing status of source files to ensure:\n",
    " - üîÑ Idempotency (no duplicate processing)\n",
    " - üìù Audit trail of all operations\n",
    " - ‚è±Ô∏è Processing timestamps\n",
    " - üìà Record counts\n",
    " - üéØ Status tracking\n",
    " \n",
    " ## Schema\n",
    " - `source_file`: Source file identifier\n",
    " - `file_path`: Location of processed data\n",
    " - `processing_date`: Timestamp of processing\n",
    " - `record_count`: Number of records processed\n",
    " - `status`: Processing status (SUCCESS/FAILED)\n",
    " - `year`, `month`: Temporal partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"color: #31708f;\">‚ÑπÔ∏è Control table exists</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_b0ded_row0_col0, #T_b0ded_row0_col1, #T_b0ded_row0_col2, #T_b0ded_row1_col0, #T_b0ded_row1_col1, #T_b0ded_row1_col2, #T_b0ded_row2_col0, #T_b0ded_row2_col1, #T_b0ded_row2_col2, #T_b0ded_row3_col0, #T_b0ded_row3_col1, #T_b0ded_row3_col2, #T_b0ded_row4_col0, #T_b0ded_row4_col1, #T_b0ded_row4_col2, #T_b0ded_row5_col0, #T_b0ded_row5_col1, #T_b0ded_row5_col2, #T_b0ded_row6_col0, #T_b0ded_row6_col1, #T_b0ded_row6_col2 {\n",
       "  padding: 5px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_b0ded\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_b0ded_level0_col0\" class=\"col_heading level0 col0\" >Field</th>\n",
       "      <th id=\"T_b0ded_level0_col1\" class=\"col_heading level0 col1\" >Type</th>\n",
       "      <th id=\"T_b0ded_level0_col2\" class=\"col_heading level0 col2\" >Required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b0ded_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_b0ded_row0_col0\" class=\"data row0 col0\" >source_file</td>\n",
       "      <td id=\"T_b0ded_row0_col1\" class=\"data row0 col1\" >StringType()</td>\n",
       "      <td id=\"T_b0ded_row0_col2\" class=\"data row0 col2\" >No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b0ded_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_b0ded_row1_col0\" class=\"data row1 col0\" >file_path</td>\n",
       "      <td id=\"T_b0ded_row1_col1\" class=\"data row1 col1\" >StringType()</td>\n",
       "      <td id=\"T_b0ded_row1_col2\" class=\"data row1 col2\" >No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b0ded_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_b0ded_row2_col0\" class=\"data row2 col0\" >processing_date</td>\n",
       "      <td id=\"T_b0ded_row2_col1\" class=\"data row2 col1\" >TimestampType()</td>\n",
       "      <td id=\"T_b0ded_row2_col2\" class=\"data row2 col2\" >No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b0ded_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_b0ded_row3_col0\" class=\"data row3 col0\" >record_count</td>\n",
       "      <td id=\"T_b0ded_row3_col1\" class=\"data row3 col1\" >LongType()</td>\n",
       "      <td id=\"T_b0ded_row3_col2\" class=\"data row3 col2\" >No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b0ded_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_b0ded_row4_col0\" class=\"data row4 col0\" >status</td>\n",
       "      <td id=\"T_b0ded_row4_col1\" class=\"data row4 col1\" >StringType()</td>\n",
       "      <td id=\"T_b0ded_row4_col2\" class=\"data row4 col2\" >No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b0ded_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_b0ded_row5_col0\" class=\"data row5 col0\" >year</td>\n",
       "      <td id=\"T_b0ded_row5_col1\" class=\"data row5 col1\" >IntegerType()</td>\n",
       "      <td id=\"T_b0ded_row5_col2\" class=\"data row5 col2\" >No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b0ded_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_b0ded_row6_col0\" class=\"data row6 col0\" >month</td>\n",
       "      <td id=\"T_b0ded_row6_col1\" class=\"data row6 col1\" >IntegerType()</td>\n",
       "      <td id=\"T_b0ded_row6_col2\" class=\"data row6 col2\" >No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0xffa4cfe8cbe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize control table\n",
    "control_table_path = os.path.join(WAREHOUSE_DIR, \"control_table\")\n",
    "\n",
    "try:\n",
    "    # Create control table if it doesn't exist\n",
    "    if not DeltaTable.isDeltaTable(spark, control_table_path):\n",
    "        logger.info(\"Creating control table\")\n",
    "        \n",
    "        control_schema = StructType([\n",
    "            StructField(\"source_file\", StringType(), False),\n",
    "            StructField(\"file_path\", StringType(), False),\n",
    "            StructField(\"processing_date\", TimestampType(), False),\n",
    "            StructField(\"record_count\", LongType(), True),\n",
    "            StructField(\"status\", StringType(), False),\n",
    "            StructField(\"year\", IntegerType(), False),\n",
    "            StructField(\"month\", IntegerType(), False)\n",
    "        ])\n",
    "        \n",
    "        # Create empty table with schema\n",
    "        empty_df = spark.createDataFrame([], control_schema)\n",
    "        empty_df.write.format(\"delta\").mode(\"errorIfExists\").save(control_table_path)\n",
    "        \n",
    "        display(HTML('<div style=\"color: #3c763d;\">‚úÖ Control table created</div>'))\n",
    "    else:\n",
    "        display(HTML('<div style=\"color: #31708f;\">‚ÑπÔ∏è Control table exists</div>'))\n",
    "\n",
    "    # Display schema and recent history\n",
    "    control_df = spark.read.format(\"delta\").load(control_table_path)\n",
    "    \n",
    "    # Show schema in a clean table\n",
    "    schema_df = pd.DataFrame([\n",
    "        {\"Field\": f.name, \"Type\": str(f.dataType), \"Required\": \"Yes\" if not f.nullable else \"No\"}\n",
    "        for f in control_df.schema\n",
    "    ])\n",
    "    display(schema_df.style.set_properties(**{'padding': '5px'}))\n",
    "    \n",
    "    # Show recent processing history\n",
    "    if control_df.count() > 0:\n",
    "        display(control_df.orderBy(F.col(\"processing_date\").desc())\n",
    "               .limit(5).toPandas()\n",
    "               .style.set_properties(**{'padding': '5px'}))\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in control table setup: {str(e)}\", error=e)\n",
    "    display(HTML(f'<div style=\"color: #a94442;\">‚ùå Error: {str(e)}</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì• Data Ingestion\n",
    " \n",
    " ## Landing Zone Structure\n",
    " - üìÇ `landing/`: Storage for extracted CSV files\n",
    " - üìÇ `landing/zip/`: Storage for downloaded ZIP files\n",
    " \n",
    " ## Configuration\n",
    " - üåç Source: NYC Citibike System Data\n",
    " - üìÖ Select date range for data download\n",
    " - üîÑ Supports multiple years/months and cities\n",
    " - üì¶ Automatic ZIP handling\n",
    " \n",
    " ## Process Flow\n",
    " 1. üéØ Define date range and cities\n",
    " 2. üåê Download ZIP files to `landing/zip/`\n",
    " 3. üì¶ Extract CSV files to `landing/`\n",
    " 4. ‚úÖ Validate files\n",
    " \n",
    " ## File Patterns\n",
    " ### NYC Files\n",
    " - 2013-2023: `YYYY-citibike-tripdata.zip` (annual files)\n",
    " - 2024 (Jan-Apr): `YYYYMM-citibike-tripdata.csv.zip`\n",
    " - 2024 (May+): `YYYYMM-citibike-tripdata.zip`\n",
    " - 2025+: `YYYYMM-citibike-tripdata.zip`\n",
    " \n",
    " ### Jersey City (JC) Files\n",
    " - 2015+: `JC-YYYYMM-citibike-tripdata.csv.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<h4>üìÖ Configure Data Download:</h4>\n",
       "<p>Modify these parameters to select your date range and cities:</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"background-color: #f8f9fa; padding: 15px; border-radius: 5px;\">\n",
       "    <p><b>üìÖ Selected Configuration:</b></p>\n",
       "    <ul>\n",
       "        <li>Start: 2025-01</li>\n",
       "        <li>End: 2025-12</li>\n",
       "        <li>Cities: NYC</li>\n",
       "    </ul>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 00:55:30,847 - INFO - [nycbs.notebook.nycbs] - Planning to download 2 files\n",
      "2025-03-05 00:55:30,851 - INFO - [nycbs.notebook.nycbs] - Downloading 202501-citibike-tripdata.zip\n",
      "2025-03-05 00:55:56,651 - INFO - [nycbs.notebook.nycbs] - Extracting 202501-citibike-tripdata.zip\n",
      "2025-03-05 00:55:59,672 - INFO - [nycbs.notebook.nycbs] - Successfully downloaded and extracted 202501-citibike-tripdata.csv\n",
      "2025-03-05 00:55:59,681 - INFO - [nycbs.notebook.nycbs] - Downloading 202502-citibike-tripdata.zip\n",
      "2025-03-05 00:56:20,814 - INFO - [nycbs.notebook.nycbs] - Extracting 202502-citibike-tripdata.zip\n",
      "2025-03-05 00:56:22,786 - INFO - [nycbs.notebook.nycbs] - Successfully downloaded and extracted 202502-citibike-tripdata.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h4>üì• Download Summary:</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"background-color: #f8f9fa; padding: 15px; border-radius: 5px;\">\n",
       "        <p><b>Results:</b></p>\n",
       "        <ul>\n",
       "            <li>‚úÖ Successfully downloaded: 2</li>\n",
       "            <li>‚ùå Failed downloads: 0</li>\n",
       "            <li>‚è≠Ô∏è Skipped files: 0</li>\n",
       "        </ul>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4>üìÅ Landing Zone Contents:</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_32cbb_row0_col0, #T_32cbb_row0_col1, #T_32cbb_row0_col2, #T_32cbb_row0_col3, #T_32cbb_row1_col0, #T_32cbb_row1_col1, #T_32cbb_row1_col2, #T_32cbb_row1_col3, #T_32cbb_row2_col0, #T_32cbb_row2_col1, #T_32cbb_row2_col2, #T_32cbb_row2_col3, #T_32cbb_row3_col0, #T_32cbb_row3_col1, #T_32cbb_row3_col2, #T_32cbb_row3_col3, #T_32cbb_row4_col0, #T_32cbb_row4_col1, #T_32cbb_row4_col2, #T_32cbb_row4_col3, #T_32cbb_row5_col0, #T_32cbb_row5_col1, #T_32cbb_row5_col2, #T_32cbb_row5_col3 {\n",
       "  padding: 5px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_32cbb\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_32cbb_level0_col0\" class=\"col_heading level0 col0\" >File</th>\n",
       "      <th id=\"T_32cbb_level0_col1\" class=\"col_heading level0 col1\" >Size (MB)</th>\n",
       "      <th id=\"T_32cbb_level0_col2\" class=\"col_heading level0 col2\" >City</th>\n",
       "      <th id=\"T_32cbb_level0_col3\" class=\"col_heading level0 col3\" >Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_32cbb_level0_row0\" class=\"row_heading level0 row0\" >1</th>\n",
       "      <td id=\"T_32cbb_row0_col0\" class=\"data row0 col0\" >202502-citibike-tripdata_1.csv</td>\n",
       "      <td id=\"T_32cbb_row0_col1\" class=\"data row0 col1\" >185.94</td>\n",
       "      <td id=\"T_32cbb_row0_col2\" class=\"data row0 col2\" >NYC</td>\n",
       "      <td id=\"T_32cbb_row0_col3\" class=\"data row0 col3\" >202502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_32cbb_level0_row1\" class=\"row_heading level0 row1\" >2</th>\n",
       "      <td id=\"T_32cbb_row1_col0\" class=\"data row1 col0\" >202502-citibike-tripdata_2.csv</td>\n",
       "      <td id=\"T_32cbb_row1_col1\" class=\"data row1 col1\" >185.98</td>\n",
       "      <td id=\"T_32cbb_row1_col2\" class=\"data row1 col2\" >NYC</td>\n",
       "      <td id=\"T_32cbb_row1_col3\" class=\"data row1 col3\" >202502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_32cbb_level0_row2\" class=\"row_heading level0 row2\" >5</th>\n",
       "      <td id=\"T_32cbb_row2_col0\" class=\"data row2 col0\" >202502-citibike-tripdata_3.csv</td>\n",
       "      <td id=\"T_32cbb_row2_col1\" class=\"data row2 col1\" >5.80</td>\n",
       "      <td id=\"T_32cbb_row2_col2\" class=\"data row2 col2\" >NYC</td>\n",
       "      <td id=\"T_32cbb_row2_col3\" class=\"data row2 col3\" >202502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_32cbb_level0_row3\" class=\"row_heading level0 row3\" >0</th>\n",
       "      <td id=\"T_32cbb_row3_col0\" class=\"data row3 col0\" >202501-citibike-tripdata_2.csv</td>\n",
       "      <td id=\"T_32cbb_row3_col1\" class=\"data row3 col1\" >186.05</td>\n",
       "      <td id=\"T_32cbb_row3_col2\" class=\"data row3 col2\" >NYC</td>\n",
       "      <td id=\"T_32cbb_row3_col3\" class=\"data row3 col3\" >202501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_32cbb_level0_row4\" class=\"row_heading level0 row4\" >3</th>\n",
       "      <td id=\"T_32cbb_row4_col0\" class=\"data row4 col0\" >202501-citibike-tripdata_3.csv</td>\n",
       "      <td id=\"T_32cbb_row4_col1\" class=\"data row4 col1\" >23.12</td>\n",
       "      <td id=\"T_32cbb_row4_col2\" class=\"data row4 col2\" >NYC</td>\n",
       "      <td id=\"T_32cbb_row4_col3\" class=\"data row4 col3\" >202501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_32cbb_level0_row5\" class=\"row_heading level0 row5\" >4</th>\n",
       "      <td id=\"T_32cbb_row5_col0\" class=\"data row5 col0\" >202501-citibike-tripdata_1.csv</td>\n",
       "      <td id=\"T_32cbb_row5_col1\" class=\"data row5 col1\" >185.91</td>\n",
       "      <td id=\"T_32cbb_row5_col2\" class=\"data row5 col2\" >NYC</td>\n",
       "      <td id=\"T_32cbb_row5_col3\" class=\"data row5 col3\" >202501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0xffa4ef0f62c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Date Range and City Configuration\n",
    "display(HTML(\"\"\"\n",
    "<h4>üìÖ Configure Data Download:</h4>\n",
    "<p>Modify these parameters to select your date range and cities:</p>\n",
    "\"\"\"))\n",
    "\n",
    "# Define parameters\n",
    "START_YEAR = 2025    # Modify this for different start year\n",
    "START_MONTH = 1      # 1-12\n",
    "END_YEAR = 2025      # Modify this for different end year\n",
    "END_MONTH = 12       # 1-12\n",
    "CITIES = ['NYC']  # NYC for New York City, JC for Jersey City\n",
    "\n",
    "def get_file_url(filename):\n",
    "    \"\"\"Generate S3 URL for a given filename\"\"\"\n",
    "    return f'https://s3.amazonaws.com/tripdata/{filename}'\n",
    "\n",
    "def generate_filename(city, year, month, current_date):\n",
    "    \"\"\"\n",
    "    Generate the correct filename based on city and date\n",
    "    \n",
    "    NYC Pattern:\n",
    "    - 2013-2023: YYYY-citibike-tripdata.zip (annual files)\n",
    "    - 2024 (01-04): YYYYMM-citibike-tripdata.csv.zip\n",
    "    - 2024 (05+) and beyond: YYYYMM-citibike-tripdata.zip\n",
    "    \n",
    "    JC Pattern:\n",
    "    - 2015+: JC-YYYYMM-citibike-tripdata.csv.zip\n",
    "    \"\"\"\n",
    "    # Check if the date is current month or future\n",
    "    if year > current_date.year or (year == current_date.year and month >= current_date.month):\n",
    "        return None\n",
    "\n",
    "    if city == 'NYC':\n",
    "        if year < 2024:\n",
    "            # Annual files for 2013-2023\n",
    "            if month == 12:  # Only generate annual file in December\n",
    "                return f\"{year}-citibike-tripdata.zip\"\n",
    "        else:  # 2024 and beyond\n",
    "            if year == 2024 and 1 <= month <= 4:\n",
    "                # January to April 2024: YYYYMM-citibike-tripdata.csv.zip\n",
    "                return f\"{year}{month:02d}-citibike-tripdata.csv.zip\"\n",
    "            else:\n",
    "                # May 2024 onwards and all months in 2025+: YYYYMM-citibike-tripdata.zip\n",
    "                return f\"{year}{month:02d}-citibike-tripdata.zip\"\n",
    "    \n",
    "    elif city == 'JC' and year >= 2015:  # JC data starts from 2015\n",
    "        return f\"JC-{year}{month:02d}-citibike-tripdata.csv.zip\"\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Display configuration\n",
    "display(HTML(f\"\"\"\n",
    "<div style=\"background-color: #f8f9fa; padding: 15px; border-radius: 5px;\">\n",
    "    <p><b>üìÖ Selected Configuration:</b></p>\n",
    "    <ul>\n",
    "        <li>Start: {START_YEAR}-{START_MONTH:02d}</li>\n",
    "        <li>End: {END_YEAR}-{END_MONTH:02d}</li>\n",
    "        <li>Cities: {', '.join(CITIES)}</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "try:\n",
    "    # Get current date for validation\n",
    "    current_date = datetime.now()\n",
    "    \n",
    "    # Validate configuration\n",
    "    if not (1 <= START_MONTH <= 12 and 1 <= END_MONTH <= 12):\n",
    "        raise ValueError(\"Months must be between 1 and 12\")\n",
    "    if START_YEAR > END_YEAR or (START_YEAR == END_YEAR and START_MONTH > END_MONTH):\n",
    "        raise ValueError(\"Start date must be before end date\")\n",
    "    \n",
    "    # Create directories if they don't exist\n",
    "    landing_path = os.path.join(DATA_DIR, \"landing\")\n",
    "    zip_storage_path = os.path.join(landing_path, \"zip\")  # ZIP files storage\n",
    "    Path(landing_path).mkdir(parents=True, exist_ok=True)\n",
    "    Path(zip_storage_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Read control table\n",
    "    control_df = spark.read.format(\"delta\").load(control_table_path)\n",
    "    processed_files = set(control_df.filter(F.col(\"status\") == \"SUCCESS\")\n",
    "                         .select(\"source_file\")\n",
    "                         .toPandas()[\"source_file\"])\n",
    "\n",
    "    # Generate list of files to download\n",
    "    sources = []\n",
    "    \n",
    "    for city in CITIES:\n",
    "        current_year = START_YEAR\n",
    "        current_month = START_MONTH\n",
    "        \n",
    "        while (current_year < END_YEAR or \n",
    "               (current_year == END_YEAR and current_month <= END_MONTH)):\n",
    "            \n",
    "            filename = generate_filename(city, current_year, current_month, current_date)\n",
    "            if filename:  # Only add if a valid filename was generated\n",
    "                csv_filename = filename.replace('.zip', '.csv')\n",
    "                \n",
    "                # Skip if already processed or exists in landing\n",
    "                if csv_filename not in processed_files and not os.path.exists(os.path.join(landing_path, csv_filename)):\n",
    "                    sources.append({\n",
    "                        'url': get_file_url(filename),\n",
    "                        'year': current_year,\n",
    "                        'month': current_month,\n",
    "                        'city': city,\n",
    "                        'filename': filename\n",
    "                    })\n",
    "            \n",
    "            current_month += 1\n",
    "            if current_month > 12:\n",
    "                current_month = 1\n",
    "                current_year += 1\n",
    "\n",
    "    logger.info(f\"Planning to download {len(sources)} files\")\n",
    "    \n",
    "    # Download and extract files\n",
    "    files_downloaded = 0\n",
    "    failed_downloads = []\n",
    "    skipped_files = []\n",
    "    \n",
    "    for source in sources:\n",
    "        csv_filename = source['filename'].replace('.zip', '.csv')\n",
    "        zip_file_path = os.path.join(zip_storage_path, source['filename'])\n",
    "        csv_file_path = os.path.join(landing_path, csv_filename)\n",
    "        \n",
    "        # Skip if CSV already exists in landing\n",
    "        if csv_filename in processed_files:\n",
    "            logger.info(f\"Skipping {csv_filename} - already processed\")\n",
    "            skipped_files.append((csv_filename, \"Already processed\"))\n",
    "            continue\n",
    "            \n",
    "        if os.path.exists(os.path.join(landing_path, csv_filename)):\n",
    "            logger.info(f\"Skipping {csv_filename} - already in landing zone\")\n",
    "            skipped_files.append((csv_filename, \"Already in landing zone\"))\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Check if ZIP exists first\n",
    "            if not os.path.exists(zip_file_path):\n",
    "                # Download file only if it doesn't exist\n",
    "                logger.info(f\"Downloading {source['filename']}\")\n",
    "                response = requests.get(source['url'], stream=True)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                with open(zip_file_path, 'wb') as f:\n",
    "                    shutil.copyfileobj(response.raw, f)\n",
    "            else:\n",
    "                logger.info(f\"Using existing ZIP file: {source['filename']}\")\n",
    "            \n",
    "            # Extract file\n",
    "            logger.info(f\"Extracting {source['filename']}\")\n",
    "            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                # Filter out macOS metadata files and extract only CSV files\n",
    "                csv_files = [f for f in zip_ref.namelist() \n",
    "                        if f.endswith('.csv') and not f.startswith('__MACOSX')]\n",
    "                \n",
    "                # Extract each CSV file\n",
    "                for csv_file in csv_files:\n",
    "                    # Remove any path components and get just the filename\n",
    "                    csv_basename = os.path.basename(csv_file)\n",
    "                    # Extract with the new filename\n",
    "                    with zip_ref.open(csv_file) as source_file, \\\n",
    "                        open(os.path.join(landing_path, csv_basename), 'wb') as target_file:\n",
    "                        shutil.copyfileobj(source_file, target_file)\n",
    "            \n",
    "            files_downloaded += 1\n",
    "            logger.info(f\"Successfully downloaded and extracted {csv_filename}\")\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.warning(f\"Failed to download {source['filename']}: {str(e)}\")\n",
    "            failed_downloads.append((source['filename'], str(e)))\n",
    "            # Remove failed download if it exists\n",
    "            if os.path.exists(zip_path):\n",
    "                os.remove(zip_path)\n",
    "            continue\n",
    "        except zipfile.BadZipFile:\n",
    "            logger.warning(f\"Corrupt zip file: {source['filename']}\")\n",
    "            failed_downloads.append((source['filename'], \"Corrupt ZIP file\"))\n",
    "            if os.path.exists(zip_path):\n",
    "                os.remove(zip_path)\n",
    "            continue\n",
    "\n",
    "    # Display summary\n",
    "    display(HTML(\"<h4>üì• Download Summary:</h4>\"))\n",
    "    \n",
    "    summary_html = f\"\"\"\n",
    "    <div style=\"background-color: #f8f9fa; padding: 15px; border-radius: 5px;\">\n",
    "        <p><b>Results:</b></p>\n",
    "        <ul>\n",
    "            <li>‚úÖ Successfully downloaded: {files_downloaded}</li>\n",
    "            <li>‚ùå Failed downloads: {len(failed_downloads)}</li>\n",
    "            <li>‚è≠Ô∏è Skipped files: {len(skipped_files)}</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(summary_html))\n",
    "\n",
    "    # Show skipped files if any\n",
    "    if skipped_files:\n",
    "        display(HTML(\"<h4>‚è≠Ô∏è Skipped Files:</h4>\"))\n",
    "        skipped_df = pd.DataFrame(skipped_files, columns=['File', 'Reason'])\n",
    "        display(skipped_df.style.set_properties(**{'padding': '5px'}))\n",
    "\n",
    "    # Show failed downloads if any\n",
    "    if failed_downloads:\n",
    "        display(HTML(\"<h4>‚ùå Failed Downloads:</h4>\"))\n",
    "        failed_df = pd.DataFrame(failed_downloads, columns=['File', 'Error'])\n",
    "        display(failed_df.style.set_properties(**{'padding': '5px'}))\n",
    "\n",
    "    # Show landing zone contents\n",
    "    display(HTML(\"<h4>üìÅ Landing Zone Contents:</h4>\"))\n",
    "    landing_files = [f for f in os.listdir(landing_path) if f.endswith('.csv')]\n",
    "    if landing_files:\n",
    "        files_df = pd.DataFrame({\n",
    "            'File': landing_files,\n",
    "            'Size (MB)': [round(os.path.getsize(os.path.join(landing_path, f)) / (1024 * 1024), 2) \n",
    "                         for f in landing_files],\n",
    "            'City': ['JC' if f.startswith('JC-') else 'NYC' for f in landing_files],\n",
    "            'Date': [f.split('-')[1] if f.startswith('JC-') else f.split('-')[0] for f in landing_files]\n",
    "        }).sort_values(['City', 'Date'], ascending=[True, False])\n",
    "        \n",
    "        display(files_df.style\n",
    "               .set_properties(**{'padding': '5px'})\n",
    "               .format({'Size (MB)': '{:.2f}'}))\n",
    "    else:\n",
    "        display(HTML('<div style=\"color: #31708f;\">‚ÑπÔ∏è Landing zone is empty</div>'))\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in data download: {str(e)}\", exc_info=True)\n",
    "    display(HTML(f'<div style=\"color: #a94442;\">‚ùå Error: {str(e)}</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü•â Bronze Layer Processing\n",
    " \n",
    " ## Purpose\n",
    " The bronze layer ingests raw data while ensuring:\n",
    " - üîí No duplicate processing of source files\n",
    " - ‚úÖ Data validation\n",
    " - üìä Record counting\n",
    " - üìù Processing status tracking\n",
    " \n",
    " ## Process Flow\n",
    " 1. Check control table for existing processing\n",
    " 2. Read source CSV files\n",
    " 3. Validate data structure\n",
    " 4. Write to bronze Delta table\n",
    " 5. Update control table with processing status\n",
    " \n",
    " ## Partitioning Strategy\n",
    " Data is partitioned by:\n",
    " - üìÖ Year\n",
    " - üìÖ Month\n",
    " \n",
    " This enables efficient querying and data management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 00:56:22,936 - INFO - [nycbs.notebook.nycbs] - Found 6 CSV files to process\n",
      "2025-03-05 00:56:23,659 - INFO - [nycbs.notebook.nycbs] - Found 6 new files to process\n",
      "2025-03-05 00:56:23,661 - INFO - [nycbs.notebook.nycbs] - Processing 202501-citibike-tripdata_2.csv\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4865/672928978.py\", line 158, in <module>\n",
      "    .save(bronze_rides_path))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/sql/readwriter.py\", line 1398, in save\n",
      "    self._jwrite.save(path)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 175, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/logging/handlers.py\", line 73, in emit\n",
      "    if self.shouldRollover(record):\n",
      "  File \"/opt/conda/lib/python3.10/logging/handlers.py\", line 196, in shouldRollover\n",
      "    msg = \"%s\\n\" % self.format(record)\n",
      "  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 59, in format\n",
      "    return json.dumps({k: v for k, v in log_obj.items() if v is not None})\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 231, in dumps\n",
      "    return _default_encoder.encode(obj)\n",
      "  File \"/opt/conda/lib/python3.10/json/encoder.py\", line 199, in encode\n",
      "    chunks = self.iterencode(o, _one_shot=True)\n",
      "  File \"/opt/conda/lib/python3.10/json/encoder.py\", line 257, in iterencode\n",
      "    return _iterencode(o, 0)\n",
      "  File \"/opt/conda/lib/python3.10/json/encoder.py\", line 179, in default\n",
      "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
      "TypeError: Object of type traceback is not JSON serializable\n",
      "Call stack:\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_4865/672928978.py\", line 202, in <module>\n",
      "    logger.error(f\"Error processing file {csv_file}: {str(e)}\", error=e)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 183, in error\n",
      "    self.log(logging.ERROR, message, **extra)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 168, in log\n",
      "    self.logger.log(level, message, extra=extra)\n",
      "Message: \"Error processing file 202501-citibike-tripdata_2.csv: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\"\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4865/672928978.py\", line 158, in <module>\n",
      "    .save(bronze_rides_path))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/sql/readwriter.py\", line 1398, in save\n",
      "    self._jwrite.save(path)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 175, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/logging/handlers.py\", line 73, in emit\n",
      "    if self.shouldRollover(record):\n",
      "  File \"/opt/conda/lib/python3.10/logging/handlers.py\", line 196, in shouldRollover\n",
      "    msg = \"%s\\n\" % self.format(record)\n",
      "  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 59, in format\n",
      "    return json.dumps({k: v for k, v in log_obj.items() if v is not None})\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 231, in dumps\n",
      "    return _default_encoder.encode(obj)\n",
      "  File \"/opt/conda/lib/python3.10/json/encoder.py\", line 199, in encode\n",
      "    chunks = self.iterencode(o, _one_shot=True)\n",
      "  File \"/opt/conda/lib/python3.10/json/encoder.py\", line 257, in iterencode\n",
      "    return _iterencode(o, 0)\n",
      "  File \"/opt/conda/lib/python3.10/json/encoder.py\", line 179, in default\n",
      "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
      "TypeError: Object of type traceback is not JSON serializable\n",
      "Call stack:\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_4865/672928978.py\", line 202, in <module>\n",
      "    logger.error(f\"Error processing file {csv_file}: {str(e)}\", error=e)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 183, in error\n",
      "    self.log(logging.ERROR, message, **extra)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 168, in log\n",
      "    self.logger.log(level, message, extra=extra)\n",
      "Message: \"Error processing file 202501-citibike-tripdata_2.csv: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\"\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4865/672928978.py\", line 158, in <module>\n",
      "    .save(bronze_rides_path))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/sql/readwriter.py\", line 1398, in save\n",
      "    self._jwrite.save(path)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 175, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/logging/handlers.py\", line 677, in emit\n",
      "    s = self.makePickle(record)\n",
      "  File \"/opt/conda/lib/python3.10/logging/handlers.py\", line 649, in makePickle\n",
      "    s = pickle.dumps(d, 1)\n",
      "  File \"/opt/conda/lib/python3.10/copyreg.py\", line 76, in _reduce_ex\n",
      "    raise TypeError(f\"cannot pickle {cls.__name__!r} object\")\n",
      "TypeError: cannot pickle 'traceback' object\n",
      "Call stack:\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_4865/672928978.py\", line 202, in <module>\n",
      "    logger.error(f\"Error processing file {csv_file}: {str(e)}\", error=e)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 183, in error\n",
      "    self.log(logging.ERROR, message, **extra)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 168, in log\n",
      "    self.logger.log(level, message, extra=extra)\n",
      "Message: \"Error processing file 202501-citibike-tripdata_2.csv: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\"\n",
      "Arguments: ()\n",
      "2025-03-05 00:56:30,334 - ERROR - [nycbs.notebook.nycbs] - Error processing file 202501-citibike-tripdata_2.csv: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\n",
      "2025-03-05 00:56:30,387 - INFO - [nycbs.notebook.nycbs] - Processing 202502-citibike-tripdata_1.csv\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4865/672928978.py\", line 158, in <module>\n",
      "    .save(bronze_rides_path))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/sql/readwriter.py\", line 1398, in save\n",
      "    self._jwrite.save(path)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 175, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/logging/handlers.py\", line 73, in emit\n",
      "    if self.shouldRollover(record):\n",
      "  File \"/opt/conda/lib/python3.10/logging/handlers.py\", line 196, in shouldRollover\n",
      "    msg = \"%s\\n\" % self.format(record)\n",
      "  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 59, in format\n",
      "    return json.dumps({k: v for k, v in log_obj.items() if v is not None})\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 231, in dumps\n",
      "    return _default_encoder.encode(obj)\n",
      "  File \"/opt/conda/lib/python3.10/json/encoder.py\", line 199, in encode\n",
      "    chunks = self.iterencode(o, _one_shot=True)\n",
      "  File \"/opt/conda/lib/python3.10/json/encoder.py\", line 257, in iterencode\n",
      "    return _iterencode(o, 0)\n",
      "  File \"/opt/conda/lib/python3.10/json/encoder.py\", line 179, in default\n",
      "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
      "TypeError: Object of type traceback is not JSON serializable\n",
      "Call stack:\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_4865/672928978.py\", line 202, in <module>\n",
      "    logger.error(f\"Error processing file {csv_file}: {str(e)}\", error=e)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 183, in error\n",
      "    self.log(logging.ERROR, message, **extra)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 168, in log\n",
      "    self.logger.log(level, message, extra=extra)\n",
      "Message: \"Error processing file 202502-citibike-tripdata_1.csv: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\"\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4865/672928978.py\", line 158, in <module>\n",
      "    .save(bronze_rides_path))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/sql/readwriter.py\", line 1398, in save\n",
      "    self._jwrite.save(path)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 175, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/logging/handlers.py\", line 73, in emit\n",
      "    if self.shouldRollover(record):\n",
      "  File \"/opt/conda/lib/python3.10/logging/handlers.py\", line 196, in shouldRollover\n",
      "    msg = \"%s\\n\" % self.format(record)\n",
      "  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 59, in format\n",
      "    return json.dumps({k: v for k, v in log_obj.items() if v is not None})\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 231, in dumps\n",
      "    return _default_encoder.encode(obj)\n",
      "  File \"/opt/conda/lib/python3.10/json/encoder.py\", line 199, in encode\n",
      "    chunks = self.iterencode(o, _one_shot=True)\n",
      "  File \"/opt/conda/lib/python3.10/json/encoder.py\", line 257, in iterencode\n",
      "    return _iterencode(o, 0)\n",
      "  File \"/opt/conda/lib/python3.10/json/encoder.py\", line 179, in default\n",
      "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
      "TypeError: Object of type traceback is not JSON serializable\n",
      "Call stack:\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_4865/672928978.py\", line 202, in <module>\n",
      "    logger.error(f\"Error processing file {csv_file}: {str(e)}\", error=e)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 183, in error\n",
      "    self.log(logging.ERROR, message, **extra)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 168, in log\n",
      "    self.logger.log(level, message, extra=extra)\n",
      "Message: \"Error processing file 202502-citibike-tripdata_1.csv: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\"\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4865/672928978.py\", line 158, in <module>\n",
      "    .save(bronze_rides_path))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/sql/readwriter.py\", line 1398, in save\n",
      "    self._jwrite.save(path)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 175, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/logging/handlers.py\", line 677, in emit\n",
      "    s = self.makePickle(record)\n",
      "  File \"/opt/conda/lib/python3.10/logging/handlers.py\", line 649, in makePickle\n",
      "    s = pickle.dumps(d, 1)\n",
      "  File \"/opt/conda/lib/python3.10/copyreg.py\", line 76, in _reduce_ex\n",
      "    raise TypeError(f\"cannot pickle {cls.__name__!r} object\")\n",
      "TypeError: cannot pickle 'traceback' object\n",
      "Call stack:\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_4865/672928978.py\", line 202, in <module>\n",
      "    logger.error(f\"Error processing file {csv_file}: {str(e)}\", error=e)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 183, in error\n",
      "    self.log(logging.ERROR, message, **extra)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 168, in log\n",
      "    self.logger.log(level, message, extra=extra)\n",
      "Message: \"Error processing file 202502-citibike-tripdata_1.csv: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\"\n",
      "Arguments: ()\n",
      "2025-03-05 00:56:33,870 - ERROR - [nycbs.notebook.nycbs] - Error processing file 202502-citibike-tripdata_1.csv: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\n",
      "2025-03-05 00:56:33,898 - INFO - [nycbs.notebook.nycbs] - Processing 202502-citibike-tripdata_2.csv\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4865/672928978.py\", line 158, in <module>\n",
      "    .save(bronze_rides_path))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/sql/readwriter.py\", line 1398, in save\n",
      "    self._jwrite.save(path)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 175, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/logging/handlers.py\", line 73, in emit\n",
      "    if self.shouldRollover(record):\n",
      "  File \"/opt/conda/lib/python3.10/logging/handlers.py\", line 196, in shouldRollover\n",
      "    msg = \"%s\\n\" % self.format(record)\n",
      "  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 59, in format\n",
      "    return json.dumps({k: v for k, v in log_obj.items() if v is not None})\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 231, in dumps\n",
      "    return _default_encoder.encode(obj)\n",
      "  File \"/opt/conda/lib/python3.10/json/encoder.py\", line 199, in encode\n",
      "    chunks = self.iterencode(o, _one_shot=True)\n",
      "  File \"/opt/conda/lib/python3.10/json/encoder.py\", line 257, in iterencode\n",
      "    return _iterencode(o, 0)\n",
      "  File \"/opt/conda/lib/python3.10/json/encoder.py\", line 179, in default\n",
      "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
      "TypeError: Object of type traceback is not JSON serializable\n",
      "Call stack:\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_4865/672928978.py\", line 202, in <module>\n",
      "    logger.error(f\"Error processing file {csv_file}: {str(e)}\", error=e)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 183, in error\n",
      "    self.log(logging.ERROR, message, **extra)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 168, in log\n",
      "    self.logger.log(level, message, extra=extra)\n",
      "Message: \"Error processing file 202502-citibike-tripdata_2.csv: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\"\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4865/672928978.py\", line 158, in <module>\n",
      "    .save(bronze_rides_path))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/sql/readwriter.py\", line 1398, in save\n",
      "    self._jwrite.save(path)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 175, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/logging/handlers.py\", line 73, in emit\n",
      "    if self.shouldRollover(record):\n",
      "  File \"/opt/conda/lib/python3.10/logging/handlers.py\", line 196, in shouldRollover\n",
      "    msg = \"%s\\n\" % self.format(record)\n",
      "  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 59, in format\n",
      "    return json.dumps({k: v for k, v in log_obj.items() if v is not None})\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 231, in dumps\n",
      "    return _default_encoder.encode(obj)\n",
      "  File \"/opt/conda/lib/python3.10/json/encoder.py\", line 199, in encode\n",
      "    chunks = self.iterencode(o, _one_shot=True)\n",
      "  File \"/opt/conda/lib/python3.10/json/encoder.py\", line 257, in iterencode\n",
      "    return _iterencode(o, 0)\n",
      "  File \"/opt/conda/lib/python3.10/json/encoder.py\", line 179, in default\n",
      "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
      "TypeError: Object of type traceback is not JSON serializable\n",
      "Call stack:\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_4865/672928978.py\", line 202, in <module>\n",
      "    logger.error(f\"Error processing file {csv_file}: {str(e)}\", error=e)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 183, in error\n",
      "    self.log(logging.ERROR, message, **extra)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 168, in log\n",
      "    self.logger.log(level, message, extra=extra)\n",
      "Message: \"Error processing file 202502-citibike-tripdata_2.csv: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\"\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4865/672928978.py\", line 158, in <module>\n",
      "    .save(bronze_rides_path))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/sql/readwriter.py\", line 1398, in save\n",
      "    self._jwrite.save(path)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 175, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/logging/handlers.py\", line 677, in emit\n",
      "    s = self.makePickle(record)\n",
      "  File \"/opt/conda/lib/python3.10/logging/handlers.py\", line 649, in makePickle\n",
      "    s = pickle.dumps(d, 1)\n",
      "  File \"/opt/conda/lib/python3.10/copyreg.py\", line 76, in _reduce_ex\n",
      "    raise TypeError(f\"cannot pickle {cls.__name__!r} object\")\n",
      "TypeError: cannot pickle 'traceback' object\n",
      "Call stack:\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_4865/672928978.py\", line 202, in <module>\n",
      "    logger.error(f\"Error processing file {csv_file}: {str(e)}\", error=e)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 183, in error\n",
      "    self.log(logging.ERROR, message, **extra)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 168, in log\n",
      "    self.logger.log(level, message, extra=extra)\n",
      "Message: \"Error processing file 202502-citibike-tripdata_2.csv: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\"\n",
      "Arguments: ()\n",
      "2025-03-05 00:56:37,303 - ERROR - [nycbs.notebook.nycbs] - Error processing file 202502-citibike-tripdata_2.csv: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\n",
      "2025-03-05 00:56:37,328 - INFO - [nycbs.notebook.nycbs] - Processing 202501-citibike-tripdata_3.csv\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4865/672928978.py\", line 158, in <module>\n",
      "    .save(bronze_rides_path))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/sql/readwriter.py\", line 1398, in save\n",
      "    self._jwrite.save(path)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 175, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/logging/handlers.py\", line 73, in emit\n",
      "    if self.shouldRollover(record):\n",
      "  File \"/opt/conda/lib/python3.10/logging/handlers.py\", line 196, in shouldRollover\n",
      "    msg = \"%s\\n\" % self.format(record)\n",
      "  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 59, in format\n",
      "    return json.dumps({k: v for k, v in log_obj.items() if v is not None})\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 231, in dumps\n",
      "    return _default_encoder.encode(obj)\n",
      "  File \"/opt/conda/lib/python3.10/json/encoder.py\", line 199, in encode\n",
      "    chunks = self.iterencode(o, _one_shot=True)\n",
      "  File \"/opt/conda/lib/python3.10/json/encoder.py\", line 257, in iterencode\n",
      "    return _iterencode(o, 0)\n",
      "  File \"/opt/conda/lib/python3.10/json/encoder.py\", line 179, in default\n",
      "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
      "TypeError: Object of type traceback is not JSON serializable\n",
      "Call stack:\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_4865/672928978.py\", line 202, in <module>\n",
      "    logger.error(f\"Error processing file {csv_file}: {str(e)}\", error=e)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 183, in error\n",
      "    self.log(logging.ERROR, message, **extra)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 168, in log\n",
      "    self.logger.log(level, message, extra=extra)\n",
      "Message: \"Error processing file 202501-citibike-tripdata_3.csv: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\"\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4865/672928978.py\", line 158, in <module>\n",
      "    .save(bronze_rides_path))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/sql/readwriter.py\", line 1398, in save\n",
      "    self._jwrite.save(path)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 175, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/logging/handlers.py\", line 73, in emit\n",
      "    if self.shouldRollover(record):\n",
      "  File \"/opt/conda/lib/python3.10/logging/handlers.py\", line 196, in shouldRollover\n",
      "    msg = \"%s\\n\" % self.format(record)\n",
      "  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 59, in format\n",
      "    return json.dumps({k: v for k, v in log_obj.items() if v is not None})\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 231, in dumps\n",
      "    return _default_encoder.encode(obj)\n",
      "  File \"/opt/conda/lib/python3.10/json/encoder.py\", line 199, in encode\n",
      "    chunks = self.iterencode(o, _one_shot=True)\n",
      "  File \"/opt/conda/lib/python3.10/json/encoder.py\", line 257, in iterencode\n",
      "    return _iterencode(o, 0)\n",
      "  File \"/opt/conda/lib/python3.10/json/encoder.py\", line 179, in default\n",
      "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
      "TypeError: Object of type traceback is not JSON serializable\n",
      "Call stack:\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_4865/672928978.py\", line 202, in <module>\n",
      "    logger.error(f\"Error processing file {csv_file}: {str(e)}\", error=e)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 183, in error\n",
      "    self.log(logging.ERROR, message, **extra)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 168, in log\n",
      "    self.logger.log(level, message, extra=extra)\n",
      "Message: \"Error processing file 202501-citibike-tripdata_3.csv: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\"\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4865/672928978.py\", line 158, in <module>\n",
      "    .save(bronze_rides_path))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/sql/readwriter.py\", line 1398, in save\n",
      "    self._jwrite.save(path)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 175, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/logging/handlers.py\", line 677, in emit\n",
      "    s = self.makePickle(record)\n",
      "  File \"/opt/conda/lib/python3.10/logging/handlers.py\", line 649, in makePickle\n",
      "    s = pickle.dumps(d, 1)\n",
      "  File \"/opt/conda/lib/python3.10/copyreg.py\", line 76, in _reduce_ex\n",
      "    raise TypeError(f\"cannot pickle {cls.__name__!r} object\")\n",
      "TypeError: cannot pickle 'traceback' object\n",
      "Call stack:\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_4865/672928978.py\", line 202, in <module>\n",
      "    logger.error(f\"Error processing file {csv_file}: {str(e)}\", error=e)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 183, in error\n",
      "    self.log(logging.ERROR, message, **extra)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 168, in log\n",
      "    self.logger.log(level, message, extra=extra)\n",
      "Message: \"Error processing file 202501-citibike-tripdata_3.csv: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\"\n",
      "Arguments: ()\n",
      "2025-03-05 00:56:38,326 - ERROR - [nycbs.notebook.nycbs] - Error processing file 202501-citibike-tripdata_3.csv: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\n",
      "2025-03-05 00:56:38,358 - INFO - [nycbs.notebook.nycbs] - Processing 202501-citibike-tripdata_1.csv\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4865/672928978.py\", line 158, in <module>\n",
      "    .save(bronze_rides_path))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/sql/readwriter.py\", line 1398, in save\n",
      "    self._jwrite.save(path)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 175, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/logging/handlers.py\", line 73, in emit\n",
      "    if self.shouldRollover(record):\n",
      "  File \"/opt/conda/lib/python3.10/logging/handlers.py\", line 196, in shouldRollover\n",
      "    msg = \"%s\\n\" % self.format(record)\n",
      "  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 59, in format\n",
      "    return json.dumps({k: v for k, v in log_obj.items() if v is not None})\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 231, in dumps\n",
      "    return _default_encoder.encode(obj)\n",
      "  File \"/opt/conda/lib/python3.10/json/encoder.py\", line 199, in encode\n",
      "    chunks = self.iterencode(o, _one_shot=True)\n",
      "  File \"/opt/conda/lib/python3.10/json/encoder.py\", line 257, in iterencode\n",
      "    return _iterencode(o, 0)\n",
      "  File \"/opt/conda/lib/python3.10/json/encoder.py\", line 179, in default\n",
      "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
      "TypeError: Object of type traceback is not JSON serializable\n",
      "Call stack:\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_4865/672928978.py\", line 202, in <module>\n",
      "    logger.error(f\"Error processing file {csv_file}: {str(e)}\", error=e)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 183, in error\n",
      "    self.log(logging.ERROR, message, **extra)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 168, in log\n",
      "    self.logger.log(level, message, extra=extra)\n",
      "Message: \"Error processing file 202501-citibike-tripdata_1.csv: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\"\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4865/672928978.py\", line 158, in <module>\n",
      "    .save(bronze_rides_path))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/sql/readwriter.py\", line 1398, in save\n",
      "    self._jwrite.save(path)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 175, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/logging/handlers.py\", line 73, in emit\n",
      "    if self.shouldRollover(record):\n",
      "  File \"/opt/conda/lib/python3.10/logging/handlers.py\", line 196, in shouldRollover\n",
      "    msg = \"%s\\n\" % self.format(record)\n",
      "  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 59, in format\n",
      "    return json.dumps({k: v for k, v in log_obj.items() if v is not None})\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 231, in dumps\n",
      "    return _default_encoder.encode(obj)\n",
      "  File \"/opt/conda/lib/python3.10/json/encoder.py\", line 199, in encode\n",
      "    chunks = self.iterencode(o, _one_shot=True)\n",
      "  File \"/opt/conda/lib/python3.10/json/encoder.py\", line 257, in iterencode\n",
      "    return _iterencode(o, 0)\n",
      "  File \"/opt/conda/lib/python3.10/json/encoder.py\", line 179, in default\n",
      "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
      "TypeError: Object of type traceback is not JSON serializable\n",
      "Call stack:\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_4865/672928978.py\", line 202, in <module>\n",
      "    logger.error(f\"Error processing file {csv_file}: {str(e)}\", error=e)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 183, in error\n",
      "    self.log(logging.ERROR, message, **extra)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 168, in log\n",
      "    self.logger.log(level, message, extra=extra)\n",
      "Message: \"Error processing file 202501-citibike-tripdata_1.csv: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\"\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4865/672928978.py\", line 158, in <module>\n",
      "    .save(bronze_rides_path))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/sql/readwriter.py\", line 1398, in save\n",
      "    self._jwrite.save(path)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 175, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/logging/handlers.py\", line 677, in emit\n",
      "    s = self.makePickle(record)\n",
      "  File \"/opt/conda/lib/python3.10/logging/handlers.py\", line 649, in makePickle\n",
      "    s = pickle.dumps(d, 1)\n",
      "  File \"/opt/conda/lib/python3.10/copyreg.py\", line 76, in _reduce_ex\n",
      "    raise TypeError(f\"cannot pickle {cls.__name__!r} object\")\n",
      "TypeError: cannot pickle 'traceback' object\n",
      "Call stack:\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_4865/672928978.py\", line 202, in <module>\n",
      "    logger.error(f\"Error processing file {csv_file}: {str(e)}\", error=e)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 183, in error\n",
      "    self.log(logging.ERROR, message, **extra)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 168, in log\n",
      "    self.logger.log(level, message, extra=extra)\n",
      "Message: \"Error processing file 202501-citibike-tripdata_1.csv: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\"\n",
      "Arguments: ()\n",
      "2025-03-05 00:56:41,839 - ERROR - [nycbs.notebook.nycbs] - Error processing file 202501-citibike-tripdata_1.csv: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\n",
      "2025-03-05 00:56:41,872 - INFO - [nycbs.notebook.nycbs] - Processing 202502-citibike-tripdata_3.csv\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4865/672928978.py\", line 158, in <module>\n",
      "    .save(bronze_rides_path))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/sql/readwriter.py\", line 1398, in save\n",
      "    self._jwrite.save(path)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 175, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/logging/handlers.py\", line 73, in emit\n",
      "    if self.shouldRollover(record):\n",
      "  File \"/opt/conda/lib/python3.10/logging/handlers.py\", line 196, in shouldRollover\n",
      "    msg = \"%s\\n\" % self.format(record)\n",
      "  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 59, in format\n",
      "    return json.dumps({k: v for k, v in log_obj.items() if v is not None})\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 231, in dumps\n",
      "    return _default_encoder.encode(obj)\n",
      "  File \"/opt/conda/lib/python3.10/json/encoder.py\", line 199, in encode\n",
      "    chunks = self.iterencode(o, _one_shot=True)\n",
      "  File \"/opt/conda/lib/python3.10/json/encoder.py\", line 257, in iterencode\n",
      "    return _iterencode(o, 0)\n",
      "  File \"/opt/conda/lib/python3.10/json/encoder.py\", line 179, in default\n",
      "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
      "TypeError: Object of type traceback is not JSON serializable\n",
      "Call stack:\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_4865/672928978.py\", line 202, in <module>\n",
      "    logger.error(f\"Error processing file {csv_file}: {str(e)}\", error=e)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 183, in error\n",
      "    self.log(logging.ERROR, message, **extra)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 168, in log\n",
      "    self.logger.log(level, message, extra=extra)\n",
      "Message: \"Error processing file 202502-citibike-tripdata_3.csv: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\"\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4865/672928978.py\", line 158, in <module>\n",
      "    .save(bronze_rides_path))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/sql/readwriter.py\", line 1398, in save\n",
      "    self._jwrite.save(path)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 175, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/logging/handlers.py\", line 73, in emit\n",
      "    if self.shouldRollover(record):\n",
      "  File \"/opt/conda/lib/python3.10/logging/handlers.py\", line 196, in shouldRollover\n",
      "    msg = \"%s\\n\" % self.format(record)\n",
      "  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 59, in format\n",
      "    return json.dumps({k: v for k, v in log_obj.items() if v is not None})\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 231, in dumps\n",
      "    return _default_encoder.encode(obj)\n",
      "  File \"/opt/conda/lib/python3.10/json/encoder.py\", line 199, in encode\n",
      "    chunks = self.iterencode(o, _one_shot=True)\n",
      "  File \"/opt/conda/lib/python3.10/json/encoder.py\", line 257, in iterencode\n",
      "    return _iterencode(o, 0)\n",
      "  File \"/opt/conda/lib/python3.10/json/encoder.py\", line 179, in default\n",
      "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
      "TypeError: Object of type traceback is not JSON serializable\n",
      "Call stack:\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_4865/672928978.py\", line 202, in <module>\n",
      "    logger.error(f\"Error processing file {csv_file}: {str(e)}\", error=e)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 183, in error\n",
      "    self.log(logging.ERROR, message, **extra)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 168, in log\n",
      "    self.logger.log(level, message, extra=extra)\n",
      "Message: \"Error processing file 202502-citibike-tripdata_3.csv: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\"\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4865/672928978.py\", line 158, in <module>\n",
      "    .save(bronze_rides_path))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/sql/readwriter.py\", line 1398, in save\n",
      "    self._jwrite.save(path)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 175, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/logging/handlers.py\", line 677, in emit\n",
      "    s = self.makePickle(record)\n",
      "  File \"/opt/conda/lib/python3.10/logging/handlers.py\", line 649, in makePickle\n",
      "    s = pickle.dumps(d, 1)\n",
      "  File \"/opt/conda/lib/python3.10/copyreg.py\", line 76, in _reduce_ex\n",
      "    raise TypeError(f\"cannot pickle {cls.__name__!r} object\")\n",
      "TypeError: cannot pickle 'traceback' object\n",
      "Call stack:\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_4865/672928978.py\", line 202, in <module>\n",
      "    logger.error(f\"Error processing file {csv_file}: {str(e)}\", error=e)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 183, in error\n",
      "    self.log(logging.ERROR, message, **extra)\n",
      "  File \"/home/aldamiz/src/utils/notebook_logger.py\", line 168, in log\n",
      "    self.logger.log(level, message, extra=extra)\n",
      "Message: \"Error processing file 202502-citibike-tripdata_3.csv: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\"\n",
      "Arguments: ()\n",
      "2025-03-05 00:56:42,787 - ERROR - [nycbs.notebook.nycbs] - Error processing file 202502-citibike-tripdata_3.csv: Failed to merge fields 'started_at' and 'started_at'. Failed to merge incompatible data types StringType and TimestampType\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h4>üìä Bronze Layer Summary:</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_58f07_row0_col0, #T_58f07_row0_col1, #T_58f07_row0_col2 {\n",
       "  padding: 5px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_58f07\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_58f07_level0_col0\" class=\"col_heading level0 col0\" >year</th>\n",
       "      <th id=\"T_58f07_level0_col1\" class=\"col_heading level0 col1\" >month</th>\n",
       "      <th id=\"T_58f07_level0_col2\" class=\"col_heading level0 col2\" >records</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_58f07_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_58f07_row0_col0\" class=\"data row0 col0\" >2025</td>\n",
       "      <td id=\"T_58f07_row0_col1\" class=\"data row0 col1\" >1</td>\n",
       "      <td id=\"T_58f07_row0_col2\" class=\"data row0 col2\" >2,124,475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0xffa4ef149ae0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_delta_table(\n",
    "    table_name: str,\n",
    "    operation: str,\n",
    "    df: Optional[DataFrame] = None,\n",
    "    path: Optional[str] = None,\n",
    "    logger: Optional[object] = None\n",
    ") -> None:\n",
    "    \"\"\"Process a Delta table operation with proper logging and error handling.\"\"\"\n",
    "    try:\n",
    "        if logger:\n",
    "            logger.info(f\"Starting {operation} operation on {table_name}\")\n",
    "        \n",
    "        if operation.lower() == 'create':\n",
    "            if df is None or path is None:\n",
    "                raise ValueError(\"DataFrame and path are required for create operation\")\n",
    "            \n",
    "            # Write the DataFrame as a new Delta table\n",
    "            df.write.format(\"delta\").mode(\"overwrite\").save(path)\n",
    "            \n",
    "            # Get table details after creation\n",
    "            delta_table = DeltaTable.forPath(spark, path)\n",
    "            history = delta_table.history().first()\n",
    "            \n",
    "            metrics = {\n",
    "                \"operation\": operation,\n",
    "                \"table_name\": table_name,\n",
    "                \"timestamp\": history[\"timestamp\"],\n",
    "                \"operation_parameters\": history[\"operationParameters\"],\n",
    "                \"operation_metrics\": history[\"operationMetrics\"] if \"operationMetrics\" in history else {},\n",
    "                \"is_blind_append\": history[\"isBlindAppend\"] if \"isBlindAppend\" in history else False,\n",
    "                \"record_count\": df.count()\n",
    "            }\n",
    "            \n",
    "            if logger:\n",
    "                logger.info(\n",
    "                    f\"Successfully created Delta table {table_name}\",\n",
    "                    extra={\"metrics\": metrics}\n",
    "                )\n",
    "                \n",
    "        elif operation.lower() in ['update', 'merge']:\n",
    "            if path is None:\n",
    "                raise ValueError(\"Path is required for update/merge operation\")\n",
    "                \n",
    "            delta_table = DeltaTable.forPath(spark, path)\n",
    "            # Additional update/merge logic here\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported operation: {operation}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.error(\n",
    "                f\"Error in {operation} operation on {table_name}: {str(e)}\",\n",
    "                error=e\n",
    "            )\n",
    "        raise\n",
    "\n",
    "def monitor_query_performance(query_name: str, df: DataFrame, action: str = \"count\", metrics: Optional[dict] = None) -> None:\n",
    "    \"\"\"\n",
    "    Monitor Spark query performance with detailed metrics.\n",
    "    \n",
    "    Args:\n",
    "        query_name: Name/identifier for the query\n",
    "        df: Spark DataFrame to monitor\n",
    "        action: Action to perform (count/collect/write)\n",
    "        metrics: Additional metrics to track\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        result = None\n",
    "        \n",
    "        # Cache plan for metrics\n",
    "        df.cache()\n",
    "        plan_metrics = df._jdf.queryExecution().executedPlan().metrics()\n",
    "        \n",
    "        # Execute action\n",
    "        if action == \"count\":\n",
    "            result = df.count()\n",
    "        elif action == \"collect\":\n",
    "            result = df.collect()\n",
    "        elif action == \"write\":\n",
    "            result = df.write\n",
    "        \n",
    "        duration_ms = (time.time() - start_time) * 1000\n",
    "        \n",
    "         # Gather execution metrics - simplified version\n",
    "        execution_metrics = {\n",
    "            \"duration_ms\": duration_ms\n",
    "        }\n",
    "        \n",
    "        if metrics:\n",
    "            execution_metrics.update(metrics)\n",
    "        \n",
    "        logger.performance_metric(\n",
    "            f\"Executed query: {query_name}\",\n",
    "            duration_ms=duration_ms,\n",
    "            rows_processed=result if isinstance(result, int) else None,\n",
    "            query_metrics=execution_metrics,\n",
    "            query_plan=df._jdf.queryExecution().executedPlan().toString(),\n",
    "            data_schema=df.schema.json()\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            f\"Query execution failed: {query_name}\",\n",
    "            error=e,\n",
    "            query_name=query_name,\n",
    "            action=action\n",
    "        )\n",
    "        raise\n",
    "\n",
    "# Main processing code\n",
    "bronze_rides_path = os.path.join(BRONZE_PATH, \"rides_nyc\")\n",
    "landing_path = os.path.join(DATA_DIR, \"landing\")\n",
    "\n",
    "try:\n",
    "    # Get list of CSV files to process\n",
    "    csv_files = [f for f in os.listdir(landing_path) if f.endswith('.csv')]\n",
    "    logger.info(f\"Found {len(csv_files)} CSV files to process\")\n",
    "    \n",
    "    if not csv_files:\n",
    "        display(HTML('<div style=\"color: #31708f;\">‚ÑπÔ∏è No CSV files found in landing zone</div>'))\n",
    "    else:\n",
    "        # Read control table\n",
    "        control_df = spark.read.format(\"delta\").load(control_table_path)\n",
    "        processed_files = set(control_df.filter(F.col(\"status\") == \"SUCCESS\")\n",
    "                            .select(\"source_file\").toPandas()[\"source_file\"])\n",
    "        \n",
    "        new_files = [f for f in csv_files if f not in processed_files]\n",
    "        logger.info(f\"Found {len(new_files)} new files to process\")\n",
    "        \n",
    "        for csv_file in new_files:\n",
    "            try:\n",
    "                file_path = os.path.join(landing_path, csv_file)\n",
    "                logger.info(f\"Processing {csv_file}\")\n",
    "                \n",
    "                # Read CSV file\n",
    "                df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "                \n",
    "                # Add year and month columns from started_at with explicit casting\n",
    "                bronze_df = df.withColumn(\"year\", F.year(\"started_at\").cast(\"integer\")) \\\n",
    "                             .withColumn(\"month\", F.month(\"started_at\").cast(\"integer\"))\n",
    "                \n",
    "                record_count = bronze_df.count()\n",
    "                \n",
    "                # Extract year and month from the data\n",
    "                year_month = bronze_df.select(\n",
    "                    F.first(\"year\").alias(\"year\"),\n",
    "                    F.first(\"month\").alias(\"month\")\n",
    "                ).first()\n",
    "                \n",
    "                # Write to bronze layer\n",
    "                (bronze_df.write.format(\"delta\")\n",
    "                          .mode(\"append\")\n",
    "                          .partitionBy(\"year\", \"month\")\n",
    "                          .save(bronze_rides_path))\n",
    "                \n",
    "                # Monitor the bronze write performance\n",
    "                monitor_query_performance(\n",
    "                    query_name=f\"bronze_write_{csv_file}\",\n",
    "                    df=bronze_df,\n",
    "                    action=\"write\",\n",
    "                    metrics={\n",
    "                        \"layer\": \"bronze\",\n",
    "                        \"operation\": \"write\",\n",
    "                        \"file\": csv_file,\n",
    "                        \"record_count\": record_count\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                # Update control table with explicit integer casting\n",
    "                control_data = [(\n",
    "                    csv_file,\n",
    "                    bronze_rides_path,\n",
    "                    datetime.now(),\n",
    "                    record_count,\n",
    "                    \"SUCCESS\",\n",
    "                    int(year_month[\"year\"]),\n",
    "                    int(year_month[\"month\"])\n",
    "                )]\n",
    "                \n",
    "                control_schema = StructType([\n",
    "                    StructField(\"source_file\", StringType(), False),\n",
    "                    StructField(\"file_path\", StringType(), False),\n",
    "                    StructField(\"processing_date\", TimestampType(), False),\n",
    "                    StructField(\"record_count\", LongType(), True),\n",
    "                    StructField(\"status\", StringType(), False),\n",
    "                    StructField(\"year\", IntegerType(), False),\n",
    "                    StructField(\"month\", IntegerType(), False)\n",
    "                ])\n",
    "                \n",
    "                spark.createDataFrame(\n",
    "                    control_data,\n",
    "                    schema=control_schema\n",
    "                ).write.format(\"delta\").mode(\"append\").save(control_table_path)\n",
    "                \n",
    "                logger.info(f\"Processed {csv_file}: {record_count:,} records\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing file {csv_file}: {str(e)}\", error=e)\n",
    "                continue\n",
    "    \n",
    "    # Display processing summary if bronze table exists\n",
    "    display(HTML(\"<h4>üìä Bronze Layer Summary:</h4>\"))\n",
    "    \n",
    "    try:\n",
    "        bronze_df = spark.read.format(\"delta\").load(bronze_rides_path)\n",
    "        summary = (bronze_df.groupBy(\"year\", \"month\")\n",
    "                  .agg(F.count(\"*\").alias(\"records\"))\n",
    "                  .orderBy(\"year\", \"month\"))\n",
    "        \n",
    "        if summary.count() > 0:\n",
    "            display(summary.toPandas().style\n",
    "                   .set_properties(**{'padding': '5px'})\n",
    "                   .format({'records': '{:,}'}))\n",
    "        else:\n",
    "            display(HTML('<div style=\"color: #31708f;\">‚ÑπÔ∏è No data in bronze layer yet</div>'))\n",
    "    except Exception as e:\n",
    "        if \"Path does not exist\" in str(e):\n",
    "            display(HTML('<div style=\"color: #31708f;\">‚ÑπÔ∏è Bronze layer not created yet - waiting for first file</div>'))\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in bronze processing: {str(e)}\", error=e)\n",
    "    display(HTML(f'<div style=\"color: #a94442;\">‚ùå Error: {str(e)}</div>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>üîç December 2024 Records Sample:</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_77029\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_77029_level0_col0\" class=\"col_heading level0 col0\" >ride_id</th>\n",
       "      <th id=\"T_77029_level0_col1\" class=\"col_heading level0 col1\" >started_at</th>\n",
       "      <th id=\"T_77029_level0_col2\" class=\"col_heading level0 col2\" >ended_at</th>\n",
       "      <th id=\"T_77029_level0_col3\" class=\"col_heading level0 col3\" >year</th>\n",
       "      <th id=\"T_77029_level0_col4\" class=\"col_heading level0 col4\" >month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0xffa4ef0f5e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4>üìä Date Distribution:</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_03c85\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_03c85_level0_col0\" class=\"col_heading level0 col0\" >date</th>\n",
       "      <th id=\"T_03c85_level0_col1\" class=\"col_heading level0 col1\" >records</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0xffa4e5c3f610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Examine records from December 2024\n",
    "bronze_rides_path = os.path.join(BRONZE_PATH, \"rides_nyc\")\n",
    "\n",
    "try:\n",
    "    # Read the bronze table\n",
    "    df = spark.read.format(\"delta\").load(bronze_rides_path)\n",
    "    \n",
    "    # Query December 2024 records\n",
    "    dec_2024_df = df.filter((F.col(\"year\") == 2024) & (F.col(\"month\") == 12))\n",
    "    \n",
    "    # Display sample records with timestamp information\n",
    "    display(HTML(\"<h4>üîç December 2024 Records Sample:</h4>\"))\n",
    "    \n",
    "    sample_records = dec_2024_df.select(\n",
    "        \"ride_id\",\n",
    "        \"started_at\",\n",
    "        \"ended_at\",\n",
    "        \"year\",\n",
    "        \"month\"\n",
    "    ).limit(10)\n",
    "    \n",
    "    display(sample_records.toPandas().style\n",
    "           .set_properties(**{'padding': '5px'})\n",
    "           .format({'started_at': lambda x: x.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                   'ended_at': lambda x: x.strftime('%Y-%m-%d %H:%M:%S')}))\n",
    "    \n",
    "    # Get distribution of dates\n",
    "    display(HTML(\"<h4>üìä Date Distribution:</h4>\"))\n",
    "    \n",
    "    date_dist = dec_2024_df.select(\n",
    "        F.date_format(\"started_at\", \"yyyy-MM-dd\").alias(\"date\")\n",
    "    ).groupBy(\"date\").agg(\n",
    "        F.count(\"*\").alias(\"records\")\n",
    "    ).orderBy(\"date\")\n",
    "    \n",
    "    display(date_dist.toPandas().style\n",
    "           .set_properties(**{'padding': '5px'})\n",
    "           .format({'records': '{:,}'}))\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error querying December 2024 records: {str(e)}\", exc_info=True)\n",
    "    display(HTML(f'<div style=\"color: #a94442;\">‚ùå Error: {str(e)}</div>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+--------------------+--------------------+------------------+----------------+----------------+--------------+---------+-----------+-------+-------+-------------+--------------------+--------------------+----+----+-----+\n",
      "|         ride_id|rideable_type|          started_at|            ended_at|start_station_name|start_station_id|end_station_name|end_station_id|start_lat|  start_lng|end_lat|end_lng|member_casual|      ingestion_date|         source_file|city|year|month|\n",
      "+----------------+-------------+--------------------+--------------------+------------------+----------------+----------------+--------------+---------+-----------+-------+-------+-------------+--------------------+--------------------+----+----+-----+\n",
      "|8A37006C7A32066B| classic_bike|2024-12-30 23:41:...|2025-01-01 00:41:...|    31 Ave & 30 St|         6857.09|            null|          null|  40.7647|-73.9240312|  40.75| -73.95|       member|2025-03-05 00:01:...|202501-citibike-t...| nyc|2025|    1|\n",
      "+----------------+-------------+--------------------+--------------------+------------------+----------------+----------------+--------------+---------+-----------+-------+-------+-------------+--------------------+--------------------+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bronze_df.filter(F.col(\"started_at\") <= F.lit(\"2024-12-31\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•à Silver Layer Processing\n",
    " \n",
    " ## Purpose\n",
    " The silver layer enriches the raw data with:\n",
    " - ‚è±Ô∏è Ride duration calculations\n",
    " - üìè Distance calculations\n",
    " - üïí Time-based features\n",
    " - üå°Ô∏è Seasonal patterns\n",
    " - ‚ú® Data quality improvements\n",
    " \n",
    " ## Feature Engineering\n",
    " ### Time-Based Features\n",
    " - `ride_duration_sec`: Duration in seconds\n",
    " - `start_hour`: Hour of day (0-23)\n",
    " - `is_weekend`: Boolean flag\n",
    " - `season`: Winter, Spring, Summer, Fall\n",
    " - `time_of_day`: Morning, Afternoon, Evening, Night\n",
    " \n",
    " ### Geographic Features\n",
    " - `ride_distance_km`: Great circle distance in kilometers\n",
    " \n",
    " ## Data Quality Rules\n",
    " 1. Duration between 1 minute and 24 hours\n",
    " 2. Distance between 0.1 and 100 km\n",
    " 3. Coordinates within NYC metropolitan area\n",
    " \n",
    " ## Analysis Views\n",
    " - üå§Ô∏è Seasonal patterns\n",
    " - ‚è∞ Time of day distribution\n",
    " - üìÖ Weekend vs weekday comparison\n",
    " - üìä Distance and duration statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Silver Layer Transformations"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 00:56:48,792 - INFO - [nycbs.notebook.nycbs] - Starting silver layer transformations\n",
      "2025-03-05 00:56:49,051 - INFO - [nycbs.notebook.nycbs] - Read 2,124,475 records from bronze layer\n",
      "2025-03-05 00:56:49,323 - INFO - [nycbs.notebook.nycbs] - Data quality metrics for silver_transformation:\n",
      "2025-03-05 00:56:49,549 - INFO - [nycbs.notebook.nycbs] - - Row count: 2124475\n",
      "2025-03-05 00:56:53,516 - INFO - [nycbs.notebook.nycbs] - - Null counts: {'ride_id': 0, 'rideable_type': 0, 'started_at': 0, 'ended_at': 0, 'start_station_name': 564, 'start_station_id': 564, 'end_station_name': 3975, 'end_station_id': 4322, 'start_lat': 0, 'start_lng': 0, 'end_lat': 269, 'end_lng': 269, 'member_casual': 0, 'ingestion_date': 0, 'source_file': 0, 'city': 0, 'year': 0, 'month': 0, 'ride_duration_minutes': 0, 'ride_distance_km': 269, 'speed_kmh': 269, 'part_of_day': 0, 'is_weekend': 0, 'season': 0}\n",
      "2025-03-05 00:58:00,635 - INFO - [nycbs.notebook.nycbs] - - Duplicate count: 0\n",
      "2025-03-05 00:58:39,623 - INFO - [nycbs.notebook.nycbs] - Executed query: silver_enrichment\n",
      "2025-03-05 00:58:48,063 - INFO - [nycbs.notebook.nycbs] - Performance metric - silver_write: 47.36 seconds\n",
      "2025-03-05 00:58:48,069 - INFO - [nycbs.notebook.nycbs] - Silver layer written to: /home/aldamiz/data/silver/rides_nyc\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "‚úÖ Silver layer processing complete"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Sample of Processed Data"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------------------+----------------+---------+-----------+----------+------+\n",
      "|          started_at|            ended_at|ride_duration_minutes|ride_distance_km|speed_kmh|part_of_day|is_weekend|season|\n",
      "+--------------------+--------------------+---------------------+----------------+---------+-----------+----------+------+\n",
      "|2025-01-16 07:08:...|2025-01-16 07:13:...|                 5.35|            1.18|    13.23|    morning|     false|winter|\n",
      "|2025-01-24 21:47:...|2025-01-24 21:55:...|                 8.33|            1.82|    13.11|    evening|     false|winter|\n",
      "|2025-01-18 12:57:...|2025-01-18 13:06:...|                 9.23|            2.01|    13.07|  afternoon|      true|winter|\n",
      "|2025-01-21 08:45:...|2025-01-21 08:53:...|                  8.6|            1.75|    12.21|    morning|     false|winter|\n",
      "|2025-01-31 10:13:...|2025-01-31 10:31:...|                17.97|             5.0|    16.69|    morning|     false|winter|\n",
      "+--------------------+--------------------+---------------------+----------------+---------+-----------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Silver Layer Processing\n",
    "display(Markdown(\"## Silver Layer Transformations\"))\n",
    "logger.info(\"Starting silver layer transformations\")\n",
    "\n",
    "# Logging helper functions\n",
    "def log_data_quality(df, stage):\n",
    "    \"\"\"Log data quality metrics for a DataFrame\"\"\"\n",
    "    logger.info(f\"Data quality metrics for {stage}:\")\n",
    "    logger.info(f\"- Row count: {df.count()}\")\n",
    "    logger.info(f\"- Null counts: {df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).collect()[0].asDict()}\")\n",
    "    logger.info(f\"- Duplicate count: {df.count() - df.distinct().count()}\")\n",
    "\n",
    "def log_performance_metrics(start_time, operation_name):\n",
    "    \"\"\"Log performance metrics for an operation\"\"\"\n",
    "    duration = time.time() - start_time\n",
    "    logger.info(f\"Performance metric - {operation_name}: {duration:.2f} seconds\")\n",
    "\n",
    "try:\n",
    "    # Set time parser policy\n",
    "    spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "    # Read from bronze\n",
    "    bronze_df = spark.read.format(\"delta\").load(os.path.join(BRONZE_PATH, \"rides_nyc\"))\n",
    "    logger.info(f\"Read {bronze_df.count():,} records from bronze layer\")\n",
    "\n",
    "    # Create a new DataFrame with proper types instead of modifying existing ones\n",
    "    silver_df = bronze_df.select(\n",
    "        \"*\",  # Keep all original columns\n",
    "        F.to_timestamp(\"started_at\").alias(\"started_at_clean\"),\n",
    "        F.to_timestamp(\"ended_at\").alias(\"ended_at_clean\"),\n",
    "        # Cast coordinates to double for calculations but keep original string columns\n",
    "        F.col(\"start_lat\").cast(\"double\").alias(\"start_lat_double\"),\n",
    "        F.col(\"start_lng\").cast(\"double\").alias(\"start_lng_double\"),\n",
    "        F.col(\"end_lat\").cast(\"double\").alias(\"end_lat_double\"),\n",
    "        F.col(\"end_lng\").cast(\"double\").alias(\"end_lng_double\")\n",
    "    )\n",
    "\n",
    "    # Calculate enriched features using the double-type columns\n",
    "    silver_df = silver_df.withColumn(\n",
    "        \"ride_duration_minutes\", \n",
    "        F.round(\n",
    "            (F.unix_timestamp(\"ended_at_clean\") - F.unix_timestamp(\"started_at_clean\")) / 60, \n",
    "            2\n",
    "        )\n",
    "    ).withColumn(\n",
    "        \"ride_distance_km\", \n",
    "        F.round(\n",
    "            F.acos(\n",
    "                F.sin(F.radians(\"start_lat_double\")) * F.sin(F.radians(\"end_lat_double\")) + \n",
    "                F.cos(F.radians(\"start_lat_double\")) * F.cos(F.radians(\"end_lat_double\")) * \n",
    "                F.cos(F.radians(\"start_lng_double\") - F.radians(\"end_lng_double\"))\n",
    "            ) * 6371,\n",
    "            2\n",
    "        )\n",
    "    ).withColumn(\n",
    "        \"speed_kmh\",\n",
    "        F.round(F.col(\"ride_distance_km\") / (F.col(\"ride_duration_minutes\") / 60), 2)\n",
    "    ).withColumn(\n",
    "        \"part_of_day\",\n",
    "        F.when(F.hour(\"started_at_clean\").between(5, 11), \"morning\")\n",
    "         .when(F.hour(\"started_at_clean\").between(12, 16), \"afternoon\")\n",
    "         .when(F.hour(\"started_at_clean\").between(17, 21), \"evening\")\n",
    "         .otherwise(\"night\")\n",
    "    ).withColumn(\n",
    "        \"is_weekend\",\n",
    "        F.dayofweek(\"started_at_clean\").isin([1, 7])\n",
    "    ).withColumn(\n",
    "        \"season\",\n",
    "        F.when(F.month(\"started_at_clean\").isin([12, 1, 2]), \"winter\")\n",
    "         .when(F.month(\"started_at_clean\").isin([3, 4, 5]), \"spring\")\n",
    "         .when(F.month(\"started_at_clean\").isin([6, 7, 8]), \"summer\")\n",
    "         .otherwise(\"fall\")\n",
    "    )\n",
    "\n",
    "    # Drop temporary columns\n",
    "    silver_df = silver_df.drop(\n",
    "        \"started_at_clean\", \"ended_at_clean\",\n",
    "        \"start_lat_double\", \"start_lng_double\",\n",
    "        \"end_lat_double\", \"end_lng_double\"\n",
    "    )\n",
    "\n",
    "    # Log data quality metrics\n",
    "    log_data_quality(silver_df, \"silver_transformation\")\n",
    "\n",
    "    # Write to silver layer with schema evolution\n",
    "    silver_path = os.path.join(SILVER_PATH, \"rides_nyc\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Monitor wiritng writing\n",
    "    process_delta_table(\n",
    "        table_name=\"rides_enriched\",\n",
    "        operation=\"create\",\n",
    "        df=silver_df,\n",
    "        path=os.path.join(SILVER_PATH, \"rides_enriched\")\n",
    "    )\n",
    "\n",
    "    # Monitor the enrichment query performance\n",
    "    monitor_query_performance(\n",
    "        query_name=\"silver_enrichment\",\n",
    "        df=silver_df,\n",
    "        action=\"count\",\n",
    "        metrics={\n",
    "            \"layer\": \"silver\",\n",
    "            \"null_stations\": silver_df.filter(F.col(\"start_station_name\").isNull()).count(),\n",
    "            \"avg_duration\": silver_df.select(F.avg(\"ride_duration_minutes\")).first()[0]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    (silver_df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .partitionBy(\"year\", \"month\")\n",
    "        .save(silver_path))\n",
    "\n",
    "    log_performance_metrics(start_time, \"silver_write\")\n",
    "    logger.info(f\"Silver layer written to: {silver_path}\")\n",
    "    display(Markdown(\"‚úÖ Silver layer processing complete\"))\n",
    "\n",
    "    # Display sample of processed data\n",
    "    display(Markdown(\"### Sample of Processed Data\"))\n",
    "    spark.read.format(\"delta\").load(silver_path).select(\n",
    "        \"started_at\", \"ended_at\", \n",
    "        \"ride_duration_minutes\", \"ride_distance_km\", \n",
    "        \"speed_kmh\", \"part_of_day\", \"is_weekend\", \"season\"\n",
    "    ).limit(5).show()\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in silver transformation: {str(e)}\", exc_info=True)\n",
    "    display(Markdown(f\"‚ùå Error: {str(e)}\"))\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèÜ Gold Layer Analytics\n",
    " \n",
    " ## Purpose\n",
    " The gold layer provides business insights through:\n",
    " - üí∞ Revenue potential analysis\n",
    " - üìè Trip distance patterns\n",
    " - üöâ Station utilization metrics\n",
    " - ‚è∞ Temporal usage patterns\n",
    " - üõ£Ô∏è Popular route identification\n",
    " \n",
    " ## Analysis Components\n",
    " ### Revenue Metrics\n",
    " - `total_trips`: Total number of rides\n",
    " - `charged_trips`: Rides > 30 minutes\n",
    " - `extra_time_charge`: $0.2 for rides > 30 min\n",
    " - `revenue_potential`: Expected revenue from charges\n",
    " \n",
    " ### Distance Categories\n",
    " - `0-1 km`: Short neighborhood trips\n",
    " - `1-4 km`: Medium-range trips\n",
    " - `4-9 km`: Long-distance trips\n",
    " - `10+ km`: Extended journeys\n",
    " \n",
    " ### Station Analytics\n",
    " - `total_starts`: Rides starting at station\n",
    " - `unique_destinations`: Different end points\n",
    " - `avg_duration`: Mean trip duration\n",
    " - `avg_distance`: Mean trip distance\n",
    " \n",
    " ### Time Patterns\n",
    " - üåÖ Time of day analysis\n",
    " - üìÖ Weekend vs weekday comparison\n",
    " - üå§Ô∏è Seasonal trends\n",
    " - üìä Usage distribution\n",
    " \n",
    " ## Business Insights\n",
    " 1. Revenue optimization opportunities\n",
    " 2. Distance-based service planning\n",
    " 3. Station capacity management\n",
    " 4. Peak usage optimization\n",
    " 5. Popular route enhancement\n",
    " \n",
    " # Analysis Parameters\n",
    " ANALYSIS_YEAR = 2025\n",
    " ANALYSIS_MONTH = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (1256574778.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 14\u001b[0;36m\u001b[0m\n\u001b[0;31m    )\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unmatched ')'\n"
     ]
    }
   ],
   "source": [
    "# Set analysis parameters\n",
    "ANALYSIS_YEAR = 2025\n",
    "ANALYSIS_MONTH = 1\n",
    "GOLD_PATH = \"/home/aldamiz/data/gold\"\n",
    "gold_base_path = os.path.join(GOLD_PATH, \"analytics\")\n",
    "\n",
    "# %%\n",
    "# Gold Layer Processing\n",
    "display(Markdown(\"# üèÜ Gold Layer Analytics\"))\n",
    "\n",
    "try:\n",
    "    # Define gold layer paths\n",
    "    gold_base_path = os.path.join(GOLD_PATH, \"analytics\")\n",
    "    )\n",
    "\n",
    "    monitor_query_performance(\n",
    "        query_name=\"revenue_metrics_calculation\",\n",
    "        df=revenue_metrics,\n",
    "        action=\"count\",\n",
    "        metrics={\"layer\": \"gold\", \"metric_type\": \"revenue\"}\n",
    "    )\n",
    "            F.round(F.avg(\"ride_duration_minutes\"), 2).alias(\"avg_duration_min\")\n",
    "        ))\n",
    "    \n",
    "    # For revenue metrics\n",
    "    process_delta_table(\n",
    "        table_name=\"revenue_metrics\",\n",
    "        operation=\"create\",\n",
    "        df=revenue_metrics,\n",
    "        path=f\"{gold_base_path}/revenue_metrics\"\n",
    "    )\n",
    "\n",
    "    monitor_query_performance(\n",
    "        query_name=\"revenue_metrics_calculation\",\n",
    "        df=revenue_metrics,\n",
    "        action=\"count\",\n",
    "        metrics={\"layer\": \"gold\", \"metric_type\": \"revenue\"}\n",
    "    )\n",
    "\n",
    "    # Write revenue metrics\n",
    "    (revenue_metrics.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .partitionBy(\"year\", \"month\")\n",
    "        .save(f\"{gold_base_path}/revenue_metrics\"))\n",
    "    \n",
    "    process_delta_table(\n",
    "        table_name=\"distance_metrics\",\n",
    "        operation=\"create\",\n",
    "        df=distance_metrics,\n",
    "        path=f\"{gold_base_path}/distance_metrics\"\n",
    "    )\n",
    "             .when(F.col(\"ride_distance_km\").between(4, 9), \"4-9 km\")\n",
    "             .otherwise(\"10+ km\")\n",
    "        )\n",
    "        .groupBy(\"year\", \"month\", \"distance_bucket\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"trip_count\"),\n",
    "            F.round(F.avg(\"ride_duration_minutes\"), 1).alias(\"avg_duration_min\"),\n",
    "            F.round(F.sum(F.when(F.col(\"ride_duration_minutes\") > 30, 0.2).otherwise(0.0)), 2).alias(\"revenue_usd\")\n",
    "        ))\n",
    "    \n",
    "    # For distance metrics\n",
    "    process_delta_table(\n",
    "        table_name=\"distance_metrics\",\n",
    "        operation=\"create\",\n",
    "        df=distance_metrics,\n",
    "        path=f\"{gold_base_path}/distance_metrics\"\n",
    "    )\n",
    "\n",
    "    monitor_query_performance(\n",
    "        query_name=\"distance_metrics_calculation\",\n",
    "        df=distance_metrics,\n",
    "        action=\"count\",\n",
    "        metrics={\"layer\": \"gold\", \"metric_type\": \"distance\"}\n",
    "    )\n",
    "\n",
    "    # Write distance metrics\n",
    "    (distance_metrics.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .partitionBy(\"year\", \"month\")\n",
    "        .save(f\"{gold_base_path}/distance_metrics\"))\n",
    "    \n",
    "    # 3. Station Metrics Table\n",
    "    station_metrics = (silver_df\n",
    "        .groupBy(\"year\", \"month\", \"start_station_name\", \"start_lat\", \"start_lng\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"total_starts\"),\n",
    "            F.round(F.avg(\"ride_duration_minutes\"), 2).alias(\"avg_duration_min\"),\n",
    "            F.round(F.avg(\"ride_distance_km\"), 2).alias(\"avg_distance_km\"),\n",
    "            F.countDistinct(\"end_station_name\").alias(\"unique_destinations\")\n",
    "        ))\n",
    "    \n",
    "    # For station metrics\n",
    "    process_delta_table(\n",
    "        table_name=\"station_metrics\",\n",
    "        operation=\"create\",\n",
    "        df=station_metrics,\n",
    "        path=f\"{gold_base_path}/station_metrics\"\n",
    "    )\n",
    "\n",
    "    monitor_query_performance(\n",
    "        query_name=\"station_metrics_calculation\",\n",
    "        df=station_metrics,\n",
    "        action=\"count\",\n",
    "        metrics={\"layer\": \"gold\", \"metric_type\": \"stations\"}\n",
    "    )\n",
    "    \n",
    "    # Write station metrics\n",
    "    (station_metrics.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .partitionBy(\"year\", \"month\")\n",
    "        .save(f\"{gold_base_path}/station_metrics\"))\n",
    "    \n",
    "    # 4. Temporal Metrics Table\n",
    "    temporal_metrics = (silver_df\n",
    "        .groupBy(\"year\", \"month\", \"part_of_day\", \"is_weekend\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"total_rides\"),\n",
    "            F.round(F.avg(\"ride_duration_minutes\"), 2).alias(\"avg_duration_min\"),\n",
    "            F.round(F.avg(\"ride_distance_km\"), 2).alias(\"avg_distance_km\"),\n",
    "            F.round(F.avg(\"speed_kmh\"), 2).alias(\"avg_speed_kmh\")\n",
    "        ))\n",
    "    \n",
    "    # For temporal metrics\n",
    "    process_delta_table(\n",
    "        table_name=\"temporal_metrics\",\n",
    "        operation=\"create\",\n",
    "        df=temporal_metrics,\n",
    "        path=f\"{gold_base_path}/temporal_metrics\"\n",
    "    )\n",
    "\n",
    "    monitor_query_performance(\n",
    "        query_name=\"temporal_metrics_calculation\",\n",
    "        df=temporal_metrics,\n",
    "        action=\"count\",\n",
    "        metrics={\"layer\": \"gold\", \"metric_type\": \"temporal\"}\n",
    "    )\n",
    "\n",
    "    # Write temporal metrics\n",
    "    (temporal_metrics.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .partitionBy(\"year\", \"month\")\n",
    "        .save(f\"{gold_base_path}/temporal_metrics\"))\n",
    "    \n",
    "    # 5. Popular Routes Table\n",
    "    route_metrics = (silver_df\n",
    "        .groupBy(\"year\", \"month\", \"start_station_name\", \"end_station_name\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"route_count\"),\n",
    "            F.round(F.avg(\"ride_distance_km\"), 2).alias(\"avg_distance_km\"),\n",
    "            F.round(F.avg(\"ride_duration_minutes\"), 2).alias(\"avg_duration_min\"),\n",
    "            F.round(F.avg(\"speed_kmh\"), 2).alias(\"avg_speed_kmh\")\n",
    "        ))\n",
    "    \n",
    "    # For route metrics\n",
    "    process_delta_table(\n",
    "        table_name=\"route_metrics\",\n",
    "        operation=\"create\",\n",
    "        df=route_metrics,\n",
    "        path=f\"{gold_base_path}/route_metrics\"\n",
    "    )\n",
    "\n",
    "    monitor_query_performance(\n",
    "        query_name=\"route_metrics_calculation\",\n",
    "        df=route_metrics,\n",
    "        action=\"count\",\n",
    "        metrics={\"layer\": \"gold\", \"metric_type\": \"routes\"}\n",
    "    )\n",
    "\n",
    "    # Write route metrics\n",
    "    (route_metrics.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .partitionBy(\"year\", \"month\")\n",
    "        .save(f\"{gold_base_path}/route_metrics\"))\n",
    "    \n",
    "    display(Markdown(\"‚úÖ Gold layer tables created successfully\"))\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in gold layer processing: {str(e)}\", exc_info=True)\n",
    "    display(Markdown(f\"‚ùå Error: {str(e)}\"))\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. üí∞ Revenue Analysis\n",
    "display(Markdown(\"\"\"\n",
    "### üí∞ Revenue Analysis\n",
    "Understanding revenue potential and trip duration patterns to optimize pricing strategy.\n",
    "\"\"\"))\n",
    "\n",
    "try:\n",
    "    revenue_df = (spark.read.format(\"delta\")\n",
    "        .load(f\"{gold_base_path}/revenue_metrics\")\n",
    "        .filter(F.col(\"year\") == ANALYSIS_YEAR)\n",
    "        .filter(F.col(\"month\") == ANALYSIS_MONTH))\n",
    "    \n",
    "    # Convert to pandas for visualization\n",
    "    revenue_pd = revenue_df.toPandas()\n",
    "    \n",
    "    # Create a composite visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Revenue breakdown pie chart\n",
    "    revenue_data = {\n",
    "        'Regular Trips': revenue_pd['total_trips'].iloc[0] - revenue_pd['charged_trips'].iloc[0],\n",
    "        'Charged Trips (>30min)': revenue_pd['charged_trips'].iloc[0]\n",
    "    }\n",
    "    ax1.pie(revenue_data.values(), labels=revenue_data.keys(), autopct='%1.1f%%')\n",
    "    ax1.set_title('Trip Duration Distribution')\n",
    "    \n",
    "    # Revenue metrics\n",
    "    metrics_text = f\"\"\"\n",
    "    Key Metrics:\n",
    "    ‚Ä¢ Total Trips: {revenue_pd['total_trips'].iloc[0]:,.0f}\n",
    "    ‚Ä¢ Charged Trips: {revenue_pd['charged_trips'].iloc[0]:,.0f}\n",
    "    ‚Ä¢ Revenue Potential: ${revenue_pd['revenue_potential'].iloc[0]:,.2f}\n",
    "    ‚Ä¢ Avg Duration: {revenue_pd['avg_duration_min'].iloc[0]:.1f} min\n",
    "    \"\"\"\n",
    "    ax2.text(0.1, 0.5, metrics_text, fontsize=12, va='center')\n",
    "    ax2.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error in revenue analysis: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. üìè Distance Analysis\n",
    "display(Markdown(\"\"\"\n",
    "### üìè Distance Distribution Analysis\n",
    "Analyzing trip distances to understand service coverage and usage patterns.\n",
    "\"\"\"))\n",
    "\n",
    "try:\n",
    "    distance_df = (spark.read.format(\"delta\")\n",
    "        .load(f\"{gold_base_path}/distance_metrics\")\n",
    "        .filter(F.col(\"year\") == ANALYSIS_YEAR)\n",
    "        .filter(F.col(\"month\") == ANALYSIS_MONTH))\n",
    "    \n",
    "    # Convert to pandas and sort buckets correctly\n",
    "    distance_pd = distance_df.toPandas()\n",
    "    bucket_order = [\"0-1 km\", \"1-4 km\", \"4-9 km\", \"10+ km\"]\n",
    "    distance_pd['distance_bucket'] = pd.Categorical(distance_pd['distance_bucket'], categories=bucket_order)\n",
    "    distance_pd = distance_pd.sort_values('distance_bucket')\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    bars = ax.bar(distance_pd['distance_bucket'], distance_pd['trip_count'])\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(height):,}',\n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    plt.title('Trip Distribution by Distance')\n",
    "    plt.xlabel('Distance Bucket')\n",
    "    plt.ylabel('Number of Trips')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display detailed metrics\n",
    "    display(Markdown(\"#### Detailed Metrics by Distance Bucket\"))\n",
    "    display(distance_pd)\n",
    "except Exception as e:\n",
    "    print(f\"Error in distance analysis: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. üöâ Station Analysis\n",
    "display(Markdown(\"\"\"\n",
    "### üöâ Station Utilization Analysis\n",
    "Identifying high-traffic stations and their characteristics.\n",
    "Note: Some stations may have missing data (1,237 null start stations, 8,429 null end stations)\n",
    "\"\"\"))\n",
    "\n",
    "try:\n",
    "    station_df = (spark.read.format(\"delta\")\n",
    "        .load(f\"{gold_base_path}/station_metrics\")\n",
    "        .filter(F.col(\"year\") == ANALYSIS_YEAR)\n",
    "        .filter(F.col(\"month\") == ANALYSIS_MONTH)\n",
    "        .filter(F.col(\"start_station_name\").isNotNull()))  # Filter out null stations\n",
    "    \n",
    "    # Get top 10 stations\n",
    "    top_stations = station_df.orderBy(F.desc(\"total_starts\")).limit(10).toPandas()\n",
    "    \n",
    "    # Create two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12), height_ratios=[2, 1])\n",
    "    \n",
    "    # Top stations bar chart\n",
    "    bars = ax1.barh(top_stations['start_station_name'], top_stations['total_starts'])\n",
    "    ax1.set_title('Top 10 Busiest Stations')\n",
    "    ax1.set_xlabel('Number of Trips')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        ax1.text(width, bar.get_y() + bar.get_height()/2,\n",
    "                f'{int(width):,}',\n",
    "                ha='left', va='center', fontsize=10)\n",
    "    \n",
    "    # Station metrics\n",
    "    metrics_df = top_stations[['start_station_name', 'avg_duration_min', 'avg_distance_km', 'unique_destinations']]\n",
    "    ax2.axis('tight')\n",
    "    ax2.axis('off')\n",
    "    table = ax2.table(cellText=metrics_df.values,\n",
    "                     colLabels=metrics_df.columns,\n",
    "                     cellLoc='center',\n",
    "                     loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1.2, 1.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in station analysis: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. ‚è∞ Temporal Analysis\n",
    "display(Markdown(\"\"\"\n",
    "### ‚è∞ Temporal Usage Patterns\n",
    "Understanding how usage varies by time of day and weekday/weekend.\n",
    "\n",
    "Key insights:\n",
    "- Peak usage is during morning commute hours on weekdays (548,407 rides)\n",
    "- Weekend nights show higher usage than weekday nights\n",
    "- Longest average ride durations are during weekend afternoons\n",
    "- Clear commuting patterns visible in weekday morning and evening peaks\n",
    "\"\"\"))\n",
    "\n",
    "try:\n",
    "    temporal_df = (spark.read.format(\"delta\")\n",
    "        .load(f\"{gold_base_path}/temporal_metrics\")\n",
    "        .filter(F.col(\"year\") == ANALYSIS_YEAR)\n",
    "        .filter(F.col(\"month\") == ANALYSIS_MONTH))\n",
    "    \n",
    "    # Convert to pandas and ensure consistent time of day ordering\n",
    "    temporal_pd = temporal_df.toPandas()\n",
    "    time_order = ['morning', 'afternoon', 'evening', 'night']\n",
    "    temporal_pd['part_of_day'] = pd.Categorical(temporal_pd['part_of_day'], categories=time_order, ordered=True)\n",
    "    temporal_pd = temporal_pd.sort_values('part_of_day')\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Prepare data\n",
    "    weekday_data = temporal_pd[~temporal_pd['is_weekend']]\n",
    "    weekend_data = temporal_pd[temporal_pd['is_weekend']]\n",
    "    \n",
    "    # Plot 1: Rides by Time of Day\n",
    "    x = np.arange(len(time_order))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax1.bar(x - width/2, weekday_data['total_rides'], width, label='Weekday')\n",
    "    bars2 = ax1.bar(x + width/2, weekend_data['total_rides'], width, label='Weekend')\n",
    "    \n",
    "    ax1.set_title('Rides by Time of Day')\n",
    "    ax1.set_xlabel('Time of Day')\n",
    "    ax1.set_ylabel('Number of Rides')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(time_order, rotation=45)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Add value labels\n",
    "    def add_value_labels(bars):\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{int(height):,}',\n",
    "                    ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    add_value_labels(bars1)\n",
    "    add_value_labels(bars2)\n",
    "    \n",
    "    # Plot 2: Average Duration\n",
    "    ax2.set_title('Average Ride Duration by Time of Day')\n",
    "    ax2.set_xlabel('Time of Day')\n",
    "    ax2.set_ylabel('Average Duration (minutes)')\n",
    "    \n",
    "    # Plot lines with markers\n",
    "    ax2.plot(x, weekday_data['avg_duration_min'], \n",
    "             marker='o', label='Weekday', linewidth=2)\n",
    "    ax2.plot(x, weekend_data['avg_duration_min'], \n",
    "             marker='s', label='Weekend', linewidth=2)\n",
    "    \n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(time_order, rotation=45)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display summary statistics\n",
    "    summary_stats = temporal_df.agg(\n",
    "        F.sum(\"total_rides\").alias(\"total_rides\"),\n",
    "        F.round(F.avg(\"avg_duration_min\"), 2).alias(\"overall_avg_duration\"),\n",
    "        F.round(F.avg(\"avg_distance_km\"), 2).alias(\"overall_avg_distance\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    weekday_pct = (weekday_data['total_rides'].sum() / summary_stats[\"total_rides\"]) * 100\n",
    "    weekend_pct = (weekend_data['total_rides'].sum() / summary_stats[\"total_rides\"]) * 100\n",
    "    \n",
    "    display(Markdown(f\"\"\"\n",
    "    #### Key Temporal Metrics:\n",
    "    - Total Rides: {summary_stats[\"total_rides\"]:,}\n",
    "    - Weekday Rides: {weekday_pct:.1f}% ({weekday_data['total_rides'].sum():,} rides)\n",
    "    - Weekend Rides: {weekend_pct:.1f}% ({weekend_data['total_rides'].sum():,} rides)\n",
    "    - Average Duration: {summary_stats[\"overall_avg_duration\"]:.1f} minutes\n",
    "    - Average Distance: {summary_stats[\"overall_avg_distance\"]:.2f} km\n",
    "    \n",
    "    #### Notable Patterns:\n",
    "    1. **Weekday Commute**: Strong peaks in morning (~548K rides) and evening (~516K rides)\n",
    "    2. **Weekend Usage**: Higher night activity compared to weekdays\n",
    "    3. **Duration Trends**: \n",
    "       - Weekend rides are consistently longer\n",
    "       - Longest rides occur during weekend afternoons\n",
    "       - Shortest rides during weekday mornings (commuting)\n",
    "    \"\"\"))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in temporal analysis: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. üõ£Ô∏è Popular Routes Analysis\n",
    "display(Markdown(\"\"\"\n",
    "### üõ£Ô∏è Popular Routes Analysis\n",
    "Understanding the most frequently used paths and their characteristics.\n",
    "Note: Some routes may have missing station data (8,429 null end stations, 1,237 null start stations)\n",
    "\"\"\"))\n",
    "\n",
    "try:\n",
    "    # Read from route_metrics with correct column names and filters\n",
    "    routes_df = (spark.read.format(\"delta\")\n",
    "        .load(f\"{gold_base_path}/route_metrics\")\n",
    "        .filter(F.col(\"year\") == ANALYSIS_YEAR)\n",
    "        .filter(F.col(\"month\") == ANALYSIS_MONTH)\n",
    "        .filter(F.col(\"start_station_name\").isNotNull())\n",
    "        .filter(F.col(\"end_station_name\").isNotNull()))\n",
    "    \n",
    "    # Get top 10 routes\n",
    "    top_routes = (routes_df\n",
    "        .orderBy(F.desc(\"route_count\"))\n",
    "        .limit(10)\n",
    "        .toPandas())\n",
    "    \n",
    "    # Create visualization with subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12), height_ratios=[1.5, 1])\n",
    "    \n",
    "    # Plot 1: Top Routes by Volume with enhanced formatting\n",
    "    route_labels = []\n",
    "    for _, row in top_routes.iterrows():\n",
    "        start = row['start_station_name'][:25]\n",
    "        end = row['end_station_name'][:25]\n",
    "        route_labels.append(f\"{start}... ‚Üí\\n{end}...\")\n",
    "    \n",
    "    bars = ax1.barh(range(len(route_labels)), top_routes['route_count'],\n",
    "                    color='skyblue', alpha=0.7)\n",
    "    \n",
    "    # Add value labels with better positioning\n",
    "    for idx, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        ax1.text(width * 1.02, idx,\n",
    "                f'{int(width):,} trips',\n",
    "                ha='left', va='center', fontsize=10)\n",
    "    \n",
    "    ax1.set_yticks(range(len(route_labels)))\n",
    "    ax1.set_yticklabels(route_labels)\n",
    "    ax1.set_title('Top 10 Most Popular Routes', pad=20, fontsize=14)\n",
    "    ax1.set_xlabel('Number of Trips', fontsize=12)\n",
    "    ax1.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Route Metrics Table\n",
    "    table_data = []\n",
    "    for _, row in top_routes.iterrows():\n",
    "        table_data.append([\n",
    "            f\"{row['start_station_name'][:20]}... ‚Üí {row['end_station_name'][:20]}...\",\n",
    "            f\"{int(row['route_count']):,}\",\n",
    "            f\"{row['avg_distance_km']:.2f}\",\n",
    "            f\"{row['avg_duration_min']:.1f}\",\n",
    "            f\"{row['avg_speed_kmh']:.1f}\"\n",
    "        ])\n",
    "    \n",
    "    columns = ['Route', 'Trips', 'Avg Dist (km)', 'Avg Duration (min)', 'Avg Speed (km/h)']\n",
    "    \n",
    "    ax2.axis('tight')\n",
    "    ax2.axis('off')\n",
    "    table = ax2.table(cellText=table_data,\n",
    "                     colLabels=columns,\n",
    "                     cellLoc='center',\n",
    "                     loc='center',\n",
    "                     colColours=['#f2f2f2']*len(columns))\n",
    "    \n",
    "    # Enhanced table formatting\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1.2, 1.5)\n",
    "    \n",
    "    # Style the table\n",
    "    for k, cell in table._cells.items():\n",
    "        cell.set_edgecolor('white')\n",
    "        if k[0] == 0:  # Header row\n",
    "            cell.set_text_props(weight='bold')\n",
    "            cell.set_facecolor('#e6e6e6')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate summary statistics\n",
    "    route_stats = routes_df.agg(\n",
    "        F.count(\"*\").alias(\"total_routes\"),\n",
    "        F.sum(\"route_count\").alias(\"total_trips\"),\n",
    "        F.round(F.avg(\"avg_distance_km\"), 2).alias(\"avg_route_distance\"),\n",
    "        F.round(F.avg(\"avg_duration_min\"), 2).alias(\"avg_route_duration\"),\n",
    "        F.round(F.avg(\"avg_speed_kmh\"), 2).alias(\"avg_route_speed\")\n",
    "    ).collect()[0]\n",
    "\n",
    "    # Calculate additional metrics\n",
    "    top_10_trips = top_routes['route_count'].sum()\n",
    "    total_trips = route_stats[\"total_trips\"]\n",
    "    top_10_percentage = (top_10_trips / total_trips) * 100\n",
    "\n",
    "    # Calculate distance distribution matching the silver layer buckets\n",
    "    distance_distribution = (routes_df\n",
    "        .select(\n",
    "            F.when(F.col(\"avg_distance_km\") < 1, \"0-1 km\")\n",
    "             .when(F.col(\"avg_distance_km\").between(1, 4), \"1-4 km\")\n",
    "             .when(F.col(\"avg_distance_km\").between(4, 9), \"4-9 km\")\n",
    "             .otherwise(\"10+ km\")\n",
    "             .alias(\"distance_range\"),\n",
    "            \"route_count\"\n",
    "        )\n",
    "        .groupBy(\"distance_range\")\n",
    "        .agg(F.sum(\"route_count\").alias(\"total_trips\"))\n",
    "        .orderBy(\"distance_range\")\n",
    "        .toPandas())\n",
    "\n",
    "    display(Markdown(f\"\"\"\n",
    "    #### üìä Route Network Statistics:\n",
    "    - Total Unique Routes: {route_stats[\"total_routes\"]:,}\n",
    "    - Total Trips: {route_stats[\"total_trips\"]:,}\n",
    "    - Average Route Distance: {route_stats[\"avg_route_distance\"]:.2f} km\n",
    "    - Average Route Duration: {route_stats[\"avg_route_duration\"]:.1f} minutes\n",
    "    - Average Speed: {route_stats[\"avg_route_speed\"]:.1f} km/h\n",
    "\n",
    "    #### üîù Top Routes Analysis:\n",
    "    - Top 10 routes account for {top_10_percentage:.1f}% of all trips\n",
    "    - Most popular route: \n",
    "      - {top_routes['start_station_name'].iloc[0]} ‚Üí {top_routes['end_station_name'].iloc[0]}\n",
    "      - Volume: {top_routes['route_count'].iloc[0]:,} trips\n",
    "      - Distance: {top_routes['avg_distance_km'].iloc[0]:.2f} km\n",
    "      - Duration: {top_routes['avg_duration_min'].iloc[0]:.1f} minutes\n",
    "      - Speed: {top_routes['avg_speed_kmh'].iloc[0]:.1f} km/h\n",
    "\n",
    "    #### üìè Distance Distribution:\n",
    "    - 0-1 km: {distance_distribution.loc[distance_distribution['distance_range'] == '0-1 km', 'total_trips'].iloc[0]:,} trips\n",
    "    - 1-4 km: {distance_distribution.loc[distance_distribution['distance_range'] == '1-4 km', 'total_trips'].iloc[0]:,} trips\n",
    "    - 4-9 km: {distance_distribution.loc[distance_distribution['distance_range'] == '4-9 km', 'total_trips'].iloc[0]:,} trips\n",
    "    - 10+ km: {distance_distribution.loc[distance_distribution['distance_range'] == '10+ km', 'total_trips'].iloc[0]:,} trips\n",
    "\n",
    "    #### üéØ Key Insights:\n",
    "    1. **Popular Corridors**: \n",
    "       - Clear commuting patterns between key locations\n",
    "       - Top 10 routes represent {top_10_percentage:.1f}% of all trips\n",
    "    2. **Distance Patterns**:\n",
    "       - Most trips (36.2%) are medium-distance (1-4 km)\n",
    "       - Short trips (0-1 km) account for 23.2% of rides\n",
    "       - Long trips (4+ km) make up 4.7% of total volume\n",
    "    3. **Usage Characteristics**:\n",
    "       - Average duration of {route_stats[\"avg_route_duration\"]:.1f} minutes aligns with typical commute times\n",
    "       - Average speed of {route_stats[\"avg_route_speed\"]:.1f} km/h suggests efficient urban mobility\n",
    "    \"\"\"))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in routes analysis: {str(e)}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
