{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New York City Bike Share Analysis (Refactored)\n",
    "\n",
    "This notebook comprehensively analyzes bike-sharing data from New York City and Jersey City using PySpark and Apache Iceberg.\n",
    "\n",
    "## Project Structure\n",
    "- `/data`: Raw and processed data files\n",
    "  - `/raw`: Original downloaded files\n",
    "  - `/processed`: Cleaned and transformed data\n",
    "- `/conf`: Spark and analysis configurations\n",
    "- `/notebooks`: Jupyter notebooks\n",
    "- `/warehouse`: Iceberg tables location\n",
    "\n",
    "## Data Sources\n",
    "The Citi Bike dataset is available from the Citi Bike System Data S3 bucket:\n",
    "- URL: https://s3.amazonaws.com/tripdata/index.html\n",
    "- File formats:\n",
    "  - Jersey City: `JC-YYYYMM-citibike-tripdata.csv.zip`\n",
    "  - NYC: `YYYYMM-citibike-tripdata.zip`\n",
    "    - 2013-2023: Annual files\n",
    "    - 2024+: Monthly files\n",
    "\n",
    "## Analysis Overview\n",
    "1. Data Acquisition & Preparation\n",
    "2. Data Processing & Cleaning\n",
    "3. Feature Engineering\n",
    "4. Analysis & Insights\n",
    "5. Visualization & Reporting Spark and analysis configurations\n",
    "- `/home/aldamiz/notebooks`: Jupyter notebooks\n",
    "- `/home/aldamiz/warehouse`: Iceberg tables location\n",
    "\n",
    "## Data Sources\n",
    "The Citi Bike dataset is available from the Citi Bike System Data S3 bucket:\n",
    "- URL: https://s3.amazonaws.com/tripdata/index.html\n",
    "- File formats:\n",
    "  - Jersey City: `JC-YYYYMM-citibike-tripdata.csv.zip`\n",
    "  - NYC: `YYYYMM-citibike-tripdata.zip`\n",
    "    - 2013-2023: Annual files\n",
    "    - 2024+: Monthly files\n",
    "\n",
    "## Analysis Overview\n",
    "1. Data Acquisition & Preparation\n",
    "2. Data Processing & Cleaning\n",
    "3. Feature Engineering\n",
    "4. Analysis & Insights\n",
    "5. Visualization & Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Setup\n",
    "\n",
    "Initialize paths, constants, and Spark session with proper configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 00:00:53 - etl - INFO - Starting NYCBS data processing pipeline\n",
      "2025-03-05 00:00:53 - etl - INFO - Logs will be written to: /home/aldamiz/notebooks/logs\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "✅ All libraries and loggers initialized successfully\n",
       "- Log directory: /home/aldamiz/notebooks/logs"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Data processing and display\n",
    "import tqdm\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Spark and Delta\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Use the notebooks directory for logs (which has write permissions)\n",
    "notebook_dir = Path('/home/aldamiz/notebooks')\n",
    "LOG_DIR = notebook_dir / 'logs'\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "# Configure logging format\n",
    "log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "date_format = '%Y-%m-%d %H:%M:%S'\n",
    "\n",
    "def setup_logger(name, log_file):\n",
    "    \"\"\"Set up a logger that writes to both file and console\"\"\"\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # Prevent duplicate handlers\n",
    "    if logger.handlers:\n",
    "        logger.handlers.clear()\n",
    "    \n",
    "    # Create file handler\n",
    "    log_path = LOG_DIR / log_file\n",
    "    fh = logging.FileHandler(str(log_path))  # Convert Path to string\n",
    "    fh.setLevel(logging.INFO)\n",
    "    fh.setFormatter(logging.Formatter(log_format, date_format))\n",
    "    \n",
    "    # Create console handler\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.INFO)\n",
    "    ch.setFormatter(logging.Formatter(log_format, date_format))\n",
    "    \n",
    "    # Add handlers\n",
    "    logger.addHandler(fh)\n",
    "    logger.addHandler(ch)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "# Create loggers\n",
    "etl_logger = setup_logger('etl', 'etl.log')\n",
    "data_logger = setup_logger('data', 'data_quality.log')\n",
    "analysis_logger = setup_logger('analysis', 'analysis.log')\n",
    "\n",
    "# Helper functions for logging\n",
    "def log_performance_metrics(start_time, operation_name):\n",
    "    duration = time.time() - start_time\n",
    "    etl_logger.info(f\"Performance metric - {operation_name}: {duration:.2f} seconds\")\n",
    "\n",
    "def log_data_quality(df, stage):\n",
    "    data_logger.info(f\"Data quality metrics for {stage}:\")\n",
    "    data_logger.info(f\"- Row count: {df.count()}\")\n",
    "    data_logger.info(f\"- Null counts: {df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).collect()[0].asDict()}\")\n",
    "    data_logger.info(f\"- Duplicate count: {df.count() - df.distinct().count()}\")\n",
    "\n",
    "def log_error_details(error, context):\n",
    "    etl_logger.error(f\"Error in {context}: {str(error)}\", exc_info=True)\n",
    "    etl_logger.error(f\"Error context: {context}\")\n",
    "\n",
    "# Configure plotting\n",
    "%matplotlib inline\n",
    "sns.set_theme(style=\"whitegrid\", context=\"notebook\")\n",
    "\n",
    "# Configure pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' if abs(x) < 1000 else '%.0f' % x)\n",
    "\n",
    "# Configure plot sizes and styles\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (12, 6),\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12\n",
    "})\n",
    "\n",
    "# Test logging\n",
    "etl_logger.info(f\"Starting NYCBS data processing pipeline\")\n",
    "etl_logger.info(f\"Logs will be written to: {LOG_DIR}\")\n",
    "\n",
    "display(Markdown(f\"✅ All libraries and loggers initialized successfully\\n- Log directory: {LOG_DIR}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Setup\n",
    "\n",
    "Before diving into the analysis, we need to set up our environment and configure Spark. This includes:\n",
    "1. Defining project paths\n",
    "2. Setting up Spark with proper configurations\n",
    "3. Configuring storage locations for different data formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 00:00:53 - etl - INFO - Using data paths: /home/aldamiz/data\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "✅ Paths configured"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 2: Path Configuration\n",
    "# Use environment variables from container\n",
    "HOME_DIR = '/home/aldamiz'  # Fixed in container\n",
    "DATA_DIR = f\"{HOME_DIR}/data\"\n",
    "WAREHOUSE_DIR = f\"{HOME_DIR}/warehouse\"\n",
    "\n",
    "# Data paths\n",
    "LANDING_PATH = f\"{DATA_DIR}/landing\"\n",
    "BRONZE_PATH = f\"{DATA_DIR}/bronze\"\n",
    "SILVER_PATH = f\"{DATA_DIR}/silver\"\n",
    "GOLD_PATH = f\"{DATA_DIR}/gold\"\n",
    "\n",
    "# Base URL and time parameters\n",
    "BASE_URL = \"https://s3.amazonaws.com/tripdata\"\n",
    "YEAR, MONTH = 2025, 1\n",
    "\n",
    "etl_logger.info(f\"Using data paths: {DATA_DIR}\")\n",
    "display(Markdown(\"✅ Paths configured\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Configuration\n",
    "\n",
    "We'll configure Spark with support for both Delta Lake and Apache Iceberg. Key configurations include:\n",
    "- Package dependencies for table formats\n",
    "- SQL extensions for advanced features\n",
    "- Catalog configurations for data management\n",
    "- Performance optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 00:01:15 - etl - INFO - ✓ Delta Lake integration verified\n",
      "2025-03-05 00:01:15 - etl - INFO - Spark Session created with configurations:\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.app.initial.jar.urls: spark://0.0.0.0:44849/jars/io.delta_delta-core_2.12-2.4.0.jar,spark://0.0.0.0:44849/jars/org.apache.iceberg_iceberg-spark-runtime-3.4_2.12-1.3.1.jar,spark://0.0.0.0:44849/jars/io.delta_delta-storage-2.4.0.jar,spark://0.0.0.0:44849/jars/org.antlr_antlr4-runtime-4.9.3.jar\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.eventLog.enabled: true\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.history.fs.cleaner.maxAge: 7d\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.jars: file:///home/aldamiz/.ivy2/jars/io.delta_delta-core_2.12-2.4.0.jar,file:///home/aldamiz/.ivy2/jars/io.delta_delta-storage-2.4.0.jar,file:///home/aldamiz/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.4_2.12-1.3.1.jar,file:///home/aldamiz/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.app.submitTime: 1741132855967\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.jars.packages: io.delta:delta-core_2.12:2.4.0,io.delta:delta-storage:2.4.0,org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.3.1\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.sql.execution.arrow.pyspark.selfDestruct.maxRecords: 200000\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.driver.memory: 1g\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.sql.execution.arrow.maxRecordsPerBatch: 10000\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.speculation.quantile: 0.75\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.databricks.delta.properties.defaults.optimizeWrite.enabled: true\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.driver.extraJavaOptions: -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.sql.execution.arrow.pyspark.maxRecordsPerFile: 1000000\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.sql.catalog.delta: org.apache.spark.sql.delta.catalog.DeltaCatalog\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.executor.memory: 1g\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.master: local[*]\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.databricks.delta.properties.defaults.columnMapping.mode: name\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.history.fs.cleaner.interval: 1d\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.sql.execution.arrow.pyspark.enabled: true\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.speculation.multiplier: 1.5\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.memory.fraction: 0.8\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.repl.local.jars: file:///home/aldamiz/.ivy2/jars/io.delta_delta-core_2.12-2.4.0.jar,file:///home/aldamiz/.ivy2/jars/io.delta_delta-storage-2.4.0.jar,file:///home/aldamiz/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.4_2.12-1.3.1.jar,file:///home/aldamiz/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.executor.id: driver\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.sql.catalog.delta.warehouse: /home/aldamiz/warehouse/delta\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.sql.catalog.iceberg.cache-enabled: true\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.executor.heartbeatInterval: 10s\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.sql.shuffle.partitions: 10\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.app.id: local-1741132860394\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.databricks.delta.properties.defaults.enableChangeDataFeed: true\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.sql.catalog.iceberg: org.apache.iceberg.spark.SparkCatalog\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.sql.execution.arrow.pyspark.fallback.enabled: true\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.sql.execution.arrow.pyspark.maxRecordsPerBatch: 10000\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.speculation: true\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.sql.files.openCostInBytes: 134217728\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.sql.catalog.iceberg.write.format: parquet\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.memory.offHeap.enabled: true\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.app.startTime: 1741132856430\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.memory.offHeap.size: 1g\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.storage.blockManagerSlaveTimeoutMs: 600s\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.default.parallelism: 10\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.app.initial.file.urls: file:///home/aldamiz/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar,file:///home/aldamiz/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.4_2.12-1.3.1.jar,file:///home/aldamiz/.ivy2/jars/io.delta_delta-storage-2.4.0.jar,file:///home/aldamiz/.ivy2/jars/io.delta_delta-core_2.12-2.4.0.jar\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.serializer.objectStreamReset: 100\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.files: file:///home/aldamiz/.ivy2/jars/io.delta_delta-core_2.12-2.4.0.jar,file:///home/aldamiz/.ivy2/jars/io.delta_delta-storage-2.4.0.jar,file:///home/aldamiz/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.4_2.12-1.3.1.jar,file:///home/aldamiz/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.submit.deployMode: client\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.sql.catalog.iceberg.cache.expiration-interval-ms: 3600000\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.sql.execution.arrow.pyspark.selfDestruct.enabled: true\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.sql.warehouse.dir: file:/home/aldamiz/warehouse\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.driver.port: 44849\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.memory.storageFraction: 0.3\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.executor.extraJavaOptions: -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.sql.execution.arrow.pyspark.selfDestruct.maxMemory: 1g\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.task.maxFailures: 4\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.sql.catalog.delta.type: hadoop\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.databricks.delta.properties.defaults.autoCompact.enabled: true\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.app.name: CitiBike-Processing\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.submit.pyFiles: /home/aldamiz/.ivy2/jars/io.delta_delta-core_2.12-2.4.0.jar,/home/aldamiz/.ivy2/jars/io.delta_delta-storage-2.4.0.jar,/home/aldamiz/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.4_2.12-1.3.1.jar,/home/aldamiz/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.sql.execution.arrow.fallback.enabled: true\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.sql.adaptive.enabled: true\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.history.fs.cleaner.enabled: true\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.driver.host: 0.0.0.0\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.sql.extensions: io.delta.sql.DeltaSparkSessionExtension,org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.speculation.interval: 100\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.sql.catalog.iceberg.type: hadoop\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.sql.adaptive.coalescePartitions.enabled: true\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.eventLog.dir: /home/aldamiz/warehouse/eventlogs\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.network.timeout: 800s\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.rdd.compress: True\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.history.fs.logDirectory: /home/aldamiz/warehouse/eventlogs\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.sql.catalog.iceberg.warehouse: /home/aldamiz/warehouse/iceberg\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.ui.showConsoleProgress: true\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.sql.files.maxPartitionBytes: 134217728\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.sql.catalog.iceberg.write.compression-codec: snappy\n",
      "2025-03-05 00:01:16 - etl - INFO - - spark.sql.catalog.spark_catalog: org.apache.spark.sql.delta.catalog.DeltaCatalog\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "✅ Spark initialized with Delta Lake and Iceberg support"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 3: Spark Initialization\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session (configurations from spark-defaults.conf)\n",
    "spark = (SparkSession.builder\n",
    "        .appName(\"CitiBike-Processing\")\n",
    "        .getOrCreate())\n",
    "\n",
    "# Verify Delta Lake\n",
    "try:\n",
    "    test_delta = (spark.range(1)\n",
    "                 .write\n",
    "                 .format(\"delta\")\n",
    "                 .mode(\"overwrite\")\n",
    "                 .save(\"/home/aldamiz/warehouse/delta/test_delta\"))\n",
    "    etl_logger.info(\"✓ Delta Lake integration verified\")\n",
    "except Exception as e:\n",
    "    etl_logger.error(f\"Error verifying Delta Lake: {str(e)}\", exc_info=True)\n",
    "    raise\n",
    "\n",
    "# Log configuration for verification\n",
    "etl_logger.info(\"Spark Session created with configurations:\")\n",
    "for k, v in spark.sparkContext.getConf().getAll():\n",
    "    etl_logger.info(f\"- {k}: {v}\")\n",
    "\n",
    "display(Markdown(\"✅ Spark initialized with Delta Lake and Iceberg support\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition\n",
    "\n",
    "Now we'll set up the data acquisition process for both NYC and Jersey City bike share data. The data follows these patterns:\n",
    "\n",
    "### File Naming Conventions:\n",
    "- NYC (2024 onwards): `YYYYMM-citibike-tripdata.zip`\n",
    "- Jersey City: `JC-YYYYMM-citibike-tripdata.csv.zip`\n",
    "\n",
    "We'll start by downloading January 2025 data for both cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Downloading NYC Data"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 00:01:16 - etl - INFO - Downloading 202501-citibike-tripdata.zip\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "📥 Downloading `202501-citibike-tripdata.zip`..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "414MB [00:31, 13.2MB/s]                              \n",
      "2025-03-05 00:01:48 - etl - INFO - Successfully downloaded 202501-citibike-tripdata.zip\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "✅ Successfully downloaded `202501-citibike-tripdata.zip`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NYC Data Download\n",
    "display(Markdown(\"## Downloading NYC Data\"))\n",
    "\n",
    "# Setup paths for NYC data\n",
    "nyc_filename = f\"{YEAR}{MONTH:02d}-citibike-tripdata.zip\"\n",
    "nyc_url = f\"{BASE_URL}/{nyc_filename}\"\n",
    "nyc_output_dir = os.path.join(LANDING_PATH, \"nyc\", str(YEAR), f\"{MONTH:02d}\")\n",
    "os.makedirs(nyc_output_dir, exist_ok=True)\n",
    "nyc_output_path = os.path.join(nyc_output_dir, nyc_filename)\n",
    "\n",
    "# Download if file doesn't exist\n",
    "if not os.path.exists(nyc_output_path):\n",
    "    etl_logger.info(f\"Downloading {nyc_filename}\")\n",
    "    display(Markdown(f\"📥 Downloading `{nyc_filename}`...\"))\n",
    "    response = urllib.request.urlopen(nyc_url)\n",
    "    total_size = int(response.headers['Content-Length'])\n",
    "    \n",
    "    with tqdm.tqdm(total=total_size, unit='B', unit_scale=True) as pbar:\n",
    "        def report_hook(count, block_size, total_size):\n",
    "            pbar.update(block_size)\n",
    "        urllib.request.urlretrieve(nyc_url, nyc_output_path, reporthook=report_hook)\n",
    "    etl_logger.info(f\"Successfully downloaded {nyc_filename}\")\n",
    "    display(Markdown(f\"✅ Successfully downloaded `{nyc_filename}`\"))\n",
    "else:\n",
    "    etl_logger.info(f\"File already exists: {nyc_filename}\")\n",
    "    display(Markdown(f\"ℹ️ File already exists: `{nyc_filename}`\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Control Table Setup"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 00:01:48 - etl - INFO - Creating new control table\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "✅ Created new control table"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Current Control Table Status"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[source_file: string, file_path: string, processing_date: timestamp, record_count: bigint, status: string, year: int, month: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Control Table Setup\n",
    "display(Markdown(\"## Control Table Setup\"))\n",
    "\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, LongType, IntegerType\n",
    "\n",
    "# Define control table path and schema\n",
    "control_table_path = os.path.join(BRONZE_PATH, \"control_table\")\n",
    "control_schema = StructType([\n",
    "    StructField(\"source_file\", StringType(), False),\n",
    "    StructField(\"file_path\", StringType(), False),\n",
    "    StructField(\"processing_date\", TimestampType(), False),\n",
    "    StructField(\"record_count\", LongType(), False),\n",
    "    StructField(\"status\", StringType(), False),\n",
    "    StructField(\"year\", IntegerType(), False),\n",
    "    StructField(\"month\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "# Load or create control table\n",
    "try:\n",
    "    control_df = spark.read.format(\"delta\").load(control_table_path)\n",
    "    etl_logger.info(\"Control table loaded successfully\")\n",
    "    display(Markdown(\"ℹ️ Using existing control table\"))\n",
    "except:\n",
    "    etl_logger.info(\"Creating new control table\")\n",
    "    empty_control_df = spark.createDataFrame([], schema=control_schema)\n",
    "    (empty_control_df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"errorIfExists\")  # This ensures we don't accidentally overwrite\n",
    "        .save(control_table_path))\n",
    "    control_df = spark.read.format(\"delta\").load(control_table_path)\n",
    "    display(Markdown(\"✅ Created new control table\"))\n",
    "\n",
    "# Display current control table status\n",
    "display(Markdown(\"## Current Control Table Status\"))\n",
    "display(control_df.orderBy(F.col(\"processing_date\").desc()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Checking File Status"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 00:01:52 - etl - INFO - File needs processing\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "✅ File needs processing"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'should_process' (bool)\n"
     ]
    }
   ],
   "source": [
    "# Check File Processing Status\n",
    "display(Markdown(\"## Checking File Status\"))\n",
    "\n",
    "# Define file paths\n",
    "nyc_filename = f\"{YEAR}{MONTH:02d}-citibike-tripdata.zip\"\n",
    "nyc_output_path = os.path.join(LANDING_PATH, \"nyc\", str(YEAR), f\"{MONTH:02d}\", nyc_filename)\n",
    "bronze_output_path = os.path.join(BRONZE_PATH, \"rides_nyc\")\n",
    "\n",
    "# Check if source file exists\n",
    "if not os.path.exists(nyc_output_path):\n",
    "    etl_logger.error(f\"Source file not found: {nyc_output_path}\")\n",
    "    display(Markdown(\"❌ Source file not found\"))\n",
    "    raise FileNotFoundError(f\"Source file not found: {nyc_output_path}\")\n",
    "\n",
    "# Check if this file has already been processed successfully\n",
    "processed_files = control_df.filter(\n",
    "    (F.col(\"source_file\") == nyc_filename) & \n",
    "    (F.col(\"year\") == YEAR) & \n",
    "    (F.col(\"month\") == MONTH) &\n",
    "    (F.col(\"status\") == \"COMPLETED\")\n",
    ")\n",
    "\n",
    "if processed_files.count() > 0:\n",
    "    last_processed = processed_files.orderBy(F.col(\"processing_date\").desc()).first()\n",
    "    etl_logger.info(f\"File already processed on {last_processed.processing_date}\")\n",
    "    display(Markdown(f\"ℹ️ File was already processed on {last_processed.processing_date}\"))\n",
    "    display(Markdown(\"⏭️ Skipping processing\"))\n",
    "    should_process = False\n",
    "else:\n",
    "    etl_logger.info(\"File needs processing\")\n",
    "    display(Markdown(\"✅ File needs processing\"))\n",
    "    should_process = True\n",
    "\n",
    "# Store the processing flag for next cell\n",
    "%store should_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Processing Data"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 00:01:53 - etl - INFO - Extracting ZIP file\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "📂 Extracting data..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 00:01:54 - etl - INFO - Reading 202501-citibike-tripdata_2.csv\n",
      "2025-03-05 00:01:55 - etl - INFO - Reading 202501-citibike-tripdata_3.csv\n",
      "2025-03-05 00:01:55 - etl - INFO - Reading 202501-citibike-tripdata_1.csv\n",
      "2025-03-05 00:02:06 - etl - INFO - Successfully processed 2124475 records\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "✅ Processed 2,124,475 records"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Final Status"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[source_file: string, file_path: string, processing_date: timestamp, record_count: bigint, status: string, year: int, month: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Retrieve the processing flag\n",
    "%store -r should_process\n",
    "\n",
    "if not should_process:\n",
    "    etl_logger.info(\"Skipping processing as file was already processed\")\n",
    "    display(Markdown(\"⏭️ Processing skipped\"))\n",
    "else:\n",
    "    display(Markdown(\"## Processing Data\"))\n",
    "    \n",
    "    try:\n",
    "        # Create temporary processing directory\n",
    "        extract_dir = os.path.join(nyc_output_dir, \"extracted\")\n",
    "        os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "        # Extract files if needed\n",
    "        if not any(fname.endswith('.csv') for fname in os.listdir(extract_dir)):\n",
    "            etl_logger.info(\"Extracting ZIP file\")\n",
    "            display(Markdown(\"📂 Extracting data...\"))\n",
    "            with zipfile.ZipFile(nyc_output_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(extract_dir)\n",
    "\n",
    "        # Process CSV files\n",
    "        csv_files = glob.glob(os.path.join(extract_dir, \"*.csv\"))\n",
    "        if not csv_files:\n",
    "            raise ValueError(\"No CSV files found in archive\")\n",
    "\n",
    "        # Read and combine all CSV files\n",
    "        all_dfs = []\n",
    "        total_records = 0\n",
    "\n",
    "        for csv_path in csv_files:\n",
    "            etl_logger.info(f\"Reading {os.path.basename(csv_path)}\")\n",
    "            df = spark.read.csv(csv_path, header=True)\n",
    "            \n",
    "            # Add metadata columns\n",
    "            df = df.withColumn(\"ingestion_date\", F.current_timestamp()) \\\n",
    "                   .withColumn(\"source_file\", F.lit(nyc_filename)) \\\n",
    "                   .withColumn(\"city\", F.lit(\"nyc\")) \\\n",
    "                   .withColumn(\"year\", F.lit(YEAR).cast(IntegerType())) \\\n",
    "                   .withColumn(\"month\", F.lit(MONTH).cast(IntegerType()))\n",
    "            \n",
    "            all_dfs.append(df)\n",
    "            total_records += df.count()\n",
    "\n",
    "        # Combine all DataFrames\n",
    "        if not all_dfs:\n",
    "            raise ValueError(\"No data was read from CSV files\")\n",
    "            \n",
    "        combined_df = all_dfs[0]\n",
    "        for df in all_dfs[1:]:\n",
    "            combined_df = combined_df.unionAll(df)\n",
    "\n",
    "        # Write data ONLY if the bronze table doesn't exist or if this partition doesn't exist\n",
    "        bronze_exists = False\n",
    "        try:\n",
    "            existing_df = spark.read.format(\"delta\").load(bronze_output_path)\n",
    "            bronze_exists = True\n",
    "            \n",
    "            # Check if this partition exists\n",
    "            partition_exists = existing_df.filter(\n",
    "                (F.col(\"year\") == YEAR) & \n",
    "                (F.col(\"month\") == MONTH)\n",
    "            ).count() > 0\n",
    "            \n",
    "            if partition_exists:\n",
    "                raise ValueError(f\"Data for {YEAR}-{MONTH} already exists in bronze layer\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            if \"Path does not exist\" not in str(e):\n",
    "                raise e\n",
    "\n",
    "        # Write data\n",
    "        write_mode = \"append\" if bronze_exists else \"errorIfExists\"\n",
    "        \n",
    "        (combined_df.write\n",
    "            .format(\"delta\")\n",
    "            .partitionBy(\"year\", \"month\")\n",
    "            .mode(write_mode)\n",
    "            .save(bronze_output_path))\n",
    "\n",
    "        # Record successful processing in control table\n",
    "        control_record = spark.createDataFrame([(\n",
    "            nyc_filename,\n",
    "            nyc_output_path,\n",
    "            datetime.now(),\n",
    "            total_records,  # Changed from long(total_records)\n",
    "            \"COMPLETED\",\n",
    "            int(YEAR),\n",
    "            int(MONTH)\n",
    "        )], schema=control_schema)\n",
    "\n",
    "        (control_record.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"append\")\n",
    "            .save(control_table_path))\n",
    "\n",
    "        etl_logger.info(f\"Successfully processed {total_records} records\")\n",
    "        display(Markdown(f\"✅ Processed {total_records:,} records\"))\n",
    "\n",
    "    except Exception as e:\n",
    "        # Record failure in control table\n",
    "        error_record = spark.createDataFrame([(\n",
    "            nyc_filename,\n",
    "            nyc_output_path,\n",
    "            datetime.now(),\n",
    "            0,  # Changed from long(0)\n",
    "            \"FAILED\",\n",
    "            int(YEAR),\n",
    "            int(MONTH)\n",
    "        )], schema=control_schema)\n",
    "\n",
    "        (error_record.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"append\")\n",
    "            .save(control_table_path))\n",
    "\n",
    "        etl_logger.error(f\"Processing failed: {str(e)}\")\n",
    "        display(Markdown(f\"❌ Processing failed: {str(e)}\"))\n",
    "        raise\n",
    "\n",
    "# Display final status\n",
    "display(Markdown(\"## Final Status\"))\n",
    "display(spark.read.format(\"delta\").load(control_table_path).orderBy(F.col(\"processing_date\").desc()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silver Layer Transformations\n",
    "\n",
    "In this section, we'll enrich our bronze data with:\n",
    "1. Ride duration calculations\n",
    "2. Distance calculations using the Haversine formula\n",
    "3. Time-based features (part of day, weekends, seasons)\n",
    "4. Speed calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Silver Layer Transformations"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 00:02:06 - etl - INFO - Starting silver layer transformations\n",
      "2025-03-05 00:02:07 - etl - INFO - Read 2,124,475 records from bronze layer\n",
      "2025-03-05 00:02:08 - data - INFO - Data quality metrics for silver_transformation:\n",
      "2025-03-05 00:02:17 - data - INFO - - Row count: 2124475\n",
      "2025-03-05 00:02:16 - data - INFO - - Null counts: {'ride_id': 0, 'rideable_type': 0, 'started_at': 0, 'ended_at': 0, 'start_station_name': 564, 'start_station_id': 564, 'end_station_name': 3975, 'end_station_id': 4322, 'start_lat': 0, 'start_lng': 0, 'end_lat': 269, 'end_lng': 269, 'member_casual': 0, 'ingestion_date': 0, 'source_file': 0, 'city': 0, 'year': 0, 'month': 0, 'ride_duration_minutes': 0, 'ride_distance_km': 269, 'speed_kmh': 269, 'part_of_day': 0, 'is_weekend': 0, 'season': 0, 'extra_time_charge': 0, 'distance_bucket': 0}\n",
      "2025-03-05 00:02:28 - data - INFO - - Duplicate count: 0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Revenue Metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-----------+\n",
      "|total_revenue_usd|charged_rides|total_rides|\n",
      "+-----------------+-------------+-----------+\n",
      "|        11,741.60|        58708|    2124475|\n",
      "+-----------------+-------------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Distance Distribution"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+--------------------+-----------+\n",
      "|distance_bucket|ride_count|avg_duration_minutes|revenue_usd|\n",
      "+---------------+----------+--------------------+-----------+\n",
      "|         0-1 km|    769615|                5.32|   2,105.60|\n",
      "|         1-4 km|   1199440|               10.32|   2,760.40|\n",
      "|         10+ km|      8408|               38.55|   1,311.40|\n",
      "|         4-9 km|    146743|               24.22|   5,510.60|\n",
      "|        unknown|       269|            1,494.26|      53.60|\n",
      "+---------------+----------+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 00:02:37 - etl - INFO - Performance metric - silver_write: 7.62 seconds\n",
      "2025-03-05 00:02:37 - etl - INFO - Silver layer written to: /home/aldamiz/data/silver/rides_enriched\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "✅ Silver layer processing complete"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Sample of Processed Data"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------------------+----------------+---------+-----------+----------+------+-----------------+---------------+\n",
      "|          started_at|            ended_at|ride_duration_minutes|ride_distance_km|speed_kmh|part_of_day|is_weekend|season|extra_time_charge|distance_bucket|\n",
      "+--------------------+--------------------+---------------------+----------------+---------+-----------+----------+------+-----------------+---------------+\n",
      "|2025-01-23 13:34:...|2025-01-23 13:42:...|                  7.4|            1.35|    10.95|  afternoon|     false|winter|              0.0|         1-4 km|\n",
      "|2025-01-25 18:58:...|2025-01-25 19:01:...|                 3.12|            0.46|     8.85|    evening|      true|winter|              0.0|         0-1 km|\n",
      "|2025-01-16 09:33:...|2025-01-16 09:43:...|                 9.57|             1.7|    10.66|    morning|     false|winter|              0.0|         1-4 km|\n",
      "|2025-01-16 08:32:...|2025-01-16 08:39:...|                  6.5|             1.3|     12.0|    morning|     false|winter|              0.0|         1-4 km|\n",
      "|2025-01-22 18:29:...|2025-01-22 18:36:...|                  7.2|            2.02|    16.83|    evening|     false|winter|              0.0|         1-4 km|\n",
      "+--------------------+--------------------+---------------------+----------------+---------+-----------+----------+------+-----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Silver Layer Processing\n",
    "display(Markdown(\"## Silver Layer Transformations\"))\n",
    "etl_logger.info(\"Starting silver layer transformations\")\n",
    "\n",
    "try:\n",
    "    # Set time parser policy\n",
    "    spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "    # Read from bronze\n",
    "    bronze_df = spark.read.format(\"delta\").load(os.path.join(BRONZE_PATH, \"rides_nyc\"))\n",
    "    etl_logger.info(f\"Read {bronze_df.count():,} records from bronze layer\")\n",
    "\n",
    "    # Create a new DataFrame with proper types instead of modifying existing ones\n",
    "    silver_df = bronze_df.select(\n",
    "        \"*\",  # Keep all original columns\n",
    "        F.to_timestamp(\"started_at\").alias(\"started_at_clean\"),\n",
    "        F.to_timestamp(\"ended_at\").alias(\"ended_at_clean\"),\n",
    "        # Cast coordinates to double for calculations but keep original string columns\n",
    "        F.col(\"start_lat\").cast(\"double\").alias(\"start_lat_double\"),\n",
    "        F.col(\"start_lng\").cast(\"double\").alias(\"start_lng_double\"),\n",
    "        F.col(\"end_lat\").cast(\"double\").alias(\"end_lat_double\"),\n",
    "        F.col(\"end_lng\").cast(\"double\").alias(\"end_lng_double\")\n",
    "    )\n",
    "\n",
    "    # Calculate enriched features using the double-type columns\n",
    "    silver_df = silver_df.withColumn(\n",
    "        \"ride_duration_minutes\", \n",
    "        F.round(\n",
    "            (F.unix_timestamp(\"ended_at_clean\") - F.unix_timestamp(\"started_at_clean\")) / 60, \n",
    "            2\n",
    "        )\n",
    "    ).withColumn(\n",
    "        \"ride_distance_km\", \n",
    "        F.round(\n",
    "            F.acos(\n",
    "                F.sin(F.radians(\"start_lat_double\")) * F.sin(F.radians(\"end_lat_double\")) + \n",
    "                F.cos(F.radians(\"start_lat_double\")) * F.cos(F.radians(\"end_lat_double\")) * \n",
    "                F.cos(F.radians(\"start_lng_double\") - F.radians(\"end_lng_double\"))\n",
    "            ) * 6371,\n",
    "            2\n",
    "        )\n",
    "    ).withColumn(\n",
    "        \"speed_kmh\",\n",
    "        F.round(F.col(\"ride_distance_km\") / (F.col(\"ride_duration_minutes\") / 60), 2)\n",
    "    ).withColumn(\n",
    "        \"part_of_day\",\n",
    "        F.when(F.hour(\"started_at_clean\").between(5, 11), \"morning\")\n",
    "         .when(F.hour(\"started_at_clean\").between(12, 16), \"afternoon\")\n",
    "         .when(F.hour(\"started_at_clean\").between(17, 21), \"evening\")\n",
    "         .otherwise(\"night\")\n",
    "    ).withColumn(\n",
    "        \"is_weekend\",\n",
    "        F.dayofweek(\"started_at_clean\").isin([1, 7])\n",
    "    ).withColumn(\n",
    "        \"season\",\n",
    "        F.when(F.month(\"started_at_clean\").isin([12, 1, 2]), \"winter\")\n",
    "         .when(F.month(\"started_at_clean\").isin([3, 4, 5]), \"spring\")\n",
    "         .when(F.month(\"started_at_clean\").isin([6, 7, 8]), \"summer\")\n",
    "         .otherwise(\"fall\")\n",
    "    ).withColumn(\n",
    "        \"extra_time_charge\",\n",
    "        F.when(\n",
    "            (F.col(\"ride_duration_minutes\") > 30) & \n",
    "            (F.col(\"ride_duration_minutes\").isNotNull()), \n",
    "            0.2\n",
    "        ).otherwise(0.0)\n",
    "    ).withColumn(\n",
    "        \"distance_bucket\",\n",
    "        F.when(F.col(\"ride_distance_km\").isNull(), \"unknown\")\n",
    "         .when(F.col(\"ride_distance_km\") < 1, \"0-1 km\")\n",
    "         .when(F.col(\"ride_distance_km\").between(1, 4), \"1-4 km\")\n",
    "         .when(F.col(\"ride_distance_km\").between(4, 9), \"4-9 km\")\n",
    "         .otherwise(\"10+ km\")\n",
    "    )\n",
    "\n",
    "    # Drop temporary columns\n",
    "    silver_df = silver_df.drop(\n",
    "        \"started_at_clean\", \"ended_at_clean\",\n",
    "        \"start_lat_double\", \"start_lng_double\",\n",
    "        \"end_lat_double\", \"end_lng_double\"\n",
    "    )\n",
    "\n",
    "    # Cache the DataFrame for multiple actions\n",
    "    silver_df.cache()\n",
    "\n",
    "    # Log data quality metrics\n",
    "    log_data_quality(silver_df, \"silver_transformation\")\n",
    "    \n",
    "    # Calculate revenue metrics\n",
    "    display(Markdown(\"### Revenue Metrics\"))\n",
    "    silver_df.agg(\n",
    "        F.format_number(F.sum(\"extra_time_charge\"), 2).alias(\"total_revenue_usd\"),\n",
    "        F.count(F.when(F.col(\"ride_duration_minutes\") > 30, True)).alias(\"charged_rides\"),\n",
    "        F.count(\"*\").alias(\"total_rides\")\n",
    "    ).show()\n",
    "\n",
    "    # Calculate distance distribution\n",
    "    display(Markdown(\"### Distance Distribution\"))\n",
    "    silver_df.groupBy(\"distance_bucket\").agg(\n",
    "        F.count(\"*\").alias(\"ride_count\"),\n",
    "        F.format_number(F.avg(\"ride_duration_minutes\"), 2).alias(\"avg_duration_minutes\"),\n",
    "        F.format_number(F.sum(\"extra_time_charge\"), 2).alias(\"revenue_usd\")\n",
    "    ).orderBy(\"distance_bucket\").show()\n",
    "\n",
    "    # Write to silver layer with schema evolution\n",
    "    silver_path = os.path.join(SILVER_PATH, \"rides_enriched\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    (silver_df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .partitionBy(\"year\", \"month\")\n",
    "        .save(silver_path))\n",
    "\n",
    "    # Uncache the DataFrame\n",
    "    silver_df.unpersist()\n",
    "\n",
    "    log_performance_metrics(start_time, \"silver_write\")\n",
    "    etl_logger.info(f\"Silver layer written to: {silver_path}\")\n",
    "    display(Markdown(\"✅ Silver layer processing complete\"))\n",
    "\n",
    "    # Display sample of processed data\n",
    "    display(Markdown(\"### Sample of Processed Data\"))\n",
    "    spark.read.format(\"delta\").load(silver_path).select(\n",
    "        \"started_at\", \"ended_at\", \n",
    "        \"ride_duration_minutes\", \"ride_distance_km\", \n",
    "        \"speed_kmh\", \"part_of_day\", \"is_weekend\", \"season\",\n",
    "        \"extra_time_charge\", \"distance_bucket\"\n",
    "    ).limit(5).show()\n",
    "\n",
    "except Exception as e:\n",
    "    etl_logger.error(f\"Error in silver transformation: {str(e)}\", exc_info=True)\n",
    "    display(Markdown(f\"❌ Error: {str(e)}\"))\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    # Ensure DataFrame is uncached in case of error\n",
    "    if 'silver_df' in locals():\n",
    "        try:\n",
    "            silver_df.unpersist()\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gold Layer Analytics\n",
    "\n",
    "We'll create several analytical views:\n",
    "1. Station Analysis - Usage patterns and metrics per station\n",
    "2. Temporal Analysis - Time-based patterns and trends\n",
    "3. Popular Routes Analysis - Most frequent routes and their characteristics\n",
    "4. Seasonal Patterns - Weather and seasonal impact on riding behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Gold Layer - Station Analysis"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 00:02:37 - etl - INFO - Starting gold layer - station analysis\n",
      "2025-03-05 00:02:42 - etl - INFO - Performance metric - station_metrics_creation: 5.13 seconds\n",
      "2025-03-05 00:02:42 - data - INFO - Data quality metrics for station_metrics:\n",
      "2025-03-05 00:02:43 - data - INFO - - Row count: 2320\n",
      "2025-03-05 00:02:43 - data - INFO - - Null counts: {'start_station_name': 145, 'start_lat': 0, 'start_lng': 0, 'total_starts': 0, 'avg_ride_duration': 0, 'avg_ride_distance': 0, 'unique_destinations': 0}\n",
      "2025-03-05 00:02:43 - data - INFO - - Duplicate count: 0\n"
     ]
    }
   ],
   "source": [
    "# Gold Layer - Station Analysis\n",
    "display(Markdown(\"## Gold Layer - Station Analysis\"))\n",
    "etl_logger.info(\"Starting gold layer - station analysis\")\n",
    "start_time = time.time()\n",
    "\n",
    "station_metrics = (silver_df\n",
    "    .groupBy(\"start_station_name\", \"start_lat\", \"start_lng\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"total_starts\"),\n",
    "        F.avg(\"ride_duration_minutes\").alias(\"avg_ride_duration\"),\n",
    "        F.avg(\"ride_distance_km\").alias(\"avg_ride_distance\"),\n",
    "        F.countDistinct(\"end_station_name\").alias(\"unique_destinations\")\n",
    "    )\n",
    "    .orderBy(F.desc(\"total_starts\")))\n",
    "\n",
    "# Write station metrics\n",
    "gold_base_path = os.path.join(GOLD_PATH, \"analytics\")\n",
    "(station_metrics.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(f\"{gold_base_path}/station_metrics\"))\n",
    "\n",
    "log_performance_metrics(start_time, \"station_metrics_creation\")\n",
    "log_data_quality(station_metrics, \"station_metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Gold Layer - Temporal Analysis"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 00:02:44 - etl - INFO - Starting gold layer - temporal analysis\n",
      "2025-03-05 00:02:46 - etl - INFO - Performance metric - temporal_metrics_creation: 2.89 seconds\n",
      "2025-03-05 00:02:46 - data - INFO - Data quality metrics for temporal_metrics:\n",
      "2025-03-05 00:02:47 - data - INFO - - Row count: 8\n",
      "2025-03-05 00:02:49 - data - INFO - - Null counts: {'year': 0, 'month': 0, 'part_of_day': 0, 'is_weekend': 0, 'total_rides': 0, 'avg_duration': 0, 'avg_distance': 0, 'avg_speed': 0}\n",
      "2025-03-05 00:02:50 - data - INFO - - Duplicate count: 0\n"
     ]
    }
   ],
   "source": [
    "# Gold Layer - Temporal Analysis\n",
    "display(Markdown(\"## Gold Layer - Temporal Analysis\"))\n",
    "etl_logger.info(\"Starting gold layer - temporal analysis\")\n",
    "start_time = time.time()\n",
    "\n",
    "temporal_metrics = (silver_df\n",
    "    .groupBy(\"year\", \"month\", \"part_of_day\", \"is_weekend\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"total_rides\"),\n",
    "        F.avg(\"ride_duration_minutes\").alias(\"avg_duration\"),\n",
    "        F.avg(\"ride_distance_km\").alias(\"avg_distance\"),\n",
    "        F.avg(\"speed_kmh\").alias(\"avg_speed\")\n",
    "    ))\n",
    "\n",
    "# Write temporal metrics\n",
    "(temporal_metrics.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"year\", \"month\")\n",
    "    .save(f\"{gold_base_path}/temporal_metrics\"))\n",
    "\n",
    "log_performance_metrics(start_time, \"temporal_metrics_creation\")\n",
    "log_data_quality(temporal_metrics, \"temporal_metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Gold Layer - Route Analysis"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 00:02:50 - etl - INFO - Starting gold layer - route analysis\n",
      "2025-03-05 00:02:54 - etl - INFO - Performance metric - route_analysis_creation: 4.08 seconds\n",
      "2025-03-05 00:02:54 - data - INFO - Data quality metrics for popular_routes:\n",
      "2025-03-05 00:02:55 - data - INFO - - Row count: 388392\n",
      "2025-03-05 00:02:58 - data - INFO - - Null counts: {'start_station_name': 344, 'end_station_name': 1255, 'route_count': 0, 'avg_distance': 67, 'avg_duration': 0}\n",
      "2025-03-05 00:02:59 - data - INFO - - Duplicate count: 0\n"
     ]
    }
   ],
   "source": [
    "# Gold Layer - Route Analysis\n",
    "display(Markdown(\"## Gold Layer - Route Analysis\"))\n",
    "etl_logger.info(\"Starting gold layer - route analysis\")\n",
    "start_time = time.time()\n",
    "\n",
    "popular_routes = (silver_df\n",
    "    .groupBy(\"start_station_name\", \"end_station_name\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"route_count\"),\n",
    "        F.avg(\"ride_distance_km\").alias(\"avg_distance\"),\n",
    "        F.avg(\"ride_duration_minutes\").alias(\"avg_duration\")\n",
    "    )\n",
    "    .orderBy(F.desc(\"route_count\")))\n",
    "\n",
    "# Write popular routes\n",
    "(popular_routes.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(f\"{gold_base_path}/popular_routes\"))\n",
    "\n",
    "log_performance_metrics(start_time, \"route_analysis_creation\")\n",
    "log_data_quality(popular_routes, \"popular_routes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Gold Layer - Seasonal Analysis"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 00:02:59 - etl - INFO - Starting gold layer - seasonal analysis\n",
      "2025-03-05 00:03:01 - etl - INFO - Performance metric - seasonal_analysis_creation: 2.71 seconds\n",
      "2025-03-05 00:03:01 - data - INFO - Data quality metrics for seasonal_patterns:\n",
      "2025-03-05 00:03:02 - data - INFO - - Row count: 4\n",
      "2025-03-05 00:03:04 - data - INFO - - Null counts: {'season': 0, 'part_of_day': 0, 'total_rides': 0, 'avg_duration': 0, 'avg_distance': 0}\n",
      "2025-03-05 00:03:05 - data - INFO - - Duplicate count: 0\n",
      "2025-03-05 00:03:05 - etl - INFO - Gold layer processing complete\n"
     ]
    }
   ],
   "source": [
    "# Gold Layer - Seasonal Analysis\n",
    "display(Markdown(\"## Gold Layer - Seasonal Analysis\"))\n",
    "etl_logger.info(\"Starting gold layer - seasonal analysis\")\n",
    "start_time = time.time()\n",
    "\n",
    "seasonal_patterns = (silver_df\n",
    "    .groupBy(\"season\", \"part_of_day\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"total_rides\"),\n",
    "        F.avg(\"ride_duration_minutes\").alias(\"avg_duration\"),\n",
    "        F.avg(\"ride_distance_km\").alias(\"avg_distance\")\n",
    "    ))\n",
    "\n",
    "# Write seasonal patterns\n",
    "(seasonal_patterns.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(f\"{gold_base_path}/seasonal_patterns\"))\n",
    "\n",
    "log_performance_metrics(start_time, \"seasonal_analysis_creation\")\n",
    "log_data_quality(seasonal_patterns, \"seasonal_patterns\")\n",
    "\n",
    "etl_logger.info(\"Gold layer processing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights Visualization\n",
    "\n",
    "Let's examine some key insights from our analysis:\n",
    "1. Busiest stations and their characteristics\n",
    "2. Most popular routes\n",
    "3. Seasonal patterns\n",
    "4. Daily usage patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Key Insights"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Top 10 Busiest Stations"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---------+------------+----------------+---------------+-------------------+\n",
      "|  start_station_name|latitude|longitude|total_starts|avg_duration_min|avg_distance_km|unique_destinations|\n",
      "+--------------------+--------+---------+------------+----------------+---------------+-------------------+\n",
      "|     W 21 St & 6 Ave| 40.7417| -73.9942|        8943|            7.89|           1.35|                474|\n",
      "|     W 31 St & 7 Ave| 40.7492| -73.9916|        7557|            9.02|           1.61|                486|\n",
      "|Pier 61 at Chelse...| 40.7469| -74.0082|        7526|           10.18|            2.0|                459|\n",
      "|Lafayette St & E ...| 40.7302|  -73.991|        7334|            8.19|           1.36|                506|\n",
      "|    11 Ave & W 41 St| 40.7603| -73.9988|        6932|            9.51|            1.8|                457|\n",
      "|  Broadway & E 14 St| 40.7345| -73.9907|        6626|            8.18|           1.39|                496|\n",
      "|     Ave A & E 14 St| 40.7303| -73.9805|        6593|             6.3|            NaN|                400|\n",
      "|     E 33 St & 1 Ave| 40.7432| -73.9745|        6587|            8.98|           1.74|                485|\n",
      "|     9 Ave & W 33 St| 40.7526| -73.9968|        6555|            9.36|           1.71|                471|\n",
      "|University Pl & E...| 40.7348| -73.9921|        6343|            8.37|           1.38|                500|\n",
      "+--------------------+--------+---------+------------+----------------+---------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Top 10 Most Popular Routes"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+---------------+----------------+\n",
      "|  start_station_name|    end_station_name|route_count|avg_distance_km|avg_duration_min|\n",
      "+--------------------+--------------------+-----------+---------------+----------------+\n",
      "|Norfolk St & Broo...| Henry St & Grand St|        504|           0.67|            3.25|\n",
      "|     W 21 St & 6 Ave|     9 Ave & W 22 St|        438|           0.78|            4.34|\n",
      "|North Moore St & ...|Vesey St & Greenw...|        411|           0.85|            4.03|\n",
      "| Henry St & Grand St|Norfolk St & Broo...|        401|           0.67|            3.79|\n",
      "|     E 77 St & 1 Ave|     E 77 St & 3 Ave|        401|           0.49|            3.03|\n",
      "|N 6 St & Bedford Ave|  S 4 St & Wythe Ave|        375|           0.81|            4.21|\n",
      "|     E 77 St & 1 Ave|     2 Ave & E 72 St|        373|           0.48|            3.64|\n",
      "|55 Ave & Center Blvd|Vernon Blvd & 50 Ave|        373|            0.6|            3.25|\n",
      "|North Moore St & ...|Vesey Pl & River ...|        366|           0.76|            3.97|\n",
      "|Richardson St & N...|Graham Ave & Cons...|        365|           0.48|            2.66|\n",
      "+--------------------+--------------------+-----------+---------------+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Seasonal Riding Patterns"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-----------+----------------+---------------+\n",
      "|season|part_of_day|total_rides|avg_duration_min|avg_distance_km|\n",
      "+------+-----------+-----------+----------------+---------------+\n",
      "|winter|  afternoon|     671015|           10.32|            NaN|\n",
      "|winter|    evening|     627970|            9.68|            NaN|\n",
      "|winter|    morning|     671714|            9.23|            NaN|\n",
      "|winter|      night|     153776|           10.13|            NaN|\n",
      "+------+-----------+-----------+----------------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Daily Pattern Analysis"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+---------------+-------------+\n",
      "|part_of_day|avg_duration_minutes|avg_distance_km|avg_speed_kmh|\n",
      "+-----------+--------------------+---------------+-------------+\n",
      "|  afternoon|               10.43|            NaN|          NaN|\n",
      "|    evening|                9.81|            NaN|          NaN|\n",
      "|    morning|                9.32|            NaN|          NaN|\n",
      "|      night|               10.22|            NaN|          NaN|\n",
      "+-----------+--------------------+---------------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Summary Statistics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Summary Statistics\n",
       "- Total number of rides: 2,124,475\n",
       "- Average ride duration: 9.94 minutes\n",
       "- Average ride distance: 0.00 km"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 00:03:07 - analysis - INFO - Key insights generated and displayed\n"
     ]
    }
   ],
   "source": [
    "# Display Key Insights\n",
    "display(Markdown(\"## Key Insights\"))\n",
    "\n",
    "try:\n",
    "    # 1. Top 10 Busiest Stations\n",
    "    display(Markdown(\"### Top 10 Busiest Stations\"))\n",
    "    station_metrics = spark.read.format(\"delta\").load(f\"{gold_base_path}/station_metrics\")\n",
    "    station_metrics.select(\n",
    "        \"start_station_name\",\n",
    "        F.round(\"start_lat\", 4).alias(\"latitude\"),\n",
    "        F.round(\"start_lng\", 4).alias(\"longitude\"),\n",
    "        \"total_starts\",\n",
    "        F.round(\"avg_ride_duration\", 2).alias(\"avg_duration_min\"),\n",
    "        F.round(\"avg_ride_distance\", 2).alias(\"avg_distance_km\"),\n",
    "        \"unique_destinations\"\n",
    "    ).orderBy(F.desc(\"total_starts\")).limit(10).show()\n",
    "\n",
    "    # 2. Popular Routes\n",
    "    display(Markdown(\"### Top 10 Most Popular Routes\"))\n",
    "    popular_routes = spark.read.format(\"delta\").load(f\"{gold_base_path}/popular_routes\")\n",
    "    popular_routes.select(\n",
    "        \"start_station_name\",\n",
    "        \"end_station_name\",\n",
    "        \"route_count\",\n",
    "        F.round(\"avg_distance\", 2).alias(\"avg_distance_km\"),\n",
    "        F.round(\"avg_duration\", 2).alias(\"avg_duration_min\")\n",
    "    ).orderBy(F.desc(\"route_count\")).limit(10).show()\n",
    "\n",
    "    # 3. Seasonal Patterns\n",
    "    display(Markdown(\"### Seasonal Riding Patterns\"))\n",
    "    seasonal_patterns = spark.read.format(\"delta\").load(f\"{gold_base_path}/seasonal_patterns\")\n",
    "    seasonal_patterns.select(\n",
    "        \"season\",\n",
    "        \"part_of_day\",\n",
    "        \"total_rides\",\n",
    "        F.round(\"avg_duration\", 2).alias(\"avg_duration_min\"),\n",
    "        F.round(\"avg_distance\", 2).alias(\"avg_distance_km\")\n",
    "    ).orderBy(\"season\", \"part_of_day\").show()\n",
    "\n",
    "    # 4. Daily Pattern Analysis\n",
    "    display(Markdown(\"### Daily Pattern Analysis\"))\n",
    "    temporal_metrics = spark.read.format(\"delta\").load(f\"{gold_base_path}/temporal_metrics\")\n",
    "    daily_patterns = temporal_metrics.groupBy(\"part_of_day\").agg(\n",
    "        F.round(F.avg(\"avg_duration\"), 2).alias(\"avg_duration_minutes\"),\n",
    "        F.round(F.avg(\"avg_distance\"), 2).alias(\"avg_distance_km\"),\n",
    "        F.round(F.avg(\"avg_speed\"), 2).alias(\"avg_speed_kmh\")\n",
    "    ).orderBy(\"part_of_day\")\n",
    "    daily_patterns.show()\n",
    "\n",
    "    # Summary Statistics with proper null and nan handling\n",
    "    total_rides = temporal_metrics.agg(F.sum(\"total_rides\")).collect()[0][0]\n",
    "    avg_duration = temporal_metrics.agg(F.avg(\"avg_duration\")).collect()[0][0]\n",
    "    avg_distance = temporal_metrics.agg(F.avg(\"avg_distance\")).collect()[0][0]\n",
    "\n",
    "    import math\n",
    "    \n",
    "    # Format summary statistics with null/nan handling\n",
    "    def format_stat(value, decimal_places=2):\n",
    "        if value is None or (isinstance(value, float) and math.isnan(value)):\n",
    "            return \"0.00\"\n",
    "        return f\"{value:.{decimal_places}f}\"\n",
    "\n",
    "    display(Markdown(\"### Summary Statistics\"))\n",
    "    summary_text = \"\\n\".join([\n",
    "        \"### Summary Statistics\",\n",
    "        f\"- Total number of rides: {total_rides:,}\",\n",
    "        f\"- Average ride duration: {format_stat(avg_duration)} minutes\",\n",
    "        f\"- Average ride distance: {format_stat(avg_distance)} km\"\n",
    "    ])\n",
    "    display(Markdown(summary_text))\n",
    "\n",
    "except Exception as e:\n",
    "    display(Markdown(f\"❌ Error: {str(e)}\"))\n",
    "    analysis_logger.error(f\"Error in key insights generation: {str(e)}\", exc_info=True)\n",
    "    raise\n",
    "\n",
    "analysis_logger.info(\"Key insights generated and displayed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
