{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 🚲 NYC Bike Share Data Analysis\n",
    " \n",
    " ## 📋 Project Overview\n",
    " This notebook analyzes bike sharing data from New York City, providing insights into riding patterns, popular routes, and usage statistics.\n",
    " \n",
    " ## 🏗️ Architecture\n",
    " The project follows a medallion architecture for data processing:\n",
    " \n",
    " ### 🥉 Bronze Layer\n",
    " - Raw data ingestion\n",
    " - Data validation\n",
    " - Source tracking\n",
    " - Audit logging\n",
    " \n",
    " ### 🥈 Silver Layer\n",
    " - Data cleaning\n",
    " - Feature engineering\n",
    " - Quality checks\n",
    " - Enrichment with:\n",
    "   - Ride duration\n",
    "   - Distance calculations\n",
    "   - Time-based features\n",
    " \n",
    " ### 🥇 Gold Layer\n",
    " - Analytics views\n",
    " - Aggregated metrics\n",
    " - Business KPIs\n",
    " - Reporting tables\n",
    " \n",
    " ## 🔧 Technical Setup\n",
    " This notebook leverages:\n",
    " - Apache Spark with Delta Lake\n",
    " - ELK Stack for logging\n",
    " - Docker containerization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import shutil\n",
    "import zipfile\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import DeltaTable\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Optional\n",
    "import sys\n",
    "\n",
    "# Add src directory to Python path\n",
    "src_dir = os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), \"..\"))\n",
    "if src_dir not in sys.path:\n",
    "    sys.path.append(src_dir)\n",
    "\n",
    "# Import local modules\n",
    "from utils.notebook_logger import NotebookLogger\n",
    "from utils.schema_manager import SchemaManager, DataFrameTransformer, DataValidator\n",
    "\n",
    "\n",
    "# Initialize logger\n",
    "logger = NotebookLogger('nycbs')\n",
    "logger.info(\"Starting notebook execution\")\n",
    "\n",
    "# Initialize Spark with Delta configurations\n",
    "start_time = time.time()\n",
    "spark = SparkSession.builder.appName(\"NYCBS_Analysis\").getOrCreate()\n",
    "duration_ms = (time.time() - start_time) * 1000\n",
    "logger.spark_operation(\n",
    "    \"Spark session initialized\",\n",
    "    operation_type=\"init\",\n",
    "    duration_ms=duration_ms,\n",
    "    spark_version=spark.version\n",
    "    ) \n",
    "\n",
    "# Define paths\n",
    "WAREHOUSE_DIR = os.getenv('WAREHOUSE_DIR', '/home/aldamiz/warehouse')\n",
    "DATA_DIR = os.getenv('DATA_DIR', '/home/aldamiz/data')\n",
    "BRONZE_PATH = os.path.join(DATA_DIR, 'bronze')\n",
    "SILVER_PATH = os.path.join(DATA_DIR, 'silver')\n",
    "GOLD_PATH = os.path.join(DATA_DIR, 'gold')\n",
    "\n",
    "# Display configuration summary\n",
    "display(Markdown(\"\"\"\n",
    "### 🔍 Configuration Summary:\n",
    "- 📂 Warehouse Directory: `{}`\n",
    "- 📂 Data Directory: `{}`\n",
    "- 🥉 Bronze Layer: `{}`\n",
    "- 🥈 Silver Layer: `{}`\n",
    "- 🥇 Gold Layer: `{}`\n",
    "\"\"\".format(WAREHOUSE_DIR, DATA_DIR, BRONZE_PATH, SILVER_PATH, GOLD_PATH)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 📊 Control Table Setup\n",
    " \n",
    " ## Purpose\n",
    " The control table tracks the processing status of source files to ensure:\n",
    " - 🔄 Idempotency (no duplicate processing)\n",
    " - 📝 Audit trail of all operations\n",
    " - ⏱️ Processing timestamps\n",
    " - 📈 Record counts\n",
    " - 🎯 Status tracking\n",
    " \n",
    " ## Schema\n",
    " - `source_file`: Source file identifier\n",
    " - `file_path`: Location of processed data\n",
    " - `processing_date`: Timestamp of processing\n",
    " - `record_count`: Number of records processed\n",
    " - `status`: Processing status (SUCCESS/FAILED)\n",
    " - `year`, `month`: Temporal partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize control table\n",
    "control_table_path = os.path.join(WAREHOUSE_DIR, \"control_table\")\n",
    "\n",
    "try:\n",
    "    # Create control table if it doesn't exist\n",
    "    if not DeltaTable.isDeltaTable(spark, control_table_path):\n",
    "        logger.info(\"Creating control table\")\n",
    "        \n",
    "        control_schema = StructType([\n",
    "            StructField(\"source_file\", StringType(), False),\n",
    "            StructField(\"file_path\", StringType(), False),\n",
    "            StructField(\"processing_date\", TimestampType(), False),\n",
    "            StructField(\"record_count\", LongType(), True),\n",
    "            StructField(\"status\", StringType(), False),\n",
    "            StructField(\"year\", IntegerType(), False),\n",
    "            StructField(\"month\", IntegerType(), False)\n",
    "        ])\n",
    "        \n",
    "        # Create empty table with schema\n",
    "        empty_df = spark.createDataFrame([], control_schema)\n",
    "        empty_df.write.format(\"delta\").mode(\"errorIfExists\").save(control_table_path)\n",
    "        \n",
    "        display(HTML('<div style=\"color: #3c763d;\">✅ Control table created</div>'))\n",
    "    else:\n",
    "        display(HTML('<div style=\"color: #31708f;\">ℹ️ Control table exists</div>'))\n",
    "\n",
    "    # Display schema and recent history\n",
    "    control_df = spark.read.format(\"delta\").load(control_table_path)\n",
    "    \n",
    "    # Show schema in a clean table\n",
    "    schema_df = pd.DataFrame([\n",
    "        {\"Field\": f.name, \"Type\": str(f.dataType), \"Required\": \"Yes\" if not f.nullable else \"No\"}\n",
    "        for f in control_df.schema\n",
    "    ])\n",
    "    display(schema_df.style.set_properties(**{'padding': '5px'}))\n",
    "    \n",
    "    # Show recent processing history\n",
    "    if control_df.count() > 0:\n",
    "        display(control_df.orderBy(F.col(\"processing_date\").desc())\n",
    "               .limit(5).toPandas()\n",
    "               .style.set_properties(**{'padding': '5px'}))\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in control table setup: {str(e)}\", error=e)\n",
    "    display(HTML(f'<div style=\"color: #a94442;\">❌ Error: {str(e)}</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📥 Data Ingestion\n",
    " \n",
    " ## Landing Zone Structure\n",
    " - 📂 `landing/`: Storage for extracted CSV files\n",
    " - 📂 `landing/zip/`: Storage for downloaded ZIP files\n",
    " \n",
    " ## Configuration\n",
    " - 🌍 Source: NYC Citibike System Data\n",
    " - 📅 Select date range for data download\n",
    " - 🔄 Supports multiple years/months and cities\n",
    " - 📦 Automatic ZIP handling\n",
    " \n",
    " ## Process Flow\n",
    " 1. 🎯 Define date range and cities\n",
    " 2. 🌐 Download ZIP files to `landing/zip/`\n",
    " 3. 📦 Extract CSV files to `landing/`\n",
    " 4. ✅ Validate files\n",
    " \n",
    " ## File Patterns\n",
    " ### NYC Files\n",
    " - 2013-2023: `YYYY-citibike-tripdata.zip` (annual files)\n",
    " - 2024 (Jan-Apr): `YYYYMM-citibike-tripdata.csv.zip`\n",
    " - 2024 (May+): `YYYYMM-citibike-tripdata.zip`\n",
    " - 2025+: `YYYYMM-citibike-tripdata.zip`\n",
    " \n",
    " ### Jersey City (JC) Files\n",
    " - 2015+: `JC-YYYYMM-citibike-tripdata.csv.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date Range and City Configuration\n",
    "display(HTML(\"\"\"\n",
    "<h4>📅 Configure Data Download:</h4>\n",
    "<p>Modify these parameters to select your date range and cities:</p>\n",
    "\"\"\"))\n",
    "\n",
    "# Define parameters\n",
    "START_YEAR = 2025    # Modify this for different start year\n",
    "START_MONTH = 1      # 1-12\n",
    "END_YEAR = 2025      # Modify this for different end year\n",
    "END_MONTH = 12       # 1-12\n",
    "CITIES = ['NYC']  # NYC for New York City, JC for Jersey City\n",
    "\n",
    "def get_file_url(filename):\n",
    "    \"\"\"Generate S3 URL for a given filename\"\"\"\n",
    "    return f'https://s3.amazonaws.com/tripdata/{filename}'\n",
    "\n",
    "def generate_filename(city, year, month, current_date):\n",
    "    \"\"\"\n",
    "    Generate the correct filename based on city and date\n",
    "    \n",
    "    NYC Pattern:\n",
    "    - 2013-2023: YYYY-citibike-tripdata.zip (annual files)\n",
    "    - 2024 (01-04): YYYYMM-citibike-tripdata.csv.zip\n",
    "    - 2024 (05+) and beyond: YYYYMM-citibike-tripdata.zip\n",
    "    \n",
    "    JC Pattern:\n",
    "    - 2015+: JC-YYYYMM-citibike-tripdata.csv.zip\n",
    "    \"\"\"\n",
    "    # Check if the date is current month or future\n",
    "    if year > current_date.year or (year == current_date.year and month >= current_date.month):\n",
    "        return None\n",
    "\n",
    "    if city == 'NYC':\n",
    "        if year < 2024:\n",
    "            # Annual files for 2013-2023\n",
    "            if month == 12:  # Only generate annual file in December\n",
    "                return f\"{year}-citibike-tripdata.zip\"\n",
    "        else:  # 2024 and beyond\n",
    "            if year == 2024 and 1 <= month <= 4:\n",
    "                # January to April 2024: YYYYMM-citibike-tripdata.csv.zip\n",
    "                return f\"{year}{month:02d}-citibike-tripdata.csv.zip\"\n",
    "            else:\n",
    "                # May 2024 onwards and all months in 2025+: YYYYMM-citibike-tripdata.zip\n",
    "                return f\"{year}{month:02d}-citibike-tripdata.zip\"\n",
    "    \n",
    "    elif city == 'JC' and year >= 2015:  # JC data starts from 2015\n",
    "        return f\"JC-{year}{month:02d}-citibike-tripdata.csv.zip\"\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Display configuration\n",
    "display(HTML(f\"\"\"\n",
    "<div style=\"background-color: #f8f9fa; padding: 15px; border-radius: 5px;\">\n",
    "    <p><b>📅 Selected Configuration:</b></p>\n",
    "    <ul>\n",
    "        <li>Start: {START_YEAR}-{START_MONTH:02d}</li>\n",
    "        <li>End: {END_YEAR}-{END_MONTH:02d}</li>\n",
    "        <li>Cities: {', '.join(CITIES)}</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "try:\n",
    "    # Get current date for validation\n",
    "    current_date = datetime.now()\n",
    "    \n",
    "    # Validate configuration\n",
    "    if not (1 <= START_MONTH <= 12 and 1 <= END_MONTH <= 12):\n",
    "        raise ValueError(\"Months must be between 1 and 12\")\n",
    "    if START_YEAR > END_YEAR or (START_YEAR == END_YEAR and START_MONTH > END_MONTH):\n",
    "        raise ValueError(\"Start date must be before end date\")\n",
    "    \n",
    "    # Create directories if they don't exist\n",
    "    landing_path = os.path.join(DATA_DIR, \"landing\")\n",
    "    zip_storage_path = os.path.join(landing_path, \"zip\")  # ZIP files storage\n",
    "    control_table_path = os.path.join(WAREHOUSE_DIR, \"control_table\")\n",
    "    Path(landing_path).mkdir(parents=True, exist_ok=True)\n",
    "    Path(zip_storage_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(control_table_path)\n",
    "    \n",
    "    # Read control table\n",
    "    control_df = spark.read.format(\"delta\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .load(control_table_path)\n",
    "\n",
    "    processed_files = set(control_df.filter(F.col(\"status\") == \"SUCCESS\")\n",
    "                         .select(\"source_file\")\n",
    "                         .toPandas()[\"source_file\"])\n",
    "\n",
    "    # Generate list of files to download\n",
    "    sources = []\n",
    "    \n",
    "    for city in CITIES:\n",
    "        current_year = START_YEAR\n",
    "        current_month = START_MONTH\n",
    "        \n",
    "        while (current_year < END_YEAR or \n",
    "               (current_year == END_YEAR and current_month <= END_MONTH)):\n",
    "            \n",
    "            filename = generate_filename(city, current_year, current_month, current_date)\n",
    "            if filename:  # Only add if a valid filename was generated\n",
    "                csv_filename = filename.replace('.zip', '.csv')\n",
    "                \n",
    "                # Skip if already processed or exists in landing\n",
    "                if csv_filename not in processed_files and not os.path.exists(os.path.join(landing_path, csv_filename)):\n",
    "                    sources.append({\n",
    "                        'url': get_file_url(filename),\n",
    "                        'year': current_year,\n",
    "                        'month': current_month,\n",
    "                        'city': city,\n",
    "                        'filename': filename\n",
    "                    })\n",
    "            \n",
    "            current_month += 1\n",
    "            if current_month > 12:\n",
    "                current_month = 1\n",
    "                current_year += 1\n",
    "\n",
    "    logger.info(f\"Planning to download {len(sources)} files\")\n",
    "    \n",
    "    # Download and extract files\n",
    "    files_downloaded = 0\n",
    "    failed_downloads = []\n",
    "    skipped_files = []\n",
    "    \n",
    "    for source in sources:\n",
    "        csv_filename = source['filename'].replace('.zip', '.csv')\n",
    "        zip_file_path = os.path.join(zip_storage_path, source['filename'])\n",
    "        csv_file_path = os.path.join(landing_path, csv_filename)\n",
    "        \n",
    "        # Skip if CSV already exists in landing\n",
    "        if csv_filename in processed_files:\n",
    "            logger.info(f\"Skipping {csv_filename} - already processed\")\n",
    "            skipped_files.append((csv_filename, \"Already processed\"))\n",
    "            continue\n",
    "            \n",
    "        if os.path.exists(os.path.join(landing_path, csv_filename)):\n",
    "            logger.info(f\"Skipping {csv_filename} - already in landing zone\")\n",
    "            skipped_files.append((csv_filename, \"Already in landing zone\"))\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Check if ZIP exists first\n",
    "            if not os.path.exists(zip_file_path):\n",
    "                # Download file only if it doesn't exist\n",
    "                logger.info(f\"Downloading {source['filename']}\")\n",
    "                response = requests.get(source['url'], stream=True)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                with open(zip_file_path, 'wb') as f:\n",
    "                    shutil.copyfileobj(response.raw, f)\n",
    "            else:\n",
    "                logger.info(f\"Using existing ZIP file: {source['filename']}\")\n",
    "            \n",
    "            # Extract file\n",
    "            logger.info(f\"Extracting {source['filename']}\")\n",
    "            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                # Filter out macOS metadata files and extract only CSV files\n",
    "                csv_files = [f for f in zip_ref.namelist() \n",
    "                        if f.endswith('.csv') and not f.startswith('__MACOSX')]\n",
    "                \n",
    "                # Extract each CSV file\n",
    "                for csv_file in csv_files:\n",
    "                    # Remove any path components and get just the filename\n",
    "                    csv_basename = os.path.basename(csv_file)\n",
    "                    # Extract with the new filename\n",
    "                    with zip_ref.open(csv_file) as source_file, \\\n",
    "                        open(os.path.join(landing_path, csv_basename), 'wb') as target_file:\n",
    "                        shutil.copyfileobj(source_file, target_file)\n",
    "            \n",
    "            files_downloaded += 1\n",
    "            logger.info(f\"Successfully downloaded and extracted {csv_filename}\")\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.warning(f\"Failed to download {source['filename']}: {str(e)}\")\n",
    "            failed_downloads.append((source['filename'], str(e)))\n",
    "            # Remove failed download if it exists\n",
    "            if os.path.exists(zip_path):\n",
    "                os.remove(zip_path)\n",
    "            continue\n",
    "        except zipfile.BadZipFile:\n",
    "            logger.warning(f\"Corrupt zip file: {source['filename']}\")\n",
    "            failed_downloads.append((source['filename'], \"Corrupt ZIP file\"))\n",
    "            if os.path.exists(zip_path):\n",
    "                os.remove(zip_path)\n",
    "            continue\n",
    "\n",
    "    # Display summary\n",
    "    display(HTML(\"<h4>📥 Download Summary:</h4>\"))\n",
    "    \n",
    "    summary_html = f\"\"\"\n",
    "    <div style=\"background-color: #f8f9fa; padding: 15px; border-radius: 5px;\">\n",
    "        <p><b>Results:</b></p>\n",
    "        <ul>\n",
    "            <li>✅ Successfully downloaded: {files_downloaded}</li>\n",
    "            <li>❌ Failed downloads: {len(failed_downloads)}</li>\n",
    "            <li>⏭️ Skipped files: {len(skipped_files)}</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(summary_html))\n",
    "\n",
    "    # Show skipped files if any\n",
    "    if skipped_files:\n",
    "        display(HTML(\"<h4>⏭️ Skipped Files:</h4>\"))\n",
    "        skipped_df = pd.DataFrame(skipped_files, columns=['File', 'Reason'])\n",
    "        display(skipped_df.style.set_properties(**{'padding': '5px'}))\n",
    "\n",
    "    # Show failed downloads if any\n",
    "    if failed_downloads:\n",
    "        display(HTML(\"<h4>❌ Failed Downloads:</h4>\"))\n",
    "        failed_df = pd.DataFrame(failed_downloads, columns=['File', 'Error'])\n",
    "        display(failed_df.style.set_properties(**{'padding': '5px'}))\n",
    "\n",
    "    # Show landing zone contents\n",
    "    display(HTML(\"<h4>📁 Landing Zone Contents:</h4>\"))\n",
    "    landing_files = [f for f in os.listdir(landing_path) if f.endswith('.csv')]\n",
    "    if landing_files:\n",
    "        files_df = pd.DataFrame({\n",
    "            'File': landing_files,\n",
    "            'Size (MB)': [round(os.path.getsize(os.path.join(landing_path, f)) / (1024 * 1024), 2) \n",
    "                         for f in landing_files],\n",
    "            'City': ['JC' if f.startswith('JC-') else 'NYC' for f in landing_files],\n",
    "            'Date': [f.split('-')[1] if f.startswith('JC-') else f.split('-')[0] for f in landing_files]\n",
    "        }).sort_values(['City', 'Date'], ascending=[True, False])\n",
    "        \n",
    "        display(files_df.style\n",
    "               .set_properties(**{'padding': '5px'})\n",
    "               .format({'Size (MB)': '{:.2f}'}))\n",
    "    else:\n",
    "        display(HTML('<div style=\"color: #31708f;\">ℹ️ Landing zone is empty</div>'))\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in data download: {str(e)}\", exc_info=True)\n",
    "    display(HTML(f'<div style=\"color: #a94442;\">❌ Error: {str(e)}</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🥉 Bronze Layer Processing\n",
    " \n",
    " ## Purpose\n",
    " The bronze layer ingests raw data while ensuring:\n",
    " - 🔒 No duplicate processing of source files\n",
    " - ✅ Data validation\n",
    " - 📊 Record counting\n",
    " - 📝 Processing status tracking\n",
    " \n",
    " ## Process Flow\n",
    " 1. Check control table for existing processing\n",
    " 2. Read source CSV files\n",
    " 3. Validate data structure\n",
    " 4. Write to bronze Delta table\n",
    " 5. Update control table with processing status\n",
    " \n",
    " ## Partitioning Strategy\n",
    " Data is partitioned by:\n",
    " - 📅 Year\n",
    " - 📅 Month\n",
    " \n",
    " This enables efficient querying and data management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_query_performance(query_name: str, df: DataFrame, action: str, metrics: dict = None):\n",
    "    \"\"\"\n",
    "    Monitor Spark query performance and log to ELK stack\n",
    "    \n",
    "    Args:\n",
    "        query_name: Name/identifier of the query\n",
    "        df: Spark DataFrame being processed\n",
    "        action: Type of action (e.g., \"count\", \"write\", \"read\")\n",
    "        metrics: Additional metrics to log\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        if action == \"count\":\n",
    "            _ = df.count()  # Trigger computation\n",
    "        \n",
    "        duration_ms = (time.time() - start_time) * 1000\n",
    "        \n",
    "        # Prepare log entry\n",
    "        log_entry = {\n",
    "            \"event_type\": \"query_performance\",\n",
    "            \"query_name\": query_name,\n",
    "            \"action\": action,\n",
    "            \"duration_ms\": duration_ms,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        if metrics:\n",
    "            log_entry.update(metrics)\n",
    "            \n",
    "        logger.info(\"Query performance metrics\", extra=log_entry)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error monitoring query: {str(e)}\", error=e)\n",
    "\n",
    "def process_delta_table(table_name: str, operation: str, df: DataFrame, path: str):\n",
    "    \"\"\"\n",
    "    Process Delta table operations with monitoring\n",
    "    \n",
    "    Args:\n",
    "        table_name: Name of the table\n",
    "        operation: Type of operation (create, append, overwrite)\n",
    "        df: Spark DataFrame to write\n",
    "        path: Path to save the Delta table\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if operation == \"create\":\n",
    "            df.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .save(path)\n",
    "        elif operation == \"append\":\n",
    "            df.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .save(path)\n",
    "            \n",
    "        duration_ms = (time.time() - start_time) * 1000\n",
    "        \n",
    "        # Log operation metrics\n",
    "        log_entry = {\n",
    "            \"event_type\": \"delta_operation\",\n",
    "            \"table_name\": table_name,\n",
    "            \"operation\": operation,\n",
    "            \"duration_ms\": duration_ms,\n",
    "            \"record_count\": df.count(),\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        logger.info(\"Delta table operation completed\", extra=log_entry)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing Delta table: {str(e)}\", error=e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define explicit schema for CitiBike data\n",
    "citibike_schema = StructType([\n",
    "    StructField(\"ride_id\", StringType(), False),\n",
    "    StructField(\"rideable_type\", StringType(), True),\n",
    "    StructField(\"started_at\", StringType(), True),\n",
    "    StructField(\"ended_at\", StringType(), True),  \n",
    "    StructField(\"start_station_name\", StringType(), True),\n",
    "    StructField(\"start_station_id\", StringType(), True),\n",
    "    StructField(\"end_station_name\", StringType(), True),\n",
    "    StructField(\"end_station_id\", StringType(), True),\n",
    "    StructField(\"start_lat\", DoubleType(), True),\n",
    "    StructField(\"start_lng\", DoubleType(), True),\n",
    "    StructField(\"end_lat\", DoubleType(), True),\n",
    "    StructField(\"end_lng\", DoubleType(), True),\n",
    "    StructField(\"member_casual\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define schema for control table\n",
    "control_schema = StructType([\n",
    "    StructField(\"source_file\", StringType(), False),\n",
    "    StructField(\"file_path\", StringType(), False),\n",
    "    StructField(\"processing_date\", TimestampType(), False),\n",
    "    StructField(\"record_count\", LongType(), True),\n",
    "    StructField(\"status\", StringType(), False),\n",
    "    StructField(\"year\", IntegerType(), False),\n",
    "    StructField(\"month\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "def process_bronze_file(file_path: str, bronze_path: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Process a single file for the bronze layer with strict schema enforcement\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the input CSV file\n",
    "        bronze_path: Path to save the bronze layer data\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (record_count, year, month)\n",
    "    \"\"\"\n",
    "    # First read the CSV file with the citibike schema\n",
    "    df = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"timestampFormat\", \"yyyy-MM-dd HH:mm:ss\") \\\n",
    "        .schema(citibike_schema) \\\n",
    "        .load(file_path)\n",
    "    \n",
    "    # Convert timestamps immediately\n",
    "    bronze_df = df \\\n",
    "        .withColumn(\"started_at\", F.to_timestamp(\"started_at\")) \\\n",
    "        .withColumn(\"ended_at\", F.to_timestamp(\"ended_at\"))\n",
    "    \n",
    "    # Then add time dimensions (which expect timestamp type)\n",
    "    bronze_df = DataFrameTransformer.add_time_dimensions(bronze_df) \\\n",
    "        .withColumn(\"ingestion_date\", F.current_timestamp()) \\\n",
    "        .withColumn(\"source_file\", F.input_file_name()) \\\n",
    "        .withColumn(\"city\", F.lit(\"nyc\").cast(StringType()))\n",
    "    \n",
    "    # Get record count and year/month\n",
    "    record_count = bronze_df.count()\n",
    "    year_month = bronze_df.select(\n",
    "        F.first(\"year\").alias(\"year\"),\n",
    "        F.first(\"month\").alias(\"month\")\n",
    "    ).first()\n",
    "    \n",
    "    # Check if bronze table exists\n",
    "    if not DeltaTable.isDeltaTable(spark, bronze_path):\n",
    "        # First write - create table with our schema\n",
    "        bronze_df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"timestampFormat\", SchemaManager.get_timestamp_format(\"bronze\")) \\\n",
    "            .partitionBy(\"year\", \"month\") \\\n",
    "            .save(bronze_path)\n",
    "    else:\n",
    "        # Table exists - read its schema first\n",
    "        existing_df = spark.read.format(\"delta\").load(bronze_path)\n",
    "        # Cast our columns to match existing schema\n",
    "        for field in existing_df.schema:\n",
    "            if field.name in bronze_df.columns:\n",
    "                bronze_df = bronze_df.withColumn(\n",
    "                    field.name,\n",
    "                    F.col(field.name).cast(field.dataType)\n",
    "                )\n",
    "        # Now write with schema evolution disabled\n",
    "        bronze_df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .option(\"mergeSchema\", \"false\") \\\n",
    "            .partitionBy(\"year\", \"month\") \\\n",
    "            .save(bronze_path)\n",
    "    \n",
    "    return record_count, year_month[\"year\"], year_month[\"month\"]\n",
    "\n",
    "# Main processing code\n",
    "try:\n",
    "    # Set paths\n",
    "    bronze_rides_path = os.path.join(BRONZE_PATH, \"rides_nyc\")\n",
    "    landing_path = os.path.join(DATA_DIR, \"landing\")\n",
    "    \n",
    "    # Get list of CSV files to process\n",
    "    csv_files = [f for f in os.listdir(landing_path) if f.endswith('.csv')]\n",
    "    logger.info(f\"Found {len(csv_files)} CSV files to process\")\n",
    "    \n",
    "    if not csv_files:\n",
    "        display(HTML('<div style=\"color: #31708f;\">ℹ️ No CSV files found in landing zone</div>'))\n",
    "    else:\n",
    "        # Read control table with explicit schema\n",
    "        control_df = spark.read.format(\"delta\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .load(control_table_path)\n",
    "        \n",
    "        processed_files = set(control_df.filter(F.col(\"status\") == \"SUCCESS\")\n",
    "                            .select(\"source_file\")\n",
    "                            .toPandas()[\"source_file\"])\n",
    "        \n",
    "        new_files = [f for f in csv_files if f not in processed_files]\n",
    "        logger.info(f\"Found {len(new_files)} new files to process\")\n",
    "        \n",
    "        for csv_file in new_files:\n",
    "            try:\n",
    "                file_path = os.path.join(landing_path, csv_file)\n",
    "                logger.info(f\"Processing {csv_file}\")\n",
    "                \n",
    "                # Process file and get metrics\n",
    "                record_count, year, month = process_bronze_file(file_path, bronze_rides_path)\n",
    "                \n",
    "                # Monitor performance\n",
    "                monitor_query_performance(\n",
    "                    query_name=f\"bronze_write_{csv_file}\",\n",
    "                    df=spark.read.format(\"delta\").load(bronze_rides_path),\n",
    "                    action=\"count\",\n",
    "                    metrics={\n",
    "                        \"layer\": \"bronze\",\n",
    "                        \"operation\": \"write\",\n",
    "                        \"file\": csv_file,\n",
    "                        \"record_count\": record_count\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                # Update control table\n",
    "                control_data = [(\n",
    "                    csv_file,\n",
    "                    bronze_rides_path,\n",
    "                    datetime.now(),\n",
    "                    record_count,\n",
    "                    \"SUCCESS\",\n",
    "                    year,\n",
    "                    month\n",
    "                )]\n",
    "                \n",
    "                spark.createDataFrame(\n",
    "                    control_data,\n",
    "                    schema=control_schema\n",
    "                ).write \\\n",
    "                    .format(\"delta\") \\\n",
    "                    .mode(\"append\") \\\n",
    "                    .save(control_table_path)\n",
    "                \n",
    "                logger.info(f\"Processed {csv_file}: {record_count:,} records\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing file {csv_file}: {str(e)}\", error=e)\n",
    "                continue\n",
    "    \n",
    "    # Display processing summary\n",
    "    display(HTML(\"<h4>📊 Bronze Layer Summary:</h4>\"))\n",
    "    \n",
    "    try:\n",
    "        bronze_df = spark.read.format(\"delta\").load(bronze_rides_path)\n",
    "        summary = (bronze_df.groupBy(\"year\", \"month\")\n",
    "                  .agg(F.count(\"*\").alias(\"records\"))\n",
    "                  .orderBy(\"year\", \"month\"))\n",
    "        \n",
    "        if summary.count() > 0:\n",
    "            display(summary.toPandas().style\n",
    "                   .set_properties(**{'padding': '5px'})\n",
    "                   .format({'records': '{:,}'}))\n",
    "        else:\n",
    "            display(HTML('<div style=\"color: #31708f;\">ℹ️ No data in bronze layer yet</div>'))\n",
    "            \n",
    "    except Exception as e:\n",
    "        if \"Path does not exist\" in str(e):\n",
    "            display(HTML('<div style=\"color: #31708f;\">ℹ️ Bronze layer not created yet - waiting for first file</div>'))\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in bronze processing: {str(e)}\", error=e)\n",
    "    display(HTML(f'<div style=\"color: #a94442;\">❌ Error: {str(e)}</div>'))\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine records from December 2024\n",
    "bronze_rides_path = os.path.join(BRONZE_PATH, \"rides_nyc\")\n",
    "\n",
    "try:\n",
    "    # Read the bronze table\n",
    "    df = spark.read.format(\"delta\").load(bronze_rides_path)\n",
    "    \n",
    "    # Query December 2024 records\n",
    "    dec_2024_df = df.filter((F.col(\"year\") == 2024) & (F.col(\"month\") == 12))\n",
    "    \n",
    "    # Display sample records with timestamp information\n",
    "    display(HTML(\"<h4>🔍 December 2024 Records Sample:</h4>\"))\n",
    "    \n",
    "    sample_records = dec_2024_df.select(\n",
    "        \"ride_id\",\n",
    "        \"started_at\",\n",
    "        \"ended_at\",\n",
    "        \"year\",\n",
    "        \"month\"\n",
    "    ).limit(10)\n",
    "    \n",
    "    display(sample_records.toPandas().style\n",
    "           .set_properties(**{'padding': '5px'})\n",
    "           .format({'started_at': lambda x: x.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                   'ended_at': lambda x: x.strftime('%Y-%m-%d %H:%M:%S')}))\n",
    "    \n",
    "    # Get distribution of dates\n",
    "    display(HTML(\"<h4>📊 Date Distribution:</h4>\"))\n",
    "    \n",
    "    date_dist = dec_2024_df.select(\n",
    "        F.date_format(\"started_at\", \"yyyy-MM-dd\").alias(\"date\")\n",
    "    ).groupBy(\"date\").agg(\n",
    "        F.count(\"*\").alias(\"records\")\n",
    "    ).orderBy(\"date\")\n",
    "    \n",
    "    display(date_dist.toPandas().style\n",
    "           .set_properties(**{'padding': '5px'})\n",
    "           .format({'records': '{:,}'}))\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error querying December 2024 records: {str(e)}\", exc_info=True)\n",
    "    display(HTML(f'<div style=\"color: #a94442;\">❌ Error: {str(e)}</div>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_df.filter(F.col(\"started_at\") <= F.lit(\"2024-12-31\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🥈 Silver Layer Processing\n",
    " \n",
    " ## Purpose\n",
    " The silver layer enriches the raw data with:\n",
    " - ⏱️ Ride duration calculations\n",
    " - 📏 Distance calculations\n",
    " - 🕒 Time-based features\n",
    " - 🌡️ Seasonal patterns\n",
    " - ✨ Data quality improvements\n",
    " \n",
    " ## Feature Engineering\n",
    " ### Time-Based Features\n",
    " - `ride_duration_sec`: Duration in seconds\n",
    " - `start_hour`: Hour of day (0-23)\n",
    " - `is_weekend`: Boolean flag\n",
    " - `season`: Winter, Spring, Summer, Fall\n",
    " - `time_of_day`: Morning, Afternoon, Evening, Night\n",
    " \n",
    " ### Geographic Features\n",
    " - `ride_distance_km`: Great circle distance in kilometers\n",
    " \n",
    " ## Data Quality Rules\n",
    " 1. Duration between 1 minute and 24 hours\n",
    " 2. Distance between 0.1 and 100 km\n",
    " 3. Coordinates within NYC metropolitan area\n",
    " \n",
    " ## Analysis Views\n",
    " - 🌤️ Seasonal patterns\n",
    " - ⏰ Time of day distribution\n",
    " - 📅 Weekend vs weekday comparison\n",
    " - 📊 Distance and duration statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silver Layer Processing\n",
    "display(Markdown(\"## Silver Layer Transformations\"))\n",
    "logger.info(\"Starting silver layer transformations\")\n",
    "\n",
    "# Logging helper functions\n",
    "def log_data_quality(df, stage):\n",
    "    \"\"\"Log data quality metrics for a DataFrame\"\"\"\n",
    "    logger.info(f\"Data quality metrics for {stage}:\")\n",
    "    logger.info(f\"- Row count: {df.count()}\")\n",
    "    logger.info(f\"- Null counts: {df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).collect()[0].asDict()}\")\n",
    "    logger.info(f\"- Duplicate count: {df.count() - df.distinct().count()}\")\n",
    "\n",
    "def log_performance_metrics(start_time, operation_name):\n",
    "    \"\"\"Log performance metrics for an operation\"\"\"\n",
    "    duration = time.time() - start_time\n",
    "    logger.info(f\"Performance metric - {operation_name}: {duration:.2f} seconds\")\n",
    "\n",
    "try:\n",
    "    # Set time parser policy\n",
    "    spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "    # Read from bronze\n",
    "    bronze_df = spark.read.format(\"delta\").load(os.path.join(BRONZE_PATH, \"rides_nyc\"))\n",
    "    logger.info(f\"Read {bronze_df.count():,} records from bronze layer\")\n",
    "\n",
    "    # Create a new DataFrame with proper types instead of modifying existing ones\n",
    "    silver_df = bronze_df.select(\n",
    "        \"*\",  # Keep all original columns\n",
    "        F.to_timestamp(\"started_at\").alias(\"started_at_clean\"),\n",
    "        F.to_timestamp(\"ended_at\").alias(\"ended_at_clean\"),\n",
    "        # Cast coordinates to double for calculations but keep original string columns\n",
    "        F.col(\"start_lat\").cast(\"double\").alias(\"start_lat_double\"),\n",
    "        F.col(\"start_lng\").cast(\"double\").alias(\"start_lng_double\"),\n",
    "        F.col(\"end_lat\").cast(\"double\").alias(\"end_lat_double\"),\n",
    "        F.col(\"end_lng\").cast(\"double\").alias(\"end_lng_double\")\n",
    "    )\n",
    "\n",
    "    # Calculate enriched features using the double-type columns\n",
    "    silver_df = silver_df.withColumn(\n",
    "        \"ride_duration_minutes\", \n",
    "        F.round(\n",
    "            (F.unix_timestamp(\"ended_at_clean\") - F.unix_timestamp(\"started_at_clean\")) / 60, \n",
    "            2\n",
    "        )\n",
    "    ).withColumn(\n",
    "        \"ride_distance_km\", \n",
    "        F.round(\n",
    "            F.acos(\n",
    "                F.sin(F.radians(\"start_lat_double\")) * F.sin(F.radians(\"end_lat_double\")) + \n",
    "                F.cos(F.radians(\"start_lat_double\")) * F.cos(F.radians(\"end_lat_double\")) * \n",
    "                F.cos(F.radians(\"start_lng_double\") - F.radians(\"end_lng_double\"))\n",
    "            ) * 6371,\n",
    "            2\n",
    "        )\n",
    "    ).withColumn(\n",
    "        \"speed_kmh\",\n",
    "        F.round(F.col(\"ride_distance_km\") / (F.col(\"ride_duration_minutes\") / 60), 2)\n",
    "    ).withColumn(\n",
    "        \"part_of_day\",\n",
    "        F.when(F.hour(\"started_at_clean\").between(5, 11), \"morning\")\n",
    "         .when(F.hour(\"started_at_clean\").between(12, 16), \"afternoon\")\n",
    "         .when(F.hour(\"started_at_clean\").between(17, 21), \"evening\")\n",
    "         .otherwise(\"night\")\n",
    "    ).withColumn(\n",
    "        \"is_weekend\",\n",
    "        F.dayofweek(\"started_at_clean\").isin([1, 7])\n",
    "    ).withColumn(\n",
    "        \"season\",\n",
    "        F.when(F.month(\"started_at_clean\").isin([12, 1, 2]), \"winter\")\n",
    "         .when(F.month(\"started_at_clean\").isin([3, 4, 5]), \"spring\")\n",
    "         .when(F.month(\"started_at_clean\").isin([6, 7, 8]), \"summer\")\n",
    "         .otherwise(\"fall\")\n",
    "    )\n",
    "\n",
    "    # Drop temporary columns\n",
    "    silver_df = silver_df.drop(\n",
    "        \"started_at_clean\", \"ended_at_clean\",\n",
    "        \"start_lat_double\", \"start_lng_double\",\n",
    "        \"end_lat_double\", \"end_lng_double\"\n",
    "    )\n",
    "\n",
    "    # Log data quality metrics\n",
    "    log_data_quality(silver_df, \"silver_transformation\")\n",
    "\n",
    "    # Write to silver layer with schema evolution\n",
    "    silver_path = os.path.join(SILVER_PATH, \"rides_nyc\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Monitor wiritng writing\n",
    "    process_delta_table(\n",
    "        table_name=\"rides_enriched\",\n",
    "        operation=\"create\",\n",
    "        df=silver_df,\n",
    "        path=os.path.join(SILVER_PATH, \"rides_enriched\")\n",
    "    )\n",
    "\n",
    "    # Monitor the enrichment query performance\n",
    "    monitor_query_performance(\n",
    "        query_name=\"silver_enrichment\",\n",
    "        df=silver_df,\n",
    "        action=\"count\",\n",
    "        metrics={\n",
    "            \"layer\": \"silver\",\n",
    "            \"null_stations\": silver_df.filter(F.col(\"start_station_name\").isNull()).count(),\n",
    "            \"avg_duration\": silver_df.select(F.avg(\"ride_duration_minutes\")).first()[0]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    (silver_df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .partitionBy(\"year\", \"month\")\n",
    "        .save(silver_path))\n",
    "\n",
    "    log_performance_metrics(start_time, \"silver_write\")\n",
    "    logger.info(f\"Silver layer written to: {silver_path}\")\n",
    "    display(Markdown(\"✅ Silver layer processing complete\"))\n",
    "\n",
    "    # Display sample of processed data\n",
    "    display(Markdown(\"### Sample of Processed Data\"))\n",
    "    spark.read.format(\"delta\").load(silver_path).select(\n",
    "        \"started_at\", \"ended_at\", \n",
    "        \"ride_duration_minutes\", \"ride_distance_km\", \n",
    "        \"speed_kmh\", \"part_of_day\", \"is_weekend\", \"season\"\n",
    "    ).limit(5).show()\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in silver transformation: {str(e)}\", exc_info=True)\n",
    "    display(Markdown(f\"❌ Error: {str(e)}\"))\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🏆 Gold Layer Analytics\n",
    " \n",
    " ## Purpose\n",
    " The gold layer provides business insights through:\n",
    " - 💰 Revenue potential analysis\n",
    " - 📏 Trip distance patterns\n",
    " - 🚉 Station utilization metrics\n",
    " - ⏰ Temporal usage patterns\n",
    " - 🛣️ Popular route identification\n",
    " \n",
    " ## Analysis Components\n",
    " ### Revenue Metrics\n",
    " - `total_trips`: Total number of rides\n",
    " - `charged_trips`: Rides > 30 minutes\n",
    " - `extra_time_charge`: $0.2 for rides > 30 min\n",
    " - `revenue_potential`: Expected revenue from charges\n",
    " \n",
    " ### Distance Categories\n",
    " - `0-1 km`: Short neighborhood trips\n",
    " - `1-4 km`: Medium-range trips\n",
    " - `4-9 km`: Long-distance trips\n",
    " - `10+ km`: Extended journeys\n",
    " \n",
    " ### Station Analytics\n",
    " - `total_starts`: Rides starting at station\n",
    " - `unique_destinations`: Different end points\n",
    " - `avg_duration`: Mean trip duration\n",
    " - `avg_distance`: Mean trip distance\n",
    " \n",
    " ### Time Patterns\n",
    " - 🌅 Time of day analysis\n",
    " - 📅 Weekend vs weekday comparison\n",
    " - 🌤️ Seasonal trends\n",
    " - 📊 Usage distribution\n",
    " \n",
    " ## Business Insights\n",
    " 1. Revenue optimization opportunities\n",
    " 2. Distance-based service planning\n",
    " 3. Station capacity management\n",
    " 4. Peak usage optimization\n",
    " 5. Popular route enhancement\n",
    " \n",
    " # Analysis Parameters\n",
    " ANALYSIS_YEAR = 2025\n",
    " ANALYSIS_MONTH = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set analysis parameters\n",
    "ANALYSIS_YEAR = 2025\n",
    "ANALYSIS_MONTH = 1\n",
    "GOLD_PATH = \"/home/aldamiz/data/gold\"\n",
    "gold_base_path = os.path.join(GOLD_PATH, \"analytics\")\n",
    "\n",
    "# Gold Layer Processing\n",
    "display(Markdown(\"# 🏆 Gold Layer Analytics\"))\n",
    "\n",
    "try:\n",
    "    # Define gold layer paths\n",
    "    gold_base_path = os.path.join(GOLD_PATH, \"analytics\")\n",
    "\n",
    "    # Read from silver layer\n",
    "    silver_df = spark.read.format(\"delta\").load(os.path.join(SILVER_PATH, \"rides_nyc\"))\n",
    "\n",
    "    # Function to safely write Delta table\n",
    "    def write_delta_table(df, path, table_name):\n",
    "        \"\"\"Safely write a Delta table with proper configuration.\"\"\"\n",
    "        try:\n",
    "            # First, try to convert existing table to non-CDF if it exists\n",
    "            spark.sql(f\"ALTER TABLE delta.`{path}` SET TBLPROPERTIES ('delta.enableChangeDataFeed' = false)\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            # Delete the existing table\n",
    "            spark.sql(f\"DROP TABLE IF EXISTS delta.`{path}`\")\n",
    "            dbutils.fs.rm(path, recurse=True)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Write the new table with CDF disabled\n",
    "        df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .option(\"delta.enableChangeDataFeed\", \"false\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .partitionBy(\"year\", \"month\") \\\n",
    "            .save(path)\n",
    "        \n",
    "        # Monitor performance\n",
    "        monitor_query_performance(\n",
    "            query_name=f\"{table_name}_write\",\n",
    "            df=df,\n",
    "            action=\"count\",\n",
    "            metrics={\"layer\": \"gold\", \"metric_type\": table_name}\n",
    "        )\n",
    "\n",
    "    # 1. Revenue Metrics Table\n",
    "    revenue_metrics = silver_df \\\n",
    "        .withColumn(\"is_charged_trip\", F.when(F.col(\"ride_duration_minutes\") > 30, 1).otherwise(0)) \\\n",
    "        .withColumn(\"extra_time_charge\", F.when(F.col(\"ride_duration_minutes\") > 30, 0.2).otherwise(0)) \\\n",
    "        .groupBy(\"year\", \"month\") \\\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"total_trips\"),\n",
    "            F.sum(\"is_charged_trip\").alias(\"charged_trips\"),\n",
    "            F.round(F.avg(\"ride_duration_minutes\"), 2).alias(\"avg_duration_min\"),\n",
    "            F.sum(\"extra_time_charge\").alias(\"revenue_potential\")\n",
    "        )\n",
    "    \n",
    "    write_delta_table(\n",
    "        revenue_metrics, \n",
    "        f\"{gold_base_path}/revenue_metrics\", \n",
    "        \"revenue\"\n",
    "    )\n",
    "\n",
    "    # 2. Distance Metrics Table\n",
    "    distance_metrics = silver_df \\\n",
    "        .withColumn(\"distance_bucket\",\n",
    "            F.when(F.col(\"ride_distance_km\") < 1, \"0-1 km\")\n",
    "            .when(F.col(\"ride_distance_km\").between(1, 4), \"1-4 km\")\n",
    "            .when(F.col(\"ride_distance_km\").between(4, 9), \"4-9 km\")\n",
    "            .otherwise(\"10+ km\")\n",
    "        ) \\\n",
    "        .groupBy(\"year\", \"month\", \"distance_bucket\") \\\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"trip_count\"),\n",
    "            F.round(F.avg(\"ride_duration_minutes\"), 2).alias(\"avg_duration_min\"),\n",
    "            F.round(F.avg(\"speed_kmh\"), 2).alias(\"avg_speed_kmh\")\n",
    "        )\n",
    "    \n",
    "    write_delta_table(\n",
    "        distance_metrics, \n",
    "        f\"{gold_base_path}/distance_metrics\", \n",
    "        \"distance\"\n",
    "    )\n",
    "\n",
    "    # 3. Station Metrics Table\n",
    "    station_metrics = silver_df \\\n",
    "        .groupBy(\"year\", \"month\", \"start_station_name\", \"start_lat\", \"start_lng\") \\\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"total_starts\"),\n",
    "            F.round(F.avg(\"ride_duration_minutes\"), 2).alias(\"avg_duration_min\"),\n",
    "            F.round(F.avg(\"ride_distance_km\"), 2).alias(\"avg_distance_km\"),\n",
    "            F.countDistinct(\"end_station_name\").alias(\"unique_destinations\")\n",
    "        )\n",
    "    \n",
    "    write_delta_table(\n",
    "        station_metrics, \n",
    "        f\"{gold_base_path}/station_metrics\", \n",
    "        \"stations\"\n",
    "    )\n",
    "\n",
    "    # 4. Temporal Metrics Table\n",
    "    temporal_metrics = silver_df \\\n",
    "        .groupBy(\"year\", \"month\", \"part_of_day\", \"is_weekend\") \\\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"total_rides\"),\n",
    "            F.round(F.avg(\"ride_duration_minutes\"), 2).alias(\"avg_duration_min\"),\n",
    "            F.round(F.avg(\"ride_distance_km\"), 2).alias(\"avg_distance_km\"),\n",
    "            F.round(F.avg(\"speed_kmh\"), 2).alias(\"avg_speed_kmh\")\n",
    "        )\n",
    "    \n",
    "    write_delta_table(\n",
    "        temporal_metrics, \n",
    "        f\"{gold_base_path}/temporal_metrics\", \n",
    "        \"temporal\"\n",
    "    )\n",
    "\n",
    "    # 5. Popular Routes Table\n",
    "    route_metrics = silver_df \\\n",
    "        .groupBy(\"year\", \"month\", \"start_station_name\", \"end_station_name\") \\\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"route_count\"),\n",
    "            F.round(F.avg(\"ride_distance_km\"), 2).alias(\"avg_distance_km\"),\n",
    "            F.round(F.avg(\"ride_duration_minutes\"), 2).alias(\"avg_duration_min\"),\n",
    "            F.round(F.avg(\"speed_kmh\"), 2).alias(\"avg_speed_kmh\")\n",
    "        )\n",
    "    \n",
    "    write_delta_table(\n",
    "        route_metrics, \n",
    "        f\"{gold_base_path}/route_metrics\", \n",
    "        \"routes\"\n",
    "    )\n",
    "\n",
    "    display(Markdown(\"✅ Gold layer tables created successfully\"))\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in gold layer processing: {str(e)}\", exc_info=True)\n",
    "    display(Markdown(f\"❌ Error: {str(e)}\"))\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 💰 Revenue Analysis\n",
    "display(Markdown(\"\"\"\n",
    "### 💰 Revenue Analysis\n",
    "Understanding revenue potential and trip duration patterns to optimize pricing strategy.\n",
    "\"\"\"))\n",
    "\n",
    "try:\n",
    "    revenue_df = (spark.read.format(\"delta\")\n",
    "        .load(f\"{gold_base_path}/revenue_metrics\")\n",
    "        .filter(F.col(\"year\") == ANALYSIS_YEAR)\n",
    "        .filter(F.col(\"month\") == ANALYSIS_MONTH))\n",
    "    \n",
    "    # Convert to pandas for visualization\n",
    "    revenue_pd = revenue_df.toPandas()\n",
    "    \n",
    "    # Create a composite visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Revenue breakdown pie chart\n",
    "    revenue_data = {\n",
    "        'Regular Trips': revenue_pd['total_trips'].iloc[0] - revenue_pd['charged_trips'].iloc[0],\n",
    "        'Charged Trips (>30min)': revenue_pd['charged_trips'].iloc[0]\n",
    "    }\n",
    "    ax1.pie(revenue_data.values(), labels=revenue_data.keys(), autopct='%1.1f%%')\n",
    "    ax1.set_title('Trip Duration Distribution')\n",
    "    \n",
    "    # Revenue metrics\n",
    "    metrics_text = f\"\"\"\n",
    "    Key Metrics:\n",
    "    • Total Trips: {revenue_pd['total_trips'].iloc[0]:,.0f}\n",
    "    • Charged Trips: {revenue_pd['charged_trips'].iloc[0]:,.0f}\n",
    "    • Revenue Potential: ${revenue_pd['revenue_potential'].iloc[0]:,.2f}\n",
    "    • Avg Duration: {revenue_pd['avg_duration_min'].iloc[0]:.1f} min\n",
    "    \"\"\"\n",
    "    ax2.text(0.1, 0.5, metrics_text, fontsize=12, va='center')\n",
    "    ax2.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error in revenue analysis: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 📏 Distance Analysis\n",
    "display(Markdown(\"\"\"\n",
    "### 📏 Distance Distribution Analysis\n",
    "Analyzing trip distances to understand service coverage and usage patterns.\n",
    "\"\"\"))\n",
    "\n",
    "try:\n",
    "    distance_df = (spark.read.format(\"delta\")\n",
    "        .load(f\"{gold_base_path}/distance_metrics\")\n",
    "        .filter(F.col(\"year\") == ANALYSIS_YEAR)\n",
    "        .filter(F.col(\"month\") == ANALYSIS_MONTH))\n",
    "    \n",
    "    # Convert to pandas and sort buckets correctly\n",
    "    distance_pd = distance_df.toPandas()\n",
    "    bucket_order = [\"0-1 km\", \"1-4 km\", \"4-9 km\", \"10+ km\"]\n",
    "    distance_pd['distance_bucket'] = pd.Categorical(distance_pd['distance_bucket'], categories=bucket_order)\n",
    "    distance_pd = distance_pd.sort_values('distance_bucket')\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    bars = ax.bar(distance_pd['distance_bucket'], distance_pd['trip_count'])\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(height):,}',\n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    plt.title('Trip Distribution by Distance')\n",
    "    plt.xlabel('Distance Bucket')\n",
    "    plt.ylabel('Number of Trips')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display detailed metrics\n",
    "    display(Markdown(\"#### Detailed Metrics by Distance Bucket\"))\n",
    "    display(distance_pd)\n",
    "except Exception as e:\n",
    "    print(f\"Error in distance analysis: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 🚉 Station Analysis\n",
    "display(Markdown(\"\"\"\n",
    "### 🚉 Station Utilization Analysis\n",
    "Identifying high-traffic stations and their characteristics.\n",
    "Note: Some stations may have missing data (1,237 null start stations, 8,429 null end stations)\n",
    "\"\"\"))\n",
    "\n",
    "try:\n",
    "    station_df = (spark.read.format(\"delta\")\n",
    "        .load(f\"{gold_base_path}/station_metrics\")\n",
    "        .filter(F.col(\"year\") == ANALYSIS_YEAR)\n",
    "        .filter(F.col(\"month\") == ANALYSIS_MONTH)\n",
    "        .filter(F.col(\"start_station_name\").isNotNull()))  # Filter out null stations\n",
    "    \n",
    "    # Get top 10 stations\n",
    "    top_stations = station_df.orderBy(F.desc(\"total_starts\")).limit(10).toPandas()\n",
    "    \n",
    "    # Create two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12), height_ratios=[2, 1])\n",
    "    \n",
    "    # Top stations bar chart\n",
    "    bars = ax1.barh(top_stations['start_station_name'], top_stations['total_starts'])\n",
    "    ax1.set_title('Top 10 Busiest Stations')\n",
    "    ax1.set_xlabel('Number of Trips')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        ax1.text(width, bar.get_y() + bar.get_height()/2,\n",
    "                f'{int(width):,}',\n",
    "                ha='left', va='center', fontsize=10)\n",
    "    \n",
    "    # Station metrics\n",
    "    metrics_df = top_stations[['start_station_name', 'avg_duration_min', 'avg_distance_km', 'unique_destinations']]\n",
    "    ax2.axis('tight')\n",
    "    ax2.axis('off')\n",
    "    table = ax2.table(cellText=metrics_df.values,\n",
    "                     colLabels=metrics_df.columns,\n",
    "                     cellLoc='center',\n",
    "                     loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1.2, 1.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in station analysis: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. ⏰ Temporal Analysis\n",
    "display(Markdown(\"\"\"\n",
    "### ⏰ Temporal Usage Patterns\n",
    "Understanding how usage varies by time of day and weekday/weekend.\n",
    "\n",
    "Key insights:\n",
    "- Peak usage is during morning commute hours on weekdays (548,407 rides)\n",
    "- Weekend nights show higher usage than weekday nights\n",
    "- Longest average ride durations are during weekend afternoons\n",
    "- Clear commuting patterns visible in weekday morning and evening peaks\n",
    "\"\"\"))\n",
    "\n",
    "try:\n",
    "    temporal_df = (spark.read.format(\"delta\")\n",
    "        .load(f\"{gold_base_path}/temporal_metrics\")\n",
    "        .filter(F.col(\"year\") == ANALYSIS_YEAR)\n",
    "        .filter(F.col(\"month\") == ANALYSIS_MONTH))\n",
    "    \n",
    "    # Convert to pandas and ensure consistent time of day ordering\n",
    "    temporal_pd = temporal_df.toPandas()\n",
    "    time_order = ['morning', 'afternoon', 'evening', 'night']\n",
    "    temporal_pd['part_of_day'] = pd.Categorical(temporal_pd['part_of_day'], categories=time_order, ordered=True)\n",
    "    temporal_pd = temporal_pd.sort_values('part_of_day')\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Prepare data\n",
    "    weekday_data = temporal_pd[~temporal_pd['is_weekend']]\n",
    "    weekend_data = temporal_pd[temporal_pd['is_weekend']]\n",
    "    \n",
    "    # Plot 1: Rides by Time of Day\n",
    "    x = np.arange(len(time_order))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax1.bar(x - width/2, weekday_data['total_rides'], width, label='Weekday')\n",
    "    bars2 = ax1.bar(x + width/2, weekend_data['total_rides'], width, label='Weekend')\n",
    "    \n",
    "    ax1.set_title('Rides by Time of Day')\n",
    "    ax1.set_xlabel('Time of Day')\n",
    "    ax1.set_ylabel('Number of Rides')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(time_order, rotation=45)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Add value labels\n",
    "    def add_value_labels(bars):\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{int(height):,}',\n",
    "                    ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    add_value_labels(bars1)\n",
    "    add_value_labels(bars2)\n",
    "    \n",
    "    # Plot 2: Average Duration\n",
    "    ax2.set_title('Average Ride Duration by Time of Day')\n",
    "    ax2.set_xlabel('Time of Day')\n",
    "    ax2.set_ylabel('Average Duration (minutes)')\n",
    "    \n",
    "    # Plot lines with markers\n",
    "    ax2.plot(x, weekday_data['avg_duration_min'], \n",
    "             marker='o', label='Weekday', linewidth=2)\n",
    "    ax2.plot(x, weekend_data['avg_duration_min'], \n",
    "             marker='s', label='Weekend', linewidth=2)\n",
    "    \n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(time_order, rotation=45)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display summary statistics\n",
    "    summary_stats = temporal_df.agg(\n",
    "        F.sum(\"total_rides\").alias(\"total_rides\"),\n",
    "        F.round(F.avg(\"avg_duration_min\"), 2).alias(\"overall_avg_duration\"),\n",
    "        F.round(F.avg(\"avg_distance_km\"), 2).alias(\"overall_avg_distance\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    weekday_pct = (weekday_data['total_rides'].sum() / summary_stats[\"total_rides\"]) * 100\n",
    "    weekend_pct = (weekend_data['total_rides'].sum() / summary_stats[\"total_rides\"]) * 100\n",
    "    \n",
    "    display(Markdown(f\"\"\"\n",
    "    #### Key Temporal Metrics:\n",
    "    - Total Rides: {summary_stats[\"total_rides\"]:,}\n",
    "    - Weekday Rides: {weekday_pct:.1f}% ({weekday_data['total_rides'].sum():,} rides)\n",
    "    - Weekend Rides: {weekend_pct:.1f}% ({weekend_data['total_rides'].sum():,} rides)\n",
    "    - Average Duration: {summary_stats[\"overall_avg_duration\"]:.1f} minutes\n",
    "    - Average Distance: {summary_stats[\"overall_avg_distance\"]:.2f} km\n",
    "    \n",
    "    #### Notable Patterns:\n",
    "    1. **Weekday Commute**: Strong peaks in morning (~548K rides) and evening (~516K rides)\n",
    "    2. **Weekend Usage**: Higher night activity compared to weekdays\n",
    "    3. **Duration Trends**: \n",
    "       - Weekend rides are consistently longer\n",
    "       - Longest rides occur during weekend afternoons\n",
    "       - Shortest rides during weekday mornings (commuting)\n",
    "    \"\"\"))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in temporal analysis: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 🛣️ Popular Routes Analysis\n",
    "display(Markdown(\"\"\"\n",
    "### 🛣️ Popular Routes Analysis\n",
    "Understanding the most frequently used paths and their characteristics.\n",
    "Note: Some routes may have missing station data (8,429 null end stations, 1,237 null start stations)\n",
    "\"\"\"))\n",
    "\n",
    "try:\n",
    "    # Read from route_metrics with correct column names and filters\n",
    "    routes_df = (spark.read.format(\"delta\")\n",
    "        .load(f\"{gold_base_path}/route_metrics\")\n",
    "        .filter(F.col(\"year\") == ANALYSIS_YEAR)\n",
    "        .filter(F.col(\"month\") == ANALYSIS_MONTH)\n",
    "        .filter(F.col(\"start_station_name\").isNotNull())\n",
    "        .filter(F.col(\"end_station_name\").isNotNull()))\n",
    "    \n",
    "    # Get top 10 routes\n",
    "    top_routes = (routes_df\n",
    "        .orderBy(F.desc(\"route_count\"))\n",
    "        .limit(10)\n",
    "        .toPandas())\n",
    "    \n",
    "    # Create visualization with subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12), height_ratios=[1.5, 1])\n",
    "    \n",
    "    # Plot 1: Top Routes by Volume with enhanced formatting\n",
    "    route_labels = []\n",
    "    for _, row in top_routes.iterrows():\n",
    "        start = row['start_station_name'][:25]\n",
    "        end = row['end_station_name'][:25]\n",
    "        route_labels.append(f\"{start}... →\\n{end}...\")\n",
    "    \n",
    "    bars = ax1.barh(range(len(route_labels)), top_routes['route_count'],\n",
    "                    color='skyblue', alpha=0.7)\n",
    "    \n",
    "    # Add value labels with better positioning\n",
    "    for idx, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        ax1.text(width * 1.02, idx,\n",
    "                f'{int(width):,} trips',\n",
    "                ha='left', va='center', fontsize=10)\n",
    "    \n",
    "    ax1.set_yticks(range(len(route_labels)))\n",
    "    ax1.set_yticklabels(route_labels)\n",
    "    ax1.set_title('Top 10 Most Popular Routes', pad=20, fontsize=14)\n",
    "    ax1.set_xlabel('Number of Trips', fontsize=12)\n",
    "    ax1.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Route Metrics Table\n",
    "    table_data = []\n",
    "    for _, row in top_routes.iterrows():\n",
    "        table_data.append([\n",
    "            f\"{row['start_station_name'][:20]}... → {row['end_station_name'][:20]}...\",\n",
    "            f\"{int(row['route_count']):,}\",\n",
    "            f\"{row['avg_distance_km']:.2f}\",\n",
    "            f\"{row['avg_duration_min']:.1f}\",\n",
    "            f\"{row['avg_speed_kmh']:.1f}\"\n",
    "        ])\n",
    "    \n",
    "    columns = ['Route', 'Trips', 'Avg Dist (km)', 'Avg Duration (min)', 'Avg Speed (km/h)']\n",
    "    \n",
    "    ax2.axis('tight')\n",
    "    ax2.axis('off')\n",
    "    table = ax2.table(cellText=table_data,\n",
    "                     colLabels=columns,\n",
    "                     cellLoc='center',\n",
    "                     loc='center',\n",
    "                     colColours=['#f2f2f2']*len(columns))\n",
    "    \n",
    "    # Enhanced table formatting\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1.2, 1.5)\n",
    "    \n",
    "    # Style the table\n",
    "    for k, cell in table._cells.items():\n",
    "        cell.set_edgecolor('white')\n",
    "        if k[0] == 0:  # Header row\n",
    "            cell.set_text_props(weight='bold')\n",
    "            cell.set_facecolor('#e6e6e6')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate summary statistics\n",
    "    route_stats = routes_df.agg(\n",
    "        F.count(\"*\").alias(\"total_routes\"),\n",
    "        F.sum(\"route_count\").alias(\"total_trips\"),\n",
    "        F.round(F.avg(\"avg_distance_km\"), 2).alias(\"avg_route_distance\"),\n",
    "        F.round(F.avg(\"avg_duration_min\"), 2).alias(\"avg_route_duration\"),\n",
    "        F.round(F.avg(\"avg_speed_kmh\"), 2).alias(\"avg_route_speed\")\n",
    "    ).collect()[0]\n",
    "\n",
    "    # Calculate additional metrics\n",
    "    top_10_trips = top_routes['route_count'].sum()\n",
    "    total_trips = route_stats[\"total_trips\"]\n",
    "    top_10_percentage = (top_10_trips / total_trips) * 100\n",
    "\n",
    "    # Calculate distance distribution matching the silver layer buckets\n",
    "    distance_distribution = (routes_df\n",
    "        .select(\n",
    "            F.when(F.col(\"avg_distance_km\") < 1, \"0-1 km\")\n",
    "             .when(F.col(\"avg_distance_km\").between(1, 4), \"1-4 km\")\n",
    "             .when(F.col(\"avg_distance_km\").between(4, 9), \"4-9 km\")\n",
    "             .otherwise(\"10+ km\")\n",
    "             .alias(\"distance_range\"),\n",
    "            \"route_count\"\n",
    "        )\n",
    "        .groupBy(\"distance_range\")\n",
    "        .agg(F.sum(\"route_count\").alias(\"total_trips\"))\n",
    "        .orderBy(\"distance_range\")\n",
    "        .toPandas())\n",
    "\n",
    "    display(Markdown(f\"\"\"\n",
    "    #### 📊 Route Network Statistics:\n",
    "    - Total Unique Routes: {route_stats[\"total_routes\"]:,}\n",
    "    - Total Trips: {route_stats[\"total_trips\"]:,}\n",
    "    - Average Route Distance: {route_stats[\"avg_route_distance\"]:.2f} km\n",
    "    - Average Route Duration: {route_stats[\"avg_route_duration\"]:.1f} minutes\n",
    "    - Average Speed: {route_stats[\"avg_route_speed\"]:.1f} km/h\n",
    "\n",
    "    #### 🔝 Top Routes Analysis:\n",
    "    - Top 10 routes account for {top_10_percentage:.1f}% of all trips\n",
    "    - Most popular route: \n",
    "      - {top_routes['start_station_name'].iloc[0]} → {top_routes['end_station_name'].iloc[0]}\n",
    "      - Volume: {top_routes['route_count'].iloc[0]:,} trips\n",
    "      - Distance: {top_routes['avg_distance_km'].iloc[0]:.2f} km\n",
    "      - Duration: {top_routes['avg_duration_min'].iloc[0]:.1f} minutes\n",
    "      - Speed: {top_routes['avg_speed_kmh'].iloc[0]:.1f} km/h\n",
    "\n",
    "    #### 📏 Distance Distribution:\n",
    "    - 0-1 km: {distance_distribution.loc[distance_distribution['distance_range'] == '0-1 km', 'total_trips'].iloc[0]:,} trips\n",
    "    - 1-4 km: {distance_distribution.loc[distance_distribution['distance_range'] == '1-4 km', 'total_trips'].iloc[0]:,} trips\n",
    "    - 4-9 km: {distance_distribution.loc[distance_distribution['distance_range'] == '4-9 km', 'total_trips'].iloc[0]:,} trips\n",
    "    - 10+ km: {distance_distribution.loc[distance_distribution['distance_range'] == '10+ km', 'total_trips'].iloc[0]:,} trips\n",
    "\n",
    "    #### 🎯 Key Insights:\n",
    "    1. **Popular Corridors**: \n",
    "       - Clear commuting patterns between key locations\n",
    "       - Top 10 routes represent {top_10_percentage:.1f}% of all trips\n",
    "    2. **Distance Patterns**:\n",
    "       - Most trips (36.2%) are medium-distance (1-4 km)\n",
    "       - Short trips (0-1 km) account for 23.2% of rides\n",
    "       - Long trips (4+ km) make up 4.7% of total volume\n",
    "    3. **Usage Characteristics**:\n",
    "       - Average duration of {route_stats[\"avg_route_duration\"]:.1f} minutes aligns with typical commute times\n",
    "       - Average speed of {route_stats[\"avg_route_speed\"]:.1f} km/h suggests efficient urban mobility\n",
    "    \"\"\"))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in routes analysis: {str(e)}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
